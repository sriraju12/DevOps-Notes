AWS:

1.EC2:
       Elastic Compute Cloud provides web services api for provisioning,managing and deprovisioning virtual server
       inside amazon clouds.

Ec2 pricing:

             1.On-Demand(pay per hour or even seconds)
             2.Reserved(Reserve capacity for 2-3 years for discounts)
             3.Spot(Bid your price for unused ec2 capacity)
             4.Dedicated Hosts(physical servers dedicated for you)



Components inside ec2 instance:
                               
                                1.AMI(Amazon Machine Image):
                                                            AMI provides the information requried to launch an instance
                                                            which is a virtual server in cloud(this like vagrant box list)

                                2.Instance Type:
                                                when you launch an instance,the instance type that you specify determines
                                                the hardware of the host computer used for your instance.
                                                (how much size and memory required)

                                3.EBS(Amazon Elastic Block Store):
                                                                   Amazon ec2 provides you with flexible,cost effective and
                                                                   easy to use data storage options for your instances.

                                                                   EBS is like virtual hard drive in which you can store 
                                                                   your OS and your data.

                               4.Tags:
                                      Tag is simple label consisting of customer-defined keys and optional values that can
                                      make it easier to manage,search for,and filter resources.

                               5.Security Groups:
                                                  A Security Group acts as a firewall that controlls the traffic for one
                                                  or more instances.

                               6.Amazon EC2 uses the public-pair cryptography to encrypt and decrypt the login information.


Note:
     1.In ec2,whenever the ec2 instance stopped,the public ip will be gone and private ip
       will be there.
     2.when you start the ec2 instance again,the new public ip will generate and private ip
       remain same.

     3.if you want static ip(fixed ip) then you can use elastic ip's.In right menu search for
        elastic ip and select the region and click on create
     4.it will create the elastic ip's and we can associate that ip to our ec2 instance.


AWS CLI:
         if you want to connect aws through command line use follwing steps:
         
         1.install aws cli in your machine.
         2.create user or existing user also fine, in user setting search for acess keys
           and create acess keys 
         3.open gitbash and type aws configure, it will ask for acesss key,secret acess key
           output format is json and region. then configuration is done


  aws sts get-caller-identity -> this command shows the account ID and account number you are
                                 using.



EBS(Amazon Elastic Block Store):
                                 Amazon ec2 provides you with flexible,cost effective and
                                 easy to use data storage options for your instances.

                                 EBS is like virtual hard drive in which you can store 
                                 your OS and your data in the form of volumns.
                                
                                 snapshot is backup of a volumn.


          EBS Types:
                     1.General purpose(SSD-Solid State Drive) - most work loads

                     2.Provisioned IOPS - Large Databases.

                     3.Throughput Optimized HD - big data and data warehouse.

                     4.Cold HDD(Hard Disk Drive) - file servers.

                     5.Magnetic - backups and Archieves.


If you want to store anything separately then follow the below steps:
 
 1.first you need to create a volume(choose the size based on your requirment).
 2.Attach that volume to the instance(just make sure that volume and instance is in same zone).
 
 fdisk -l -> which will show all the disks.

 3.volume we have created for this one,partition is not present we need to create it.

             first step is to create the partitioning.

 4.fdisk /dev/xvdf(hard disk path) -> it will open the hard disk utility.
   click m -> for help
 5. n -> to create the partition.and then click enter upto last step.
 6. p -> to print the disk partition.
 7. w -> to write the partitions(partitions are created).

            next step is to formatting it.

8.mkfs and click the tab button 2 times -> it will show all the available utilities.
9.choose any utility you want and mkfs.ext4 /dev/xvdf1(hard disk path) click enter.
10.the partition is formatted with ext4 format.

           next step is to mount it.

11.create a directory  in temp directory and moves all images file to created directory.
   here mkdir /tmp/img-backups
        mv /var/www/html/images/* /tmp/img-backups/
12.now the images directory is empty.
13.This is a temporary mount -> mount /dev/xvdf1(hard disk path) /var/www/html/images/(where you want to mount i.e path).
14. run df -h command to see the mounted.(how much is used and how much is available)
15.if you want unmount it(delete it) -> umount /var/www/html/images/(path)

now lets see the permanent mount

16.open vim /etc/fstab -> add /dev/xvdf1	/var/www/html/images/	ext4	defaults	0 0
17. mount -a -> this will mount all the entries from fstab file.
18. next is to move the images from temp directory to mounted directory.
    mv /tmp/img-backups/* /var/www/html/images/
19.restart the service -> systemctl restart httpd
20.check the status of the service if mounting is failed then the service will not run -> systemctl status httpd.
21.check in the browser whether it shows images or not.



EBS Snapshots:
               Snapshots are usually to backup's and restores the data.

 
              we will use the previous instance for the backup

      for that first, change the name of the instance and deattach the volume from this instance.
      unmount the previous partition -> umount /var/www/html/images/.
	
 create  a new volume and attach that volume to ec2 instance.


 fdisk -l -> which will show all the disks.

 3.volume we have created for this one partition is not present we need to create it.

             first step is to create the partitioning.

 4.fdisk /dev/xvdf(hard disk path) -> it will open the hard disk utility.
   click m -> for help
 5. n -> to create the partition.and then click enter upto last step.
 6. p -> to print the disk partition.
 7. w -> to write the partitions(partitions are created).

            next step is to formatting it.

8.mkfs and click the tab button 2 times -> it will show all the available utilities.
9.choose any utility you want and mkfs.ext4 /dev/xvdf1(hard disk path) click enter.
10.the partition is formatted with ext4 format.
11.mkdir -p /var/lib/mysql -> this is where mysql database is present.

           now mount it.

16.open vim /etc/fstab -> add /dev/xvdf1	/var/lib/mysql	ext4	defaults	0 0
17. mount -a -> this will mount all the entries from fstab file.

18 now lets install mysql -> yum install mariadb-server -y (since it is a centos).
19.start the service -> systemctl start mariadb
20. ls /var/lib/mysql/ -> here you can see the downloaded files.

Note:
      if you want to restore the existing partitioning then snapshot will not do that instead
      of that snapshot will create a volume for it and store the data.


     Snapshots backups and restores:

     if you loose the data and already taken the snapshot then,

                                     1.unmount partition.
                                     2.deattach volume
                                     3.create a new volume from snapshot
                                     4.attach the created volume from snapshot
                                     5.mount it back.

21.create a snapshot from volume(in volume section).
22.go to /var/lib/mysql directory delete the mysql data from it  -> rm -rf *
23.stop the service -> systemctl stop mariadb.
24.unmount the partition -> unmount /var/lib/mysql/
25.deattch the volume(in volume section).
26.go to snapshot section and select the snapshot and click on create volume.
27.now attach the recovered volume to ec2 instance.
28.mount -a -> this will mount all the entries.
29.now check the data whether it came or not -> ls /var/lib/mysql/.




ELB-Elastic Load Balancer:
                          Elastic load balancing distributes incoming applications or network
                          traffic across multiple targets such as ec2 instances or containers etc.

      Elastic load balancer supports 3 types of load balancers:
                     
                                  1.Application load balancer - which supports only web traffic
                                  2.network load balancer - which is very high performing load balancer and expensive too.
                                  3.classic load balancer - which is simplest one.


     Main flow of elastic load balancer is:
                                            1.create a ec2 instance with website.
                                            2.create AMI for that instance.
                                            3.create launch template and launch it.
                                            4.create target groups.
                                            5.create load balancer.



CloudWatch:
            it monitors the performance of aws environment-standard infrastructure metrics.


            Metrics:
                    Aws cloud watch allows you to record metrics for services such as EBS,EC2
                    Amazon RDS,ColudFront etc.

            Events: 
                  Aws events delivers a near real time stream of systems that describe change
                  in amazon web services resources.

            Logs:
                you can use amazon cloudwatch logs to monitor,store and acess your log files
                from amazon ec2 instance(Elastic Compute Cloud) and other resources.

Note:
     1.alarms monitors cloudwatch metrics for instances.

      SNS(Simple Notification Services) is a web service that co-ordinate and manage the
      sending of messages to subscribing endpoints.

 flow:
       ec2 instance ----> amazon cloudwatch------> Alarms------------>SNS(email notifications).



EFS(Amazon Elastic File System):
                                 It is a shared file system for data storage.

    1.It is similar to EBS(Elastic Block Store) the only difference is, in EBS we can store data
      for single ec2 instance only.
    2.But in EFS(Elastic File System) it shared file system i.e common storage for multiple
      ec2 instances.

      flow:
           1.create the ec2 instance
           2.in EFS,create the file sytem
           3.create the acess points to acess that file system
           4.we need to mount this in /etc/fstab file.



Auto Scaling:
              Auto scaling is a service that automatically monitors and adjust compute
              resource to maintain the performance for applications hosted in the aws.


       flow:
            we need to create the auto scaling group which will provide in launch configuration
            template to launch instances based on the load (cpu utilization) alarm will be triggered
            if it cross the threshold and scaling policy will trigger the launch of new instances
            in the auto scaling group or even reduce the instances based on the scaling policies.



Amazon S3(Simple Storage Service):
                                   Amazon S3 is a storage for the internet.you can use
                                   S3 to store and retrieve any amount of data at any time
                                   from anywhere on the web. i.e it is just like google drive.


         S3 Storage Classes:
                             1.S3 Standard: 
                                            General purpose storage of frequently accessed data.
                                            Fast acess and object replication in multi available zones.

                             2.S3 Infrequent Acess:
                                                    Long lived,but less frequently accessed data.
                                                     Slow access,object replication in multi available zones.

                            3.S3 One Zone-Infrequent Access:
                                                             It is for data that is accessed less frequently,
                                                             but requires rapid access when needed.slow access,
                                                             no object replication.

                           4.S3 Inteligent Tiering:
                                                    Automatically moves data to most cost effective tier.

                           5.S3 Glacier:
                                        Low cost storage for data archiving.

                           6. S3 Glacier Deep Archive:
                                                       Lowest cost storage,retrival time of 12hrs.

Note:
      1.By default everything in S3 Bucket is private.
      2.Bucket versioning: 
                           1.If you disable bucket versioning, if you delete anything in the bucket, you can not retrive it.
                           2.If you enable the bucket versioning,you delete anything in bucket we can recover it but the size 
                             of the bucket will grow(size).if you keep on recovering the objects, the size will increases.

      3.while uploading the objects to S3 bucket,you can choose the storage classes.



If you want to host static website in S3 Bucket:
                                                  1.download the static website from tooplate.com
                                                  2.upload the files to S3 Bucket.
                                                  3.In permission,enable the public acess and make the files as public by selecting the files.
                                                  4.enable the static website hosting in properties option.
                                                  5.use the endpoint to display the content in browser(endpoint present static website hosting in properties).

   
    4.By mistakenly you override the object then follow these steps:
                                                                      
        select the file you want to recover and go to versioning there you can find the previous version of your object 
        and download it and then upload the file.make it public then it will display in browser.This is how you can recover
        the objects in S3 bucket.Just make sure to enable bucket versioning.


    5.If you delete the objects in S3 Bucket you want to recover it follow these steps:

       In the S3 Bucket,you will see the list of objects option,you just enable it.there you can see
       the all versions of yours object.The delete files also shown here.select the delete file and click on the delete option
       then it will come to S3 Bucket.


Flow of project setup Vprofile in aws:

                                       1.Login into aws account.
                                       2.Create key pairs.
                                       3.Create security groups.
                                       4.Launch ec2 instances with user data(bash scripts).
                                       5.Update IP to name mapping in route 53.
                                       6.Bulid application from source code.
                                       7.Upload to s3 bucket(refer point 1 to point 3 in note).
                                       8.Download the artifact to tomcat ec2 instances(refer point 4 onwords for this).
                                       9.Setup Elastic Load Balancer(ELB) with https(certificate from amazon certificate manager).
                                       10.Map Elastic Load Balancer endpoint to website name in Godaddy DNS.
                                       11.Verify the entire setup.
                                       12.Bulid auto scaling group for tomcat instances.(1.AMI
                                                                                         2.launch template 
                                                                                         3.autoscaling group).

Note:
      To upload the artifact into amazon S3 bucket follow these steps:
   
       1.first we need to configure the IAM role in AWS CLI(commmand -> aws configure)
       2.create the s3 bucket using command -> aws s3 mb s3://hkh-code-artis(Bucket name).
       3.we copy the artifact to S3 bucket -> aws s3 cp target/vprofile-v2.war s3://hkh-code-artis/

       4.create the role for the s3 bucket and attach that role to the tomcat instance.
       5. aws s3 ls -> it will show all the s3 buckets.
       6.copy the artifact to temp folder -> aws s3://hkh-code-artis/vprofile-v2.war /tmp/
       7.delete the default tomcat page -> rm -rf /var/lib/tomcat9/webapps/ROOT
       8.copy the artifact from temp folder to tomcat default page -> cp /tmp/vprofile-v2.war /var/lib/tomcat9/webapps/ROOT.war




Refactoring the above project:
                          

                                 Comparision


 1.BeanStalk                                    1.Tomacat EC2 instance on vm
 2.ELB in beanstalk                             2.nginx LB/ELB
 3.autoscaling                                  3.autoscaling
 4.EFS/S3                                       4.EFS/S3
 5.RDS                                          5.mysql on ec2 instances.
 6.Elastic cache                                6.memcache on ec2 instance.
 7.Active MQ                                    7.Rabbit MQ on ec2 instance.
 8.Route 53                                     8.Godaddy,local DNS
 9.Cloud Front                                  9.Multi delivery content across world.



Flow of execution:  

                                                           monitors by amazon cloud watch
                                                                               ^
                                                                               |
user ---> Route 53 ---> cloud front ---> application load balancer ---->  ec2 instances in bean stalk ----> stores artifact in s3 bucket.
                                                                               |
                                                                               |
                                    RDS(mysql)<-----Elastic Cache<----- Active MQ
                                                                         

1.Login into aws account.
2.create the key pair for beanstalk instances login.
3.create the security group for Elastic cache,Active MQ and RDS.
4.create
         RDS,Amazon Elastic Cache,Amazon MQ.

5.create Elastic Beanstalk environment.
6.update security group for backend to allow traffic from beanstalk security group.
7.update security group for backend to allow internal traffic.

8.launch EC2 instances for DB initializing.
9.login into the instance and initialize the RDS DB.
10.change the health check on beanstalk to /login.
11.add 443 https listener to ELB.

12.build artifact from backend information.
13.deploy artifact to beanstalk.
14.create CDN(Content Delivery Network) with ssl certificate.	
15.update entry in godaddy DNS Zones.
16.test the url.


Amazon CloudFront:
                   Amazon CloudFront is a content delivery network operated by Amazon Web Services.
                   The content delivery network was created to provide a globally-distributed network of proxy servers
                   to cache content, such as web videos or other bulky media, more locally to consumers,
                   to improve access speed for downloading the content.

Note:
      1.Generally whenver the user request some thing on the browser it will route to S3 since the data is present in S3.
      2.When you use cloudfront,for the first time whenever the user request for data from s3, the data is cached to 
        nearest edge location.so again whenever you request the same data then the data will come from the edge location 
        instead of S3.So that we can able to acess the data very fast.






AWS-part-2:

         VPC(Virtual Private Cloud):
                                     1.vpc is a logical data center within a region.
                                     2.vpc is an on-demand configurable pool of shared computing resources allocated within a public cloud environment.
                                     3.control over network environment,select ip address range,subnets and configure route tables and gateways.

         Subnet Masks:
                       A subnet mask is used to divide an IP address into two parts.
                       One part identifies the host (computer), 
                       the other part identifies the network to which it belongs.
                       To better understand how IP addresses and subnet masks work, look at an IP address and see how it's organized.

                        ex: our ip is 192.168.0.172
                            and subnet mask is 255.255.255.0
                            here 192.168.0.0 is network ip(class c) and last ip is 192.168.0.255 is for broadcast.

                         based on the subnet masks only we can avaiable ips.
                         like start with 192.168.0.1 to 192.168.0.255.here first three octet are fixed because of subnets(255.255.255.0)

         CIDR(Classless Inter-Domain Routing):
                                              Classless Inter-Domain Routing (CIDR) notation is a way to represent an IP address and its network mask.

                                               ex: 192.168.0.172 is network ip                  
                                                   255.255.255.0 is subnet masks

                                          This can be represent as 192.168.0.0/24 which is 255.255.255.0 as subnet

                                                 255.0.0.0
                                                 /8

             this can be represented as          11111111.00000000.00000000.00000000
                                           
                                                 255.255.0.0
                                                 /16

                                                 11111111.11111111.00000000.00000000

                                                 255.255.255.0
                                                 /24
                         
                                                 11111111.11111111.11111111.00000000


         VPC design and components:
                                     vpc is divide into two parts.one is public subnet and another is private subnet.

                                     public subnet is connected to internet gateway and private subnet is connected to NAT gateway present inside public subnet.

         NAT gateway:
                      Network Address Translation(NAT) gateway to enable instances in a private subnet to connect to the internet or other aws resources. 

         Internet Gateway:
                           An internet gateway is horizontally scaled,redudant and 
                           highly available VPC components that allows communication between instances in your vpc and the internet.

        Bastion Host:
                      Bastion host is the single point entry to access your resources in the private subnet.

        VPC Peering:
                     A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses.
                     Instances in either VPC can communicate with each other as if they are within the same network.

        Note:
              1.we can not create a default VPC in aws.
              2.to copy key from one instance to another instance through terminal
                        
                      -> scp -i Downloads/bostion-key.pem Downloads/web-key.pem ubuntu@52.53.153.72:/home/ubuntu/
                         here web-key.pem file is copied into the bostion host instance in /home/ubuntu/ directory.

      flow for creating VPC:
                              1.create VPC
                              2.create subnets(public subnets & private subnets)
                              3.create internet gateway.
                              4.create NAT gateway and before that create Elastic Ip and attach that to NAT gateway.
                              5.create Route Tables and Add the subnet association and Route(public subnet attach to internet gateway)
                                                                                             & private subnet attach to NAT gateway
                              6.Go to subnets and click on action, click on edit subnet settings and Enable auto-assign public IPv4 address.
                                do this for all public subnets.

                              7.Go to your VPC and click on action,click on edit VPC settings and Enable DNS hostnames. 
                             
                              8.create one ec2 instance(Bostion host) and while creating ec2 instance select the created vpc and choose any public subnet in subnet.

                              9.now we will create one ec2 instance and deploy one website in ec2 instance in private subnet and that can be access through bastion host.

                              10.to copy key from  instance(website instance) to another instance(bastion host) through terminal
                        
                                   -> scp -i Downloads/bostion-key.pem Downloads/web-key.pem ubuntu@52.53.153.72:/home/ubuntu/
                                      here web-key.pem file is copied into the bostion host instance in /home/ubuntu/ directory.

                                        chmod 400 web-key.pem (file permission).

                              11.create the elastic load balancer and before that first create the target groups then you can access the website using load balancer DNS name.


EC2 Logs:
         1. common way is achieve the logs and send it to somewhere out of the system and then clean the logs.

steps:
       1.create ec2 instance and deploy website on it.So that logs will generate.
       2.ssh to instance and go to logs path and tar the logs files.
       3.clear the logs by cat /dev/null > acess_log(log filename)
       4.create the s3 bucket and we can move the tar file into s3 bucket.
       5.for that first, create IAM user and create access key and secret key.
       6.ssh to instance and install awscli
       7.configure aws, provide access key and secret key,region and output format as json->aws configure
       8.copy the tar file into s3 bucket -> aws s3 cp fullpath(including filename) s3://bucketname/

                                or

            aws s3 sync sourcepath s3://bucketname/

        NOTE: instead point 5,6 simply you can use -> 1.create a role and select the policy you want,in our case it is AmazonS3fullAccess
                                                      2.attach this role to ec2 instance.click action -> security -> modify IAM role.

 
       2.To stream logs live:


            steps:
                    1.create ec2 instance and deploy website on it.So that logs will generate.
                    2.create a role and select the policy you want,in our case it is Amazoncloudwatchlogfullaccess
                    3.attach this role to ec2 instance.click action -> security -> modify IAM role.
                    4.ssh to instance and install awscli and aws logs(this cloudwatch agent takes logs from our log file that we specify 
                      and stream it to cloudwatch
                    5.edit the file, at the end add the file our log file path(/var/log/httpd/access_log) -> vim /etc/awslogs/awslogs.conf
                    6.restart the awslogs service -> systemctl restart awslogsd and  systemctl enabled awslogsd
                    7.check in the cloudwatch, logs will generate over here.

                     if you want to more specifically send this log file to cloudwatch then
                           1.copy the log path -> /var/log/httpd/access_log
                           2.edit the file, at the end add the file our log file path(/var/log/httpd/access_log) -> vi /var/awslogs/etc/awslogs.conf

                   8.from cloudwatch, you can export logs to s3 bucket.


=================================================================================================================================================================


AWS:

* one subnet can be present in one availability zone only. it can not be present in 2 availability zones.

* one availability zone can have multiple subnets.

* if a route table having route to internet via internet gateway then that route table becomes public route table.

* if you attach this public route table to a subnet then that subnet becomes public subnet.

* in NACL's, both inbound and outbound rules can be defined is 40 rules only.

* In AWS, ephemeral ports are important for configuring Network Access Control Lists (NACLs). While Security Groups are stateful, automatically allowing return traffic
  for a given connection, NACLs are stateless. This means you must explicitly define rules for both inbound and outbound traffic.

* When a client (like your computer) connects to a server (like an EC2 instance), it uses a well-known port for the inbound request (e.g., port 80 for HTTP).
  However, the return traffic from the server back to the client uses an ephemeral port.  

* Because NACLs are stateless, you must add an outbound rule to your NACL that allows traffic back to the client on the ephemeral port range. 
  If you don't, the server's response will be blocked by the NACL, and the connection will fail, even if your inbound rule is correct.


  Example:
            * Inbound Request: A client sends a request to your EC2 instance on a port you've opened, like port 80.

            * Outbound Response: The EC2 instance processes the request and sends a response back to the client. This response is sent from your
                                 instance's source port (80) to the client's randomly assigned ephemeral port.

            * NACL Rule: To allow this response, your NACL needs an explicit outbound rule that permits traffic to the ephemeral port range. 
                         This range is defined by the operating system of the client, not the server. For most Linux kernels, the range is 32768-60999.
                         For Windows and macOS, it's typically 49152-65535. A common practice is to allow a wide port range to cover various client types.


* Ephemeral Ports for:
                       Linux: 32768 to 60999
          
                       Microsoft Windows: 49152 to 65535


* In NACL's, lower rule number has a higher priority and rules are evaluated or processed in sequencially i.e #100 is evaluated before rule #200.

* rule number starts with 100.




VPC:

* create the VPC

* create the two subnets and then create the 2 route tables.

* one route table is for public subnet and another route table is for private subnet.

* for public route table, add internet gateway and then associate it with public subnet.

* for private route table, associate with private subnet.

* create the NACL's for private subnet and public subnet.

* launch two ec2 instances and place one ec2 instance in public subnet and another in private subnet.

* create the security groups for each ec2 instance.

* Now, if you want to connect from ec2 instance from public subnet to ec2 instance in private subnet then
  first copy the pem file to the ec2 instance in public subnet using scp command.

* you can connect via ssh from public ec2 instance to private ec2 instance.

* for that you need remember 4 things like public subnet security group and then public subnet NACL outboud rule and
  then private subnet NACL inbound rule and then private subnet security group.

* for public subnet security group, we don't need to do anything because security group is stateful, so we don't edit outbound rules.

* for public subnet NACL's, we need to create NACL outbound rule for SSH from VPC range.

* for private subnet NACL, we need to create NACL inbound rule for SSH from VPC range because here traffic is incoming.

* for private subnet security group, we need to add SSH rule from either private ip address of the ec2 instance in public subnet or
  security group of ec2 instance in public subnet. Both should work here.

* Now, the ssh connection from ec2 instance of public subnet to ec2 instance of private subnet won't work because we need to do for outgoing traffic also.
  till now we have done it for incoming traffic only.

* for that you need remember 4 things like private subnet security group and then private subnet NACL outboud rule and
  then public subnet NACL inbound rule and then public subnet security group. 

* for private subnet security group, we don't need to do anything because security group is stateful, so we don't edit outbound rules. 

* for private subnet NACL's, we need to create NACL outbound rule for SSH from VPC range and for port range we need to use Ephemeral Ports(Linux: 32768 to 60999).

* for public subnet NACL, we need to create NACL outbound rule for SSH from VPC range and for port range we need to use Ephemeral Ports(Linux: 32768 to 60999).

* for public subnet security group, we don't need to do anything because it is stateful and it should remember this connection.

* This connection is also called jump server or bostion host.

* if the ec2 instance in the private subnet wants to connects to internet then you need to create NAT Gateway and then attach this NAT gateway to the 
  private subnet route table. i.e NAT Gateway should be present in public subnet.

* how it goes to the internet, ec2 instance in private subnet sends requests then it will go to NAT Gateway that is present in public subnet through 
  private subnet route table. from NAT Gateway, it will go to Internet Gateway(IGW) and from IGW to internet.

* create the NAT Gateway and while creating don't forgot to place in public subnet.

* go to private subnet route table and then add route 0.0.0.0/0 to NAT Gateway.

* if you want to do curl https://google.com from ec2 instance in private subnet i.e here it is trying to connect to the internet.

* let's see what things we need to keep in mind to establish this connection.

* first security group private subnet and then create the NACL outbound rule and then create the public subnet NACL inbound rule and then create public subnet NACL outbound rule.

* for private subnet security group, we don't need to do anything because here traffic is outgoing.

* for private subnet NACL, we need to create outbound rule for HTTPS(443) to 0.0.0.0/0 

* next we need to create the public subnet NACL inbound rule for HTTPS(443) to 0.0.0.0/0 or our VPC Range

* and also we need to create the public subnet NACL outbound rule for HTTPS(443) to 0.0.0.0/0. i.e it should go to IGW(internet gateway) via public subnet inorder to reach
  internet.

* Now, we need to look after incoming traffic from internet(response).

* from internet, incoming(response) traffic reaches to NAT Gateway that is present in public subnet. so we need to create a NACL inbound rule(we have already allowed Ephemeral Ports)

* from public subnet NAT Gateway to private subnet traffic go. so we need to allow/create public subnet NACL outbound rule(we have already allowed Ephemeral Ports).

* Now we need to create the private subnet NACL inbound rule(we need to allow Ephemeral Ports to 0.0.0.0/0 (because response is coming from internet.)

* for private subnet ec2 instance security group we don't need to do anything because it will remember the connection.


NOTE:

* Ephemeral Ports are used for response 

* Elastic IP are fixed IP's. if you attached to ec2 instance and then you have deleted ec2 instance still Elastic Ip is present. You need to manually delete that IP.

* VPC CIDR range should be between /16 to /28 only i.e 10.24.0.0/16

* can we create VPC with same CIDR range. yes we can create it because VPC is isolated environment. but it is of no use in the same account. Always try to create VPC
  with different CIDR range.



Multi CIDR Range:

* for VPC, we can create maximum of 5 CIDR range for more IP's. for example, 10.24.0.0/16, 10.25.0.0/16, 10.26.0.0/16, 10.27.0.0/16, 10.28.0.0/16

* previously this option is not there that's why they are creating another VPC and deploying resources in that VPC. 

* if you need more IP's then you can use multi CIDR range in the VPC.

* when you add the CIDR range in the VPC, then automatically that CIDR range will be added to the all route tables in that VPC.

* in the route table, in routes if you see the any status as Blackhole that means that service is not working (mainly the route table uses NAT Gateway service only).
  we need to check health of the service for troubleshooting.

* in the VPC, first created CIDR range can not be deleted but you can delete remaning 4 CICR ranges.


Reserved IP Addresses:

* in the VPC, you have given CIDR range as 10.24.0.0/16 and while creating the subnet you have given CIDR range as 10.24.0.0/24 then for this subnet 
  total available IP's should be 256.

* but when you check in the subnet, it will show only 251.

* remaining 5 IP address are reserved IP Addresses for that subnet.

* here our subnet CIDR range is 10.24.0.0/24 then

    10.24.0.0 IP is allocated to network address

    10.24.0.1 IP is allocated to VPC router

    10.24.0.2 IP is allocated to DNS

    10.24.0.3 Ip is allocated to future use

    10.24.0.255 Ip is allocated to network broadcast address

* These 5 Ip's are called Reserved IP Addresses 

* if you are creating the VPC with CIDR range as 10.24.0.0/28 then while creating the subnet CIDR range as 10.24.0.0/29 here available IP's are 8 only in that 5 are
  reserved IP's. you can only create one subnet here.




How to Extend/Add EBS Volumes to ec2 instance:

* One EBS can only attach it to the single ec2 instance only

* for ec2 instance if you attach volume then that volume becomes Block Devices.

* while attaching the volume to ec2 instance then both ec2 and volume should be present in same avilability zone only.

* These volumes is called EBS(Elastic Block Storage).

* you can increase the storage but you cannot decrease it.


For Windows how to extend volume:

* you have increase volume size in aws and we need to add that unused volume to our disk then click window + R then type diskmgmt.msc

* it will open the disk management window, here it will show unallocated storage. 

* right click on the c drive and then select extend volume. it will open popup and then click on next till very end. That's it.


Attaching new volume to Windows:

* create the volume and then attach it to the ec2 instance.

* click window + R then type diskmgmt.msc and it will open the disk management window, here it will show unallocated storage. 

* click on the disk storage and then select initialize disk

* click on the allocated storage and then select new simple volume and then click on next and then select storage and then select drive letter and then click on finish.



For Linux how to extend volume:

* lsblk -> this command to list all block devices.

* you have increase volume size in aws

* first we need to extend the partition -> sudo growpart /dev/nvme0n1 1    i.e here /dev/nvme0n1 is disk and 1 is partition number

* extend the file system -> sudo xfs_growfs -d /

* run this command to verify the size is showing or not -> df -hT


Attaching new volume to Linux:

* create the volume and then attach it to the ec2 instance.

* df -kh -> it will display the newly created disk.

* first need to check which disk name it is using and then run this command -> file -s /dev/diskname    i.e file -s /dev/nvm1n1

* make xfs file system i.e it will create the xfs file system -> mkfs -t xfs /dev/nvm1n1

* we need to create a folder -> mkdir raju

* now we need to mount this folder to the xfs file system that we have created above  -> sudo mount /dev/nvm1n1 /raju

* run df -kh -> it will display this /dev/nvm1n1 is mounted to /raju


                     OR

* lsblk -> this command to list all block devices.

* first need to check which disk name it is using and then run this command(it will show no data) -> file -s /dev/diskname i.e file -s /dev/nvm1n1  

* format the disk -> sudo fdisk /dev/nvm1n1 

* then type n to create partition and then type p for primary partition and then type 1-4 any number(partition number) and then enter and then enter and then type w for save and quit.

* if you run this command, it will show partiton to your disk -> lsblk

* now create the xfs file system from that partition  -> sudo mkfs -t xfs /dev/nvm1n1

* create a folder -> mkdir raju1

* mount this folder to the xfs file system that we have created above -> sudo mount /dev/nvm1n1p1(partition name) /raju1

* run df -kh -> it will display this /dev/nvm1n1p1 is mounted to /raju1

                     

EFS(Elastic File System):

* it is a shared file system and can attach it to multiple ec2 instance.

* EFS can be present in any availability zone still we can attach it to the ec2 instance present in different availability zone. 


How to attach EFS to ec2 instance:

* create the EFS and then click on attach then it will show two option to mount EFS to ec2.

* one is mount via DNS and another one is mount via IP.

* choose mount via IP and then select your availability zone that your ec2 instance is present. it will display one command. copy it.

* we need to create one folder -> mkdir raju

* paste that command and then in that command last word is folder name just replace with the folder name that we have created now.

* run df -kh -> it will display the ip address and then that IP is mounted to that folder that we have created.

* run lsblk -> here EFS will not show because it is not Block device. it is network based storage




How to create backup's of ec2 using AMI:

* go to ec2 instance and then select the instance you want to take backup and then click on actions and then click on image and templates and then click on create image.

* fill the details and then click on create image.

* whenever you create AMI's(backup) of ec2 instance then it also create the snapshot and when snapshot is created then only AMI comes to available state from pending state.


How to restore ec2 from AMI:

* go to AMI in ec2  and then select the AMI you want to restore and then click on launch instance from AMI and then fill the details like name, instancetype, security groups etc.

* This is how we can restore the ec2 instance from AMI.


NOTE:

* we can also create snapshot(backup) for the volumes

* select the volume and then click on Actions and then click on create snapshot. it will create a snapshot of your volumes.

* can we create a AMI from snapshot. Yes, we can create AMI from snapshot.

* for that, select the snapshot and then click on Actions and then click on create image from snapshot.

* can we create an ec2 instance from this AMI. Yes, we can create ec2 instance from the AMI that we have created but you won't work because there is no OS(Operating System)
  in the AMI. it will on come in running state but status check will stuck in initiliazing state.

* when you delete AMI then snapshot will not delete, you need to manually delete those snapshots.  






DNS(Domain Name Server):

* it is used to map ip address to name.

* Route53 is DNS server in AWS.

* in Route53, 53 is port for DNS.

* in Route53 you can buy domain and then create hostedZones and then create record for it.

* and also you can buy domain in third party like godaddy.com and then create hostedZones in route53 and then create record for it.

* mainly used records are A, AAAA and CName:

        A -> maps to ipv4

        AAAA -> maps to ipv6

        CName -> for creating alias for the same domain i.e name to name.

* While creating the record, there is TTL(Time To Live) basically an expiration period in sec like 300. after 300 sec the ip address stored in it will be expired and then
  fetches again.

* For suppose, you have updated the IP address of the record and when you try to nslookup on that IP address, it will not show that updated IP address because we need to check
  the TTL seconds, after that time only it will fetch again and then it will show updated IP address.

Routing Policies:

* Simple routing:
                  just map the web server to name domain. NO extra policies.


* failover routing:
                    failover routing lets you route traffic to a resource is healthy or to a different resource when the first resource is unhealthy. you need to create 2 records
                    i.e one is primary and another one is secondary.


* weighted routing:
                    based on the weight we can distribute the traffic to the resources. here you need to create 2 records for the resource. and mention the weight for that resource
                    i.e o to 255. if one resouce you have given 55 then for another resource you need to give 200.


* Geolocation routing:
                       if the users are using your website in different countries then inorder to reduce the latency then we need to deploy to multiple regions and based on
                       the location it will redirect the traffic.



NOTE:

* Lifecycle rules in S3 are a set of rules that define actions to be taken on objects throughout their lifetime. These rules help you manage your storage
  costs by automatically transitioning objects to a more cost-effective storage class or by deleting them after a certain period.

* Lifecycle rules work on a per-bucket basis and can be applied to all objects in a bucket or to a specific subset of objects based on a shared prefix (folder) or tags  

A lifecycle rule consists of two main components:

* Transition Actions: These actions move objects from one storage class to another. For example, you can create a rule to automatically move objects
                      from S3 Standard (for frequently accessed data) to S3 Standard-Infrequent Access (IA) after 30 days, and then to S3 Glacier Flexible Retrieval
                      (for archival data) after 90 days. This helps you save money on storage for data that is no longer accessed frequently.

* Expiration Actions: These actions permanently delete objects after a specified period. For example, you can create a rule to delete temporary
                      log files after 60 days to prevent them from accumulating and costing you money.  


VPC:
Let’s see how VPC works:
1. The user sends requests from the browser and it will goes to the VPC.
2. In the VPC, that request is received by the internet gateway and internet gateway to public subnet which is single public subnet in the VPC.
3. From the public subnet to elastic load balancer it will pass.
4. In elastic load balancer has the target group through that it will transfer the request with the help of route table , it will reach to the subnet where the application is deployed.
5. To enter into the application in ec2 , security group should allow that request. And then only request will be processed.

In simple, this from internet to vpc
User sends request -> internet gateway -> public subnet -> elastic load balancer -> route table -> security groups -> ec2 instance in subnet.

If you want to access anything from the internet then the private subnet should not allow it’s ip to the internet. For that, it follows the masking of ip through router or elastic load balancer. If that masking is done using elastic load balancer then it is called as SNAT gateway. If that masking is done using router then it is called as NAT gateway.

SECURITY GROUPS AND NACL:

Security groups:
Security groups are applied at instance level to only allow the traffic to the instance like ec2 instance.

NACL: NACL stands for Network Access Control List
Which is used at the subnet level and is used to allow and deny what kinds of traffic that is incoming.

Difference between Security groups and NACL:
   1. security groups applied at instance level.
2. NACL is applied at subnet level
3. Security groups are  used to allow traffic only.
4. NACL is used to allow and deny the traffic.

Consider this scenario, for ec2 instance security groups are used to allow what kind of traffic should access the instance. Where as NACL, the ec2 instance that is placed inside the subnet. The NACL should allow or deny the traffic and it allows to act as extra layer of security to it.

Let’s see there are 50 ec2 instance are placed inside the subnet and you want to control the traffic instead of creating the security groups for every ec2 instance, you can create the NACL and it will applied to subnet that means it will apply automatically for all the instance that are present inside it. We can save a lot of time.

Route53:
Route53 is DNS solution . DNS stands for Domain Name System.
DNS is used to map the ip address to the Name.
Like flipkart.com, amazon.com etc.
Instead of using the ip address, we are using the names to access the application because we cannot remember the ip address but we can easily remember the names.

They are two ways to configure the DNS:
1. In Route53, you can purchase the domain and create the hosted zones. In Hosted zones we will create the records that will map the ip address to names.
2. You can purchase the domain from outside like godaddy.com and you can create the hosted zones in the Route53.

NOTE:
Bastion host is used to access the resources that are present in the private subnet i.e ec2

If you want to create the auto scaling groups then first you need to create the template for it in that you need to mention the details like ami, instance type, in network create the security group and then allow the port whatever you want and in VPC, select the VPC we have created and then launch it.

Let’s see how we can create the Boston host:
1. Just create the ec2 instance and in that place this in VPC that we have created and allow ssh to this and also enable public ip to it.
2. We need to ssh to it and from this to, we need to ssh to the ec2 that is present in the private subnet and also we need to export the key pair to the Boston host because to ssh to it we need key pair.
3. how to copy file to remote server from local
4. scp -i path of Boston key pair path of private ec2 instance key pair username@public ip:path where you want to paste the keys.
5. Login into Bostion host and check whether the keys are present or not.
6. ssh to the private ec2 instance using private ip. You can successfully login into that.

In simply, if you want to create the autoscaling group then you need to create the launch template.
If you want to create the load balancer then you need to create the target groups.

# Scenario Based Interview Questions on EC2, IAM and VPC


Q: You have been assigned to design a VPC architecture for a 2-tier application. The application needs to be highly available and scalable. 
   How would you design the VPC architecture?

A: In this scenario, I would design a VPC architecture in the following way.
   I would create 2 subnets: public and private. The public subnet would contain the load balancers and be accessible from the internet. The private subnet would host the application servers. 
   I would distribute the subnets across multiple Availability Zones for high availability. Additionally, I would configure auto scaling groups for the application servers.

Q: Your organization has a VPC with multiple subnets. You want to restrict outbound internet access for resources in one subnet, but allow outbound internet access for resources in another subnet. How would you achieve this?

A: To restrict outbound internet access for resources in one subnet, we can modify the route table associated with that subnet. In the route table, we can remove the default route (0.0.0.0/0) that points to an internet gateway. 
   This would prevent resources in that subnet from accessing the internet. For the subnet where outbound internet access is required, we can keep the default route pointing to the internet gateway.

Q: You have a VPC with a public subnet and a private subnet. Instances in the private subnet need to access the internet for software updates. How would you allow internet access for instances in the private subnet?

A: To allow internet access for instances in the private subnet, we can use a NAT Gateway or a NAT instance. 
   We would place the NAT Gateway/instance in the public subnet and configure the private subnet route table to send outbound traffic to the NAT Gateway/instance. This way, instances in the private subnet can access the internet through the NAT Gateway/instance.

Q: You have launched EC2 instances in your VPC, and you want them to communicate with each other using private IP addresses. What steps would you take to enable this communication?

A: By default, instances within the same VPC can communicate with each other using private IP addresses. 
  To ensure this communication, we need to make sure that the instances are launched in the same VPC and are placed in the same subnet or subnets that are connected through a peering connection or a VPC peering link. 
  Additionally, we should check the security groups associated with the instances to ensure that the necessary inbound and outbound rules are configured to allow communication between them.

Q: You want to implement strict network access control for your VPC resources. How would you achieve this?

A: To implement granular network access control for VPC resources, we can use Network Access Control Lists (ACLs). 
  NACLs are stateless and operate at the subnet level. We can define inbound and outbound rules in the NACLs to allow or deny traffic based on source and destination IP addresses, ports, and protocols. 
  By carefully configuring NACL rules, we can enforce fine-grained access control for traffic entering and leaving the subnets.

Q: Your organization requires an isolated environment within the VPC for running sensitive workloads. How would you set up this isolated environment?

A: To set up an isolated environment within the VPC, we can create a subnet with no internet gateway attached. 
   This subnet, known as an "isolated subnet," will not have direct internet connectivity. We can place the sensitive workloads in this subnet, ensuring that they are protected from inbound and outbound internet traffic. 
   However, if these workloads require outbound internet access, we can set up a NAT Gateway or NAT instance in a different subnet and configure the isolated subnet's route table to send outbound traffic through the NAT Gateway/instance.

Q: Your application needs to access AWS services, such as S3 securely within your VPC. How would you achieve this?

A: To securely access AWS services within the VPC, we can use VPC endpoints. VPC endpoints allow instances in the VPC to communicate with AWS services privately, without requiring internet gateways or NAT gateways. 
  We can create VPC endpoints for specific AWS services, such as S3 and DynamoDB, and associate them with the VPC. 
  This enables secure and efficient communication between the instances in the VPC and the AWS services.

Q: What is the difference between NACL and Security groups ? Explain with a use case ?

A: For example, I want to design a security architecture, I would use a combination of NACLs and security groups. At the subnet level, I would configure NACLs to enforce inbound and outbound traffic restrictions based on source and destination IP addresses, ports, and protocols. NACLs are stateless and can provide an additional layer of defense by filtering traffic at the subnet boundary.
  At the instance level, I would leverage security groups to control inbound and outbound traffic. Security groups are stateful and operate at the instance level. By carefully defining security group rules, I can allow or deny specific traffic to and from the instances based on the application's security requirements.
  By combining NACLs and security groups, I can achieve granular security controls at both the network and instance level, providing defense-in-depth for the sensitive application.

Q: What is the difference between IAM users, groups, roles and policies ?

A: IAM User: An IAM user is an identity within AWS that represents an individual or application needing access to AWS resources. IAM users have permanent long-term credentials, such as a username and password, or access keys (Access Key ID and Secret Access Key). IAM users can be assigned directly to IAM policies or added to IAM groups for easier management of permissions.
   IAM Role: An IAM role is similar to an IAM user but is not associated with a specific individual. Instead, it is assumed by entities such as IAM users, applications, or services to obtain temporary security credentials. IAM roles are useful when you want to grant permissions to entities that are external to your AWS account or when you want to delegate access to AWS resources across accounts. IAM roles have policies attached to them that define the permissions granted when the role is assumed.
   IAM Group: An IAM group is a collection of IAM users. By organizing IAM users into groups, you can manage permissions collectively. IAM groups make it easier to assign permissions to multiple users simultaneously. Users within an IAM group inherit the permissions assigned to that group. For example, you can create a "Developers" group and assign appropriate policies to grant permissions required for developers across your organization.
   IAM Policy: An IAM policy is a document that defines permissions and access controls in AWS. IAM policies can be attached to IAM users, IAM roles, and IAM groups to define what actions can be performed on which AWS resources. IAM policies use JSON (JavaScript Object Notation) syntax to specify the permissions and can be created and managed independently of the users, roles, or groups. IAM policies consist of statements that include the actions allowed or denied, the resources on which the actions can be performed, and any additional conditions.

Q: You have a private subnet in your VPC that contains a number of instances that should not have direct internet access. However, you still need to be able to securely access these instances for administrative purposes. How would you set up a bastion host to facilitate this access?

A: To securely access the instances in the private subnet, you can set up a bastion host (also known as a jump host or jump box). The bastion host acts as a secure entry point to your private subnet. Here's how you can set up a bastion host:
      Create a new EC2 instance in a public subnet, which will serve as the bastion host. Ensure that this instance has a public IP address or is associated with an Elastic IP address for persistent access.
      Configure the security group for the bastion host to allow inbound SSH (or RDP for Windows) traffic from your IP address or a restricted range of trusted IP addresses. This limits access to the bastion host to authorized administrators only.
      Place the instances in the private subnet and configure their security groups to allow inbound SSH (or RDP) traffic from the bastion host security group.
      SSH (or RDP) into the bastion host using your private key or password. From the bastion host, you can then SSH (or RDP) into the instances in the private subnet using their private IP addresses.


S3 BUCKET:
S3 is used to store the data of any type like files, folders, videos, images, CSV files etc.
1. The upload file size should not be exceed 5TB.
2. S3 is also supports versioning that means it will keep the previous version of the file if it’s been modified.
3. S3 is used to host the static web sites.
4. S3 buckets are globally accessible using HTTP protocol.
5. S3 supports various storage classes based on your requirements you can choose it.
6. The reliability of S3 is 99.11 9’s

The five major advantages of S3 are:
1. Availability and durability that means it will not goes down. 
2. Scalability that means you can upload as many files you want to the s3 bucket.
3. Security that means you strict the bucket who can access etc.
4. Cost effective that means based on your requirements you can choose the storage account so that you will not charge more.
5. Performance that means we can easily access this bucket anywhere


Cloud Watch:
Cloud watch is a gatekeeper or watchmen to the aws cloud account and which help in understanding and implementing the monitoring, alerting, reporting and logging. With the help of this features, we can keep track of activities that are happening in aws account.

Monitoring - it is used to see whether how much the resource utilised for example ec2 instance how much cpu it is consumed.

Alerting: it is used to give notification to the user.
In the above example, if the ec2 instance reaches 60 then you want to get notified then you need to do using alerting.

Logging - it will provide the insight to you like when you logged into the instance and etc. This logs are present in the console. 

Custom metrics: if you to get track of the memory of the ec2 instance, it will not provide. If you want to track of this custom things then you need to use custom metrics to send data to the cloud watch.

Lambda:
AWS Lambda is a compute, server less and event driven architecture.

Let’s compare with ec2 instance, ec2 is also compute. You can define the details like size ram etc it will automatically creates and deletes when the job is done. Whereas with the ec2 instance, we have to manual stop the instance.

Most common task using lambda is to remove the unused resources in AWS like removing the snapshots, volumes etc.

Cloud Front:
Cloud Front is used for Content Delivery Network(CDN) that means if the application is deployed in some other region, if you try to access the application you will face latency. To reduce the latency, you have use CDN that will cache the content to your nearest locations. So that you will not face any latency during accessing the application.

Most common use-case is hosting the static website in s3 and accessing through the cloud front.

ECR - Elastic Container Registry:
ECR is a container registry which is used to store the container images in it.

Difference between ECR And DOCKER HUB:
Docker hub is public registry. If you create any repository in it, by default it is in public and only one private repository you can create it for free. For storing non sensitive images which is best option.
ECR is AWS container registry. If you create any repository in it, by default it will in private repository. Which is safe and secure to store the organisation images of the application. If you have an AWS account, you can create the ECR for free.

Secrets Management and Systems Management:
Systems Management:
It is used to store the insensitive information like username, registry url etc.

Secrets Management:
It is used to store the sensitive information like passwords and it will provide you the functionality like rotation that means for every 30 days or some period of time, it will automatically ask you to change the password for the security purposes.

In realtime, it is very good practice to use both pf them together for cost optimisation.

Hashi Corp Vault is also secrets management which is not specific to one cloud provider.

##############################################################################################################################################

1.what is ALB and NLB.
ALB (Application Load Balancer)
Works at Layer 7 (Application Layer) of the OSI model.
ALB is ideal for HTTP/HTTPS traffic and can make decisions based on content (like the URL, headers, or cookies).
It’s typically used for web applications or APIs, where the server needs to route traffic based on specific content within the request.

When to Use ALB:
You have web applications or REST APIs.
You need to make routing decisions based on content of the HTTP request (like URL, headers, etc.).
You want to manage SSL encryption (HTTPS) at the load balancer level.

NLB (Network Load Balancer)
Works at Layer 4 (Transport Layer) of the OSI model.
NLB is ideal for TCP/UDP traffic and handles high-throughput and low-latency use cases.
It is optimized for scenarios where speed and reliability are critical, such as handling large amounts of network traffic or distributing connections to databases or microservices.
TCP/UDP Load Balancing: Routes traffic based on IP address and port.

When to Use NLB:
You need to route TCP/UDP traffic (like database connections, FTP, or gaming).
You require low-latency and high-throughput performance.
You need a static IP for your application or service.

2.what is Elastic BeanStalk.
Elastic Beanstalk is a PAAS i.e Plateform As A Serivce and fully managed service that simplifies the deployment and management of applications in the cloud. It automates the setup of infrastructure and lets you focus on writing code rather than managing servers. With support for a wide range of programming languages, easy scaling, and integrated monitoring, it’s an excellent option for developers who want to quickly deploy and scale web applications and services without worrying about the underlying infrastructure.

How to Deploy an Application on Elastic Beanstalk:
Let’s say you have a simple Node.js application you want to deploy to Elastic Beanstalk.

Create an Application:

Open the Elastic Beanstalk Console in the AWS Management Console.
Click on Create Application and select Node.js as the platform.
Deploy Code:

Package your Node.js app into a .zip file (including your app.js and package.json).
Upload the .zip file through the Elastic Beanstalk console.
Elastic Beanstalk Setup:

Elastic Beanstalk automatically provisions the necessary EC2 instances, a load balancer, and other resources for your app to run.
Monitor and Scale:

Once deployed, you can monitor your app using CloudWatch, view logs, and configure automatic scaling.
Elastic Beanstalk scales your application automatically based on traffic (e.g., adding more EC2 instances if the app gets more requests).

3.What is ECS.
Amazon ECS (Elastic Container Service) is a managed service that makes it easy to run and scale Docker containers in the cloud. It allows you to focus on building your applications while AWS handles the operational overhead of provisioning, scaling, and managing the infrastructure. ECS is highly flexible, supports both EC2 instances and Fargate (serverless containers), and integrates seamlessly with other AWS services. It’s a great choice for running microservices, web applications, and batch jobs in containers without worrying about the underlying infrastructure.

How to Use ECS:
Create a Cluster:

In the ECS console, create a new ECS cluster. You can choose to use EC2 instances or Fargate to run your containers.
Define a Task Definition:

Write a task definition that specifies your container image (e.g., a Docker image for your app) and configuration details like how much CPU and memory it should use.
Run the Task or Service:

Deploy the task by running it directly or as part of a service that will ensure your app is always running.
Monitor and Scale:

Use CloudWatch to monitor your containers’ health, performance, and logs. If needed, ECS can automatically scale your containers up or down.

4.what is RDS.
Amazon RDS (Relational Database Service) is a fully managed service provided by AWS that makes it easy to set up, operate, and scale relational databases in the cloud. A relational database stores data in tables with rows and columns, and it is commonly used for applications that require structured data (like customer information, orders, and transactions).

What Does Amazon RDS Do?
RDS helps you manage relational databases like MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB without the need to worry about the infrastructure, patches, backups, and scaling.
AWS handles the heavy lifting for you, such as:
Provisioning the database instances (virtual servers).
Patching the database software to keep it up-to-date.
Backups and restore capabilities for disaster recovery.
Scaling up the database when traffic increases.

How Amazon RDS Works:
Launch a Database Instance:

You choose a database engine (e.g., MySQL, PostgreSQL), a specific version, and a desired instance size (how much CPU, memory, etc.) for your database.
AWS provisions the database instance, which runs on an EC2 instance, and manages the underlying infrastructure.
Connect to Your Database:

Once your database instance is set up, you can connect to it just like any other database using standard database connection tools and drivers.
Manage Data:

You can perform typical database tasks like creating tables, inserting data, running queries, and more, using SQL or the management tools supported by your database engine.
Backup and Recovery:

RDS automatically takes backups of your database and allows you to restore it to a specific point in time if needed.
Scaling and Maintenance:

You can scale the database up or down without worrying about underlying hardware.
RDS handles software patching and updates to ensure that the database remains secure and up-to-date.

5.what is NAT gateway and explain how to configure it.
How to Configure a NAT Gateway
Follow these steps to configure a NAT Gateway in AWS:

1. Set Up a VPC with Private and Public Subnets
If you don’t already have a VPC, you’ll need to create one with both public and private subnets:

Public Subnet: This subnet will have a route to the Internet Gateway (IGW), allowing resources in this subnet to access the internet.

Private Subnet: This subnet will not have a direct route to the internet.

Steps:

Open the VPC Dashboard in the AWS Management Console.
Create a new VPC (or use an existing one) and ensure it has at least two subnets: one public and one private.
Ensure the public subnet has a route to an Internet Gateway (IGW).
The private subnet will not have any route to the internet by default.
2. Create a NAT Gateway in the Public Subnet
Allocate an Elastic IP (EIP):

In the VPC Console, go to the Elastic IPs section and click Allocate new address.
This EIP will be used by the NAT Gateway for communication with the internet.
Create the NAT Gateway:

Go to the NAT Gateways section in the VPC Dashboard and click Create NAT Gateway.
Choose the Public Subnet where you want the NAT Gateway to reside.
Assign the previously allocated Elastic IP (EIP) to the NAT Gateway.
Click Create NAT Gateway.
3. Update Route Table for Private Subnet
Create or Modify the Route Table for the Private Subnet:

In the VPC Console, go to Route Tables and select the route table associated with the private subnet.
Click Edit Routes, and add a route:
Destination: 0.0.0.0/0 (this represents all internet-bound traffic).
Target: Select the NAT Gateway you just created.
Save the changes.
This route tells AWS that any outbound internet traffic from instances in the private subnet should go through the NAT Gateway.

4. Update Security Groups and Network ACLs
Security Groups: Ensure that the security group for your EC2 instances in the private subnet allows outbound traffic to the internet on necessary ports (e.g., port 80 for HTTP, port 443 for HTTPS).

Network ACLs: Ensure that the Network ACLs for the private and public subnets are configured to allow appropriate traffic. By default, they allow all outbound traffic, but you may need to adjust inbound rules for specific requirements.

5. Verify Configuration
Launch EC2 Instance in Private Subnet: Launch an EC2 instance in the private subnet that does not have a public IP address.

Test Internet Access: SSH into an EC2 instance in the private subnet (via a bastion host or other secure method) and try to access the internet. For example, use ping to test connectivity:

6.launch template vs launch configuration.
Launch Template:
A Launch Template is a more advanced and flexible way to define the configuration of an EC2 instance. It's a newer service that has more features and options compared to Launch Configurations.

Key Features of Launch Template:
More Flexible: You can specify additional options, such as Elastic IPs, instance types, AMI IDs, key pairs, user data, and more, in a single template.
Versioning: Launch Templates allow you to create multiple versions of a template. This means you can have different configurations for your instances, and you can specify which version to use when launching instances. This makes managing instance configurations much easier over time.
Supports More Features: Launch Templates support Spot Instances, Capacity Reservations, and more advanced features.
Used with Both Auto Scaling and EC2 Launching: Launch Templates can be used with Auto Scaling Groups, EC2 RunInstances API, and other EC2 features, giving you more control over instance management.

Launch Configuration:
A Launch Configuration is an older and simpler way to define the configuration of EC2 instances in an Auto Scaling Group.

Key Features of Launch Configuration:
Less Flexible: Launch Configurations are simpler and allow you to define only the essential settings needed to launch an EC2 instance (like instance type, AMI, security groups, and key pairs).
No Versioning: Unlike Launch Templates, Launch Configurations do not support versioning. If you need to make changes to your configuration, you must create a new launch configuration and update the Auto Scaling Group.
No Advanced Features: Launch Configurations do not support more advanced features like Spot Instances, Capacity Reservations, or advanced configurations like Elastic IPs.
Limited to Auto Scaling: Launch Configurations can only be used with Auto Scaling Groups. They cannot be used directly with EC2 instances or other AWS services.

7.stateful vs stateless firewalls in AWS.
Stateful Firewalls (like Security Groups in AWS) track active connections and automatically handle inbound and outbound traffic for established sessions, making them easier to use and manage.
Stateless Firewalls (like NACLs in AWS) treat each packet independently and require explicit rules for both inbound and outbound traffic, offering more control but needing more detailed configuration.

8.what is the standard port of RDP.
standard port is TCP 3389

9.what are types of routing policies in route 53.
Amazon Route 53 offers several types of routing policies to control how traffic is routed to your resources. Here are the main types:

1. Simple Routing
Use case: For a single resource (e.g., a web server).
How it works: Routes traffic to a single record without any conditions. It’s the default routing policy.
2. Weighted Routing
Use case: To distribute traffic across multiple resources based on assigned weights.
How it works: Traffic is split according to the weights you assign to each record. For example, if one resource has a weight of 70 and another has a weight of 30, 70% of the traffic goes to the first resource.
3. Latency Routing
Use case: To route traffic to the resource with the lowest latency.
How it works: Routes traffic to the AWS region with the lowest latency for the user’s location.
4. Failover Routing
Use case: For high availability, routing traffic to a backup resource in case the primary fails.
How it works: Routes traffic to a primary resource and fails over to a secondary resource if the primary becomes unavailable.
5. Geolocation Routing
Use case: To route traffic based on the user's geographic location.
How it works: Routes traffic to different resources based on the country or region of the user’s IP address.
6. Geoproximity Routing
Use case: To route traffic based on both geographic location and resource proximity.
How it works: Routes traffic to resources based on the geographic location of both the user and the AWS resource, with the option to bias traffic to be closer to a specific resource.
7. Multivalue Answer Routing
Use case: To return multiple IP addresses for a DNS query, improving availability and fault tolerance.
How it works: Returns multiple values (IP addresses), and the client can choose one to connect to. It works similarly to weighted routing but with multiple values.
Summary:
Simple: Single resource.
Weighted: Distribute traffic based on weights.
Latency: Route based on lowest latency.
Failover: Failover to a backup if the primary fails.
Geolocation: Route based on geographic location.
Geoproximity: Route based on location and proximity.
Multivalue: Return multiple IPs for fault tolerance.

10.let's have a scenario you are experiencing high cpu utilization on your ec2 instances how do you use cloudwatch to troubleshoot the issue.
Check CPUUtilization metric in CloudWatch to identify if CPU usage is high.
Set up a CloudWatch Alarm for proactive monitoring.
Review other EC2 metrics like disk I/O, network activity, and memory usage.
Analyze EC2 instance logs for application or system issues.
Use CloudWatch Logs Insights to query and analyze detailed logs.
Investigate application-level behavior (using top or htop).
Check for any Auto Scaling or instance health issues.


11. how many objects can a s3 bucket can store

* An S3 bucket can store an unlimited number of objects. There is no hard limit on the total number of files or the total amount of data
  you can store within a single bucket.

* An individual S3 object can range in size from 0 bytes to a maximum of 5 TB  


12. your ec2 instance in a private subnet needs to download packages without NAT gateway. what alternatives exists.

When an EC2 instance in a private subnet needs to download packages without using a NAT Gateway, the most common and
secure alternative is to use VPC Endpoints.

13.how do you setup geographical based routing using aws services

To set up geographic-based routing in AWS, you primarily use Amazon Route 53 with either its Geolocation or Geoproximity routing policies. 
These policies determine which resource to route a user's traffic to based on their geographic location.


14. how do you troubleshoot SSH issues in an ec2 instance.

client-side:

* SSH Command & Key Pair: Ensure you're using the correct SSH command. The most common mistake is using the wrong private key file (.pem)
  or not setting the correct file permissions. The key file must have read-only permissions for the user, which you can set with chmod 400 
  my-key.pem. Also, confirm you're using the correct username for your specific AMI (e.g., ec2-user for Amazon Linux, ubuntu for Ubuntu, centos for CentOS).

* SSH Verbose Output: Run the SSH command with verbose output (ssh -vvv -i my-key.pem ec2-user@your-instance-ip). The output will provide
  detailed information about each step of the connection process, helping you pinpoint the exact point of failure, such as authentication 
  issues or a timeout.

AWS-Side Network Configuration:

* Security Groups: This is the most frequent cause of connection issues

* Network ACLs (NACLs): If Security Groups are correct, check the NACLs for the subnet. Remember that NACLs are stateless, so you need both inbound and outbound rules.

* Public IP Address & Route Tables: Confirm your EC2 instance has a public IP address and is in a public subnet. The subnet's route table
  must have a route to an Internet Gateway to allow external traffic to reach the instance.

Instance-Level Issues:

* Instance State: The instance must be in a running state. If it's stopped, starting it might change its public IP, so be sure to get the new one.

* SSH Daemon (sshd) Service: The SSH service on the instance itself may not be running or is misconfigured. If you can't connect,
  you can't check this directly. This is where other tools come in.


15. what problems occur when two VPC's have overlapping CIDR blocks in AWS  

* Having two VPCs with overlapping CIDR blocks in AWS can cause significant network communication problems. While AWS allows you to create 
  separate VPCs with the same IP range, the issue arises when you try to establish connectivity between them.

* VPC Peering Fails: You cannot create a VPC peering connection between two VPCs if their CIDR blocks overlap. This is a fundamental limitation
  of VPC peering. If you attempt it, the connection will immediately fail.

* Transit Gateway Routing Issues: AWS Transit Gateway, which is designed to connect thousands of VPCs, also has issues with overlapping CIDRs. 
  A Transit Gateway will not propagate routes for VPCs with overlapping CIDR blocks, meaning traffic will not be able to be routed between them.

* Complex Workarounds: To get around these issues, you must implement complex workarounds, such as using Network Address Translation (NAT).
  This involves setting up NAT gateways or instances in each VPC to translate the IP addresses, which adds complexity, management overhead,
  and potential latency. This is generally considered a last resort.


16. how can you enable communication between overlapping CIDR VPC's

 * Communicating between VPCs with overlapping CIDR blocks is complex and is generally not recommended by AWS. Direct methods like VPC Peering
   and AWS Transit Gateway will fail if the CIDRs overlap because the routing becomes ambiguous.

* To enable communication, you must use a method that translates the IP addresses to avoid the conflict. The most common solution is to use
  Network Address Translation (NAT).









  





























