AWS:

1.EC2:
       Elastic Compute Cloud provides web services api for provisioning,managing and deprovisioning virtual server
       inside amazon clouds.

Ec2 pricing:

             1.On-Demand(pay per hour or even seconds)
             2.Reserved(Reserve capacity for 2-3 years for discounts)
             3.Spot(Bid your price for unused ec2 capacity)
             4.Dedicated Hosts(physical servers dedicated for you)



Components inside ec2 instance:
                               
                                1.AMI(Amazon Machine Image):
                                                            AMI provides the information requried to launch an instance
                                                            which is a virtual server in cloud(this like vagrant box list)

                                2.Instance Type:
                                                when you launch an instance,the instance type that you specify determines
                                                the hardware of the host computer used for your instance.
                                                (how much size and memory required)

                                3.EBS(Amazon Elastic Block Store):
                                                                   Amazon ec2 provides you with flexible,cost effective and
                                                                   easy to use data storage options for your instances.

                                                                   EBS is like virtual hard drive in which you can store 
                                                                   your OS and your data.

                               4.Tags:
                                      Tag is simple label consisting of customer-defined keys and optional values that can
                                      make it easier to manage,search for,and filter resources.

                               5.Security Groups:
                                                  A Security Group acts as a firewall that controlls the traffic for one
                                                  or more instances.

                               6.Amazon EC2 uses the public-pair cryptography to encrypt and decrypt the login information.


Note:
     1.In ec2,whenever the ec2 instance stopped,the public ip will be gone and private ip
       will be there.
     2.when you start the ec2 instance again,the new public ip will generate and private ip
       remain same.

     3.if you want static ip(fixed ip) then you can use elastic ip's.In right menu search for
        elastic ip and select the region and click on create
     4.it will create the elastic ip's and we can associate that ip to our ec2 instance.


AWS CLI:
         if you want to connect aws through command line use follwing steps:
         
         1.install aws cli in your machine.
         2.create user or existing user also fine, in user setting search for acess keys
           and create acess keys 
         3.open gitbash and type aws configure, it will ask for acesss key,secret acess key
           output format is json and region. then configuration is done


  aws sts get-caller-identity -> this command shows the account ID and account number you are
                                 using.



EBS(Amazon Elastic Block Store):
                                 Amazon ec2 provides you with flexible,cost effective and
                                 easy to use data storage options for your instances.

                                 EBS is like virtual hard drive in which you can store 
                                 your OS and your data in the form of volumns.
                                
                                 snapshot is backup of a volumn.


          EBS Types:
                     1.General purpose(SSD-Solid State Drive) - most work loads

                     2.Provisioned IOPS - Large Databases.

                     3.Throughput Optimized HD - big data and data warehouse.

                     4.Cold HDD(Hard Disk Drive) - file servers.

                     5.Magnetic - backups and Archieves.


If you want to store anything separately then follow the below steps:
 
 1.first you need to create a volume(choose the size based on your requirment).
 2.Attach that volume to the instance(just make sure that volume and instance is in same zone).
 
 fdisk -l -> which will show all the disks.

 3.volume we have created for this one,partition is not present we need to create it.

             first step is to create the partitioning.

 4.fdisk /dev/xvdf(hard disk path) -> it will open the hard disk utility.
   click m -> for help
 5. n -> to create the partition.and then click enter upto last step.
 6. p -> to print the disk partition.
 7. w -> to write the partitions(partitions are created).

            next step is to formatting it.

8.mkfs and click the tab button 2 times -> it will show all the available utilities.
9.choose any utility you want and mkfs.ext4 /dev/xvdf1(hard disk path) click enter.
10.the partition is formatted with ext4 format.

           next step is to mount it.

11.create a directory  in temp directory and moves all images file to created directory.
   here mkdir /tmp/img-backups
        mv /var/www/html/images/* /tmp/img-backups/
12.now the images directory is empty.
13.This is a temporary mount -> mount /dev/xvdf1(hard disk path) /var/www/html/images/(where you want to mount i.e path).
14. run df -h command to see the mounted.(how much is used and how much is available)
15.if you want unmount it(delete it) -> umount /var/www/html/images/(path)

now lets see the permanent mount

16.open vim /etc/fstab -> add /dev/xvdf1	/var/www/html/images/	ext4	defaults	0 0
17. mount -a -> this will mount all the entries from fstab file.
18. next is to move the images from temp directory to mounted directory.
    mv /tmp/img-backups/* /var/www/html/images/
19.restart the service -> systemctl restart httpd
20.check the status of the service if mounting is failed then the service will not run -> systemctl status httpd.
21.check in the browser whether it shows images or not.



EBS Snapshots:
               Snapshots are usually to backup's and restores the data.

 
              we will use the previous instance for the backup

      for that first, change the name of the instance and deattach the volume from this instance.
      unmount the previous partition -> umount /var/www/html/images/.
	
 create  a new volume and attach that volume to ec2 instance.


 fdisk -l -> which will show all the disks.

 3.volume we have created for this one partition is not present we need to create it.

             first step is to create the partitioning.

 4.fdisk /dev/xvdf(hard disk path) -> it will open the hard disk utility.
   click m -> for help
 5. n -> to create the partition.and then click enter upto last step.
 6. p -> to print the disk partition.
 7. w -> to write the partitions(partitions are created).

            next step is to formatting it.

8.mkfs and click the tab button 2 times -> it will show all the available utilities.
9.choose any utility you want and mkfs.ext4 /dev/xvdf1(hard disk path) click enter.
10.the partition is formatted with ext4 format.
11.mkdir -p /var/lib/mysql -> this is where mysql database is present.

           now mount it.

16.open vim /etc/fstab -> add /dev/xvdf1	/var/lib/mysql	ext4	defaults	0 0
17. mount -a -> this will mount all the entries from fstab file.

18 now lets install mysql -> yum install mariadb-server -y (since it is a centos).
19.start the service -> systemctl start mariadb
20. ls /var/lib/mysql/ -> here you can see the downloaded files.

Note:
      if you want to restore the existing partitioning then snapshot will not do that instead
      of that snapshot will create a volume for it and store the data.


     Snapshots backups and restores:

     if you loose the data and already taken the snapshot then,

                                     1.unmount partition.
                                     2.deattach volume
                                     3.create a new volume from snapshot
                                     4.attach the created volume from snapshot
                                     5.mount it back.

21.create a snapshot from volume(in volume section).
22.go to /var/lib/mysql directory delete the mysql data from it  -> rm -rf *
23.stop the service -> systemctl stop mariadb.
24.unmount the partition -> unmount /var/lib/mysql/
25.deattch the volume(in volume section).
26.go to snapshot section and select the snapshot and click on create volume.
27.now attach the recovered volume to ec2 instance.
28.mount -a -> this will mount all the entries.
29.now check the data whether it came or not -> ls /var/lib/mysql/.




ELB-Elastic Load Balancer:
                          Elastic load balancing distributes incoming applications or network
                          traffic across multiple targets such as ec2 instances or containers etc.

      Elastic load balancer supports 3 types of load balancers:
                     
                                  1.Application load balancer - which supports only web traffic
                                  2.network load balancer - which is very high performing load balancer and expensive too.
                                  3.classic load balancer - which is simplest one.


     Main flow of elastic load balancer is:
                                            1.create a ec2 instance with website.
                                            2.create AMI for that instance.
                                            3.create launch template and launch it.
                                            4.create target groups.
                                            5.create load balancer.



CloudWatch:
            it monitors the performance of aws environment-standard infrastructure metrics.


            Metrics:
                    Aws cloud watch allows you to record metrics for services such as EBS,EC2
                    Amazon RDS,ColudFront etc.

            Events: 
                  Aws events delivers a near real time stream of systems that describe change
                  in amazon web services resources.

            Logs:
                you can use amazon cloudwatch logs to monitor,store and acess your log files
                from amazon ec2 instance(Elastic Compute Cloud) and other resources.

Note:
     1.alarms monitors cloudwatch metrics for instances.

      SNS(Simple Notification Services) is a web service that co-ordinate and manage the
      sending of messages to subscribing endpoints.

 flow:
       ec2 instance ----> amazon cloudwatch------> Alarms------------>SNS(email notifications).



EFS(Amazon Elastic File System):
                                 It is a shared file system for data storage.

    1.It is similar to EBS(Elastic Block Store) the only difference is, in EBS we can store data
      for single ec2 instance only.
    2.But in EFS(Elastic File System) it shared file system i.e common storage for multiple
      ec2 instances.

      flow:
           1.create the ec2 instance
           2.in EFS,create the file sytem
           3.create the acess points to acess that file system
           4.we need to mount this in /etc/fstab file.



Auto Scaling:
              Auto scaling is a service that automatically monitors and adjust compute
              resource to maintain the performance for applications hosted in the aws.


       flow:
            we need to create the auto scaling group which will provide in launch configuration
            template to launch instances based on the load (cpu utilization) alarm will be triggered
            if it cross the threshold and scaling policy will trigger the launch of new instances
            in the auto scaling group or even reduce the instances based on the scaling policies.



Amazon S3(Simple Storage Service):
                                   Amazon S3 is a storage for the internet.you can use
                                   S3 to store and retrieve any amount of data at any time
                                   from anywhere on the web. i.e it is just like google drive.


         S3 Storage Classes:
                             1.S3 Standard: 
                                            General purpose storage of frequently accessed data.
                                            Fast acess and object replication in multi available zones.

                             2.S3 Infrequent Acess:
                                                    Long lived,but less frequently accessed data.
                                                     Slow access,object replication in multi available zones.

                            3.S3 One Zone-Infrequent Access:
                                                             It is for data that is accessed less frequently,
                                                             but requires rapid access when needed.slow access,
                                                             no object replication.

                           4.S3 Inteligent Tiering:
                                                    Automatically moves data to most cost effective tier.

                           5.S3 Glacier:
                                        Low cost storage for data archiving.

                           6. S3 Glacier Deep Archive:
                                                       Lowest cost storage,retrival time of 12hrs.

Note:
      1.By default everything in S3 Bucket is private.
      2.Bucket versioning: 
                           1.If you disable bucket versioning, if you delete anything in the bucket, you can not retrive it.
                           2.If you enable the bucket versioning,you delete anything in bucket we can recover it but the size 
                             of the bucket will grow(size).if you keep on recovering the objects, the size will increases.

      3.while uploading the objects to S3 bucket,you can choose the storage classes.



If you want to host static website in S3 Bucket:
                                                  1.download the static website from tooplate.com
                                                  2.upload the files to S3 Bucket.
                                                  3.In permission,enable the public acess and make the files as public by selecting the files.
                                                  4.enable the static website hosting in properties option.
                                                  5.use the endpoint to display the content in browser(endpoint present static website hosting in properties).

   
    4.By mistakenly you override the object then follow these steps:
                                                                      
        select the file you want to recover and go to versioning there you can find the previous version of your object 
        and download it and then upload the file.make it public then it will display in browser.This is how you can recover
        the objects in S3 bucket.Just make sure to enable bucket versioning.


    5.If you delete the objects in S3 Bucket you want to recover it follow these steps:

       In the S3 Bucket,you will see the list of objects option,you just enable it.there you can see
       the all versions of yours object.The delete files also shown here.select the delete file and click on the delete option
       then it will come to S3 Bucket.


Flow of project setup Vprofile in aws:

                                       1.Login into aws account.
                                       2.Create key pairs.
                                       3.Create security groups.
                                       4.Launch ec2 instances with user data(bash scripts).
                                       5.Update IP to name mapping in route 53.
                                       6.Bulid application from source code.
                                       7.Upload to s3 bucket(refer point 1 to point 3 in note).
                                       8.Download the artifact to tomcat ec2 instances(refer point 4 onwords for this).
                                       9.Setup Elastic Load Balancer(ELB) with https(certificate from amazon certificate manager).
                                       10.Map Elastic Load Balancer endpoint to website name in Godaddy DNS.
                                       11.Verify the entire setup.
                                       12.Bulid auto scaling group for tomcat instances.(1.AMI
                                                                                         2.launch template 
                                                                                         3.autoscaling group).

Note:
      To upload the artifact into amazon S3 bucket follow these steps:
   
       1.first we need to configure the IAM role in AWS CLI(commmand -> aws configure)
       2.create the s3 bucket using command -> aws s3 mb s3://hkh-code-artis(Bucket name).
       3.we copy the artifact to S3 bucket -> aws s3 cp target/vprofile-v2.war s3://hkh-code-artis/

       4.create the role for the s3 bucket and attach that role to the tomcat instance.
       5. aws s3 ls -> it will show all the s3 buckets.
       6.copy the artifact to temp folder -> aws s3://hkh-code-artis/vprofile-v2.war /tmp/
       7.delete the default tomcat page -> rm -rf /var/lib/tomcat9/webapps/ROOT
       8.copy the artifact from temp folder to tomcat default page -> cp /tmp/vprofile-v2.war /var/lib/tomcat9/webapps/ROOT.war




Refactoring the above project:
                          

                                 Comparision


 1.BeanStalk                                    1.Tomacat EC2 instance on vm
 2.ELB in beanstalk                             2.nginx LB/ELB
 3.autoscaling                                  3.autoscaling
 4.EFS/S3                                       4.EFS/S3
 5.RDS                                          5.mysql on ec2 instances.
 6.Elastic cache                                6.memcache on ec2 instance.
 7.Active MQ                                    7.Rabbit MQ on ec2 instance.
 8.Route 53                                     8.Godaddy,local DNS
 9.Cloud Front                                  9.Multi delivery content across world.



Flow of execution:  

                                                           monitors by amazon cloud watch
                                                                               ^
                                                                               |
user ---> Route 53 ---> cloud front ---> application load balancer ---->  ec2 instances in bean stalk ----> stores artifact in s3 bucket.
                                                                               |
                                                                               |
                                    RDS(mysql)<-----Elastic Cache<----- Active MQ
                                                                         

1.Login into aws account.
2.create the key pair for beanstalk instances login.
3.create the security group for Elastic cache,Active MQ and RDS.
4.create
         RDS,Amazon Elastic Cache,Amazon MQ.

5.create Elastic Beanstalk environment.
6.update security group for backend to allow traffic from beanstalk security group.
7.update security group for backend to allow internal traffic.

8.launch EC2 instances for DB initializing.
9.login into the instance and initialize the RDS DB.
10.change the health check on beanstalk to /login.
11.add 443 https listener to ELB.

12.build artifact from backend information.
13.deploy artifact to beanstalk.
14.create CDN(Content Delivery Network) with ssl certificate.	
15.update entry in godaddy DNS Zones.
16.test the url.


Amazon CloudFront:
                   Amazon CloudFront is a content delivery network operated by Amazon Web Services.
                   The content delivery network was created to provide a globally-distributed network of proxy servers
                   to cache content, such as web videos or other bulky media, more locally to consumers,
                   to improve access speed for downloading the content.

Note:
      1.Generally whenver the user request some thing on the browser it will route to S3 since the data is present in S3.
      2.When you use cloudfront,for the first time whenever the user request for data from s3, the data is cached to 
        nearest edge location.so again whenever you request the same data then the data will come from the edge location 
        instead of S3.So that we can able to acess the data very fast.






AWS-part-2:

         VPC(Virtual Private Cloud):
                                     1.vpc is a logical data center within a region.
                                     2.vpc is an on-demand configurable pool of shared computing resources allocated within a public cloud environment.
                                     3.control over network environment,select ip address range,subnets and configure route tables and gateways.

         Subnet Masks:
                       A subnet mask is used to divide an IP address into two parts.
                       One part identifies the host (computer), 
                       the other part identifies the network to which it belongs.
                       To better understand how IP addresses and subnet masks work, look at an IP address and see how it's organized.

                        ex: our ip is 192.168.0.172
                            and subnet mask is 255.255.255.0
                            here 192.168.0.0 is network ip(class c) and last ip is 192.168.0.255 is for broadcast.

                         based on the subnet masks only we can avaiable ips.
                         like start with 192.168.0.1 to 192.168.0.255.here first three octet are fixed because of subnets(255.255.255.0)

         CIDR(Classless Inter-Domain Routing):
                                              Classless Inter-Domain Routing (CIDR) notation is a way to represent an IP address and its network mask.

                                               ex: 192.168.0.172 is network ip                  
                                                   255.255.255.0 is subnet masks

                                          This can be represent as 192.168.0.0/24 which is 255.255.255.0 as subnet

                                                 255.0.0.0
                                                 /8

             this can be represented as          11111111.00000000.00000000.00000000
                                           
                                                 255.255.0.0
                                                 /16

                                                 11111111.11111111.00000000.00000000

                                                 255.255.255.0
                                                 /24
                         
                                                 11111111.11111111.11111111.00000000


         VPC design and components:
                                     vpc is divide into two parts.one is public subnet and another is private subnet.

                                     public subnet is connected to internet gateway and private subnet is connected to NAT gateway present inside public subnet.

         NAT gateway:
                      Network Address Translation(NAT) gateway to enable instances in a private subnet to connect to the internet or other aws resources. 

         Internet Gateway:
                           An internet gateway is horizontally scaled,redudant and 
                           highly available VPC components that allows communication between instances in your vpc and the internet.

        Bastion Host:
                      Bastion host is the single point entry to access your resources in the private subnet.

        VPC Peering:
                     A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses.
                     Instances in either VPC can communicate with each other as if they are within the same network.

        Note:
              1.we can not create a default VPC in aws.
              2.to copy key from one instance to another instance through terminal
                        
                      -> scp -i Downloads/bostion-key.pem Downloads/web-key.pem ubuntu@52.53.153.72:/home/ubuntu/
                         here web-key.pem file is copied into the bostion host instance in /home/ubuntu/ directory.

      flow for creating VPC:
                              1.create VPC
                              2.create subnets(public subnets & private subnets)
                              3.create internet gateway.
                              4.create NAT gateway and before that create Elastic Ip and attach that to NAT gateway.
                              5.create Route Tables and Add the subnet association and Route(public subnet attach to internet gateway)
                                                                                             & private subnet attach to NAT gateway
                              6.Go to subnets and click on action, click on edit subnet settings and Enable auto-assign public IPv4 address.
                                do this for all public subnets.

                              7.Go to your VPC and click on action,click on edit VPC settings and Enable DNS hostnames. 
                             
                              8.create one ec2 instance(Bostion host) and while creating ec2 instance select the created vpc and choose any public subnet in subnet.

                              9.now we will create one ec2 instance and deploy one website in ec2 instance in private subnet and that can be access through bastion host.

                              10.to copy key from  instance(website instance) to another instance(bastion host) through terminal
                        
                                   -> scp -i Downloads/bostion-key.pem Downloads/web-key.pem ubuntu@52.53.153.72:/home/ubuntu/
                                      here web-key.pem file is copied into the bostion host instance in /home/ubuntu/ directory.

                                        chmod 400 web-key.pem (file permission).

                              11.create the elastic load balancer and before that first create the target groups then you can access the website using load balancer DNS name.


EC2 Logs:
         1. common way is achieve the logs and send it to somewhere out of the system and then clean the logs.

steps:
       1.create ec2 instance and deploy website on it.So that logs will generate.
       2.ssh to instance and go to logs path and tar the logs files.
       3.clear the logs by cat /dev/null > acess_log(log filename)
       4.create the s3 bucket and we can move the tar file into s3 bucket.
       5.for that first, create IAM user and create access key and secret key.
       6.ssh to instance and install awscli
       7.configure aws, provide access key and secret key,region and output format as json->aws configure
       8.copy the tar file into s3 bucket -> aws s3 cp fullpath(including filename) s3://bucketname/

                                or

            aws s3 sync sourcepath s3://bucketname/

        NOTE: instead point 5,6 simply you can use -> 1.create a role and select the policy you want,in our case it is AmazonS3fullAccess
                                                      2.attach this role to ec2 instance.click action -> security -> modify IAM role.

 
       2.To stream logs live:


            steps:
                    1.create ec2 instance and deploy website on it.So that logs will generate.
                    2.create a role and select the policy you want,in our case it is Amazoncloudwatchlogfullaccess
                    3.attach this role to ec2 instance.click action -> security -> modify IAM role.
                    4.ssh to instance and install awscli and aws logs(this cloudwatch agent takes logs from our log file that we specify 
                      and stream it to cloudwatch
                    5.edit the file, at the end add the file our log file path(/var/log/httpd/access_log) -> vim /etc/awslogs/awslogs.conf
                    6.restart the awslogs service -> systemctl restart awslogsd and  systemctl enabled awslogsd
                    7.check in the cloudwatch, logs will generate over here.

                     if you want to more specifically send this log file to cloudwatch then
                           1.copy the log path -> /var/log/httpd/access_log
                           2.edit the file, at the end add the file our log file path(/var/log/httpd/access_log) -> vi /var/awslogs/etc/awslogs.conf

                   8.from cloudwatch, you can export logs to s3 bucket.


=================================================================================================================================================================


AWS:

VPC:
Let’s see how VPC works:
1. The user sends requests from the browser and it will goes to the VPC.
2. In the VPC, that request is received by the internet gateway and internet gateway to public subnet which is single public subnet in the VPC.
3. From the public subnet to elastic load balancer it will pass.
4. In elastic load balancer has the target group through that it will transfer the request with the help of route table , it will reach to the subnet where the application is deployed.
5. To enter into the application in ec2 , security group should allow that request. And then only request will be processed.

In simple, this from internet to vpc
User sends request -> internet gateway -> public subnet -> elastic load balancer -> route table -> security groups -> ec2 instance in subnet.

If you want to access anything from the internet then the private subnet should not allow it’s ip to the internet. For that, it follows the masking of ip through router or elastic load balancer. If that masking is done using elastic load balancer then it is called as SNAT gateway. If that masking is done using router then it is called as NAT gateway.

SECURITY GROUPS AND NACL:

Security groups:
Security groups are applied at instance level to only allow the traffic to the instance like ec2 instance.

NACL: NACL stands for Network Access Control List
Which is used at the subnet level and is used to allow and deny what kinds of traffic that is incoming.

Difference between Security groups and NACL:
   1. security groups applied at instance level.
2. NACL is applied at subnet level
3. Security groups are  used to allow traffic only.
4. NACL is used to allow and deny the traffic.

Consider this scenario, for ec2 instance security groups are used to allow what kind of traffic should access the instance. Where as NACL, the ec2 instance that is placed inside the subnet. The NACL should allow or deny the traffic and it allows to act as extra layer of security to it.

Let’s see there are 50 ec2 instance are placed inside the subnet and you want to control the traffic instead of creating the security groups for every ec2 instance, you can create the NACL and it will applied to subnet that means it will apply automatically for all the instance that are present inside it. We can save a lot of time.

Route53:
Route53 is DNS solution . DNS stands for Domain Name System.
DNS is used to map the ip address to the Name.
Like flipkart.com, amazon.com etc.
Instead of using the ip address, we are using the names to access the application because we cannot remember the ip address but we can easily remember the names.

They are two ways to configure the DNS:
1. In Route53, you can purchase the domain and create the hosted zones. In Hosted zones we will create the records that will map the ip address to names.
2. You can purchase the domain from outside like godaddy.com and you can create the hosted zones in the Route53.

NOTE:
Bastion host is used to access the resources that are present in the private subnet i.e ec2

If you want to create the auto scaling groups then first you need to create the template for it in that you need to mention the details like ami, instance type, in network create the security group and then allow the port whatever you want and in VPC, select the VPC we have created and then launch it.

Let’s see how we can create the Boston host:
1. Just create the ec2 instance and in that place this in VPC that we have created and allow ssh to this and also enable public ip to it.
2. We need to ssh to it and from this to, we need to ssh to the ec2 that is present in the private subnet and also we need to export the key pair to the Boston host because to ssh to it we need key pair.
3. how to copy file to remote server from local
4. scp -i path of Boston key pair path of private ec2 instance key pair username@public ip:path where you want to paste the keys.
5. Login into Bostion host and check whether the keys are present or not.
6. ssh to the private ec2 instance using private ip. You can successfully login into that.

In simply, if you want to create the autoscaling group then you need to create the launch template.
If you want to create the load balancer then you need to create the target groups.

# Scenario Based Interview Questions on EC2, IAM and VPC


Q: You have been assigned to design a VPC architecture for a 2-tier application. The application needs to be highly available and scalable. 
   How would you design the VPC architecture?

A: In this scenario, I would design a VPC architecture in the following way.
   I would create 2 subnets: public and private. The public subnet would contain the load balancers and be accessible from the internet. The private subnet would host the application servers. 
   I would distribute the subnets across multiple Availability Zones for high availability. Additionally, I would configure auto scaling groups for the application servers.

Q: Your organization has a VPC with multiple subnets. You want to restrict outbound internet access for resources in one subnet, but allow outbound internet access for resources in another subnet. How would you achieve this?

A: To restrict outbound internet access for resources in one subnet, we can modify the route table associated with that subnet. In the route table, we can remove the default route (0.0.0.0/0) that points to an internet gateway. 
   This would prevent resources in that subnet from accessing the internet. For the subnet where outbound internet access is required, we can keep the default route pointing to the internet gateway.

Q: You have a VPC with a public subnet and a private subnet. Instances in the private subnet need to access the internet for software updates. How would you allow internet access for instances in the private subnet?

A: To allow internet access for instances in the private subnet, we can use a NAT Gateway or a NAT instance. 
   We would place the NAT Gateway/instance in the public subnet and configure the private subnet route table to send outbound traffic to the NAT Gateway/instance. This way, instances in the private subnet can access the internet through the NAT Gateway/instance.

Q: You have launched EC2 instances in your VPC, and you want them to communicate with each other using private IP addresses. What steps would you take to enable this communication?

A: By default, instances within the same VPC can communicate with each other using private IP addresses. 
  To ensure this communication, we need to make sure that the instances are launched in the same VPC and are placed in the same subnet or subnets that are connected through a peering connection or a VPC peering link. 
  Additionally, we should check the security groups associated with the instances to ensure that the necessary inbound and outbound rules are configured to allow communication between them.

Q: You want to implement strict network access control for your VPC resources. How would you achieve this?

A: To implement granular network access control for VPC resources, we can use Network Access Control Lists (ACLs). 
  NACLs are stateless and operate at the subnet level. We can define inbound and outbound rules in the NACLs to allow or deny traffic based on source and destination IP addresses, ports, and protocols. 
  By carefully configuring NACL rules, we can enforce fine-grained access control for traffic entering and leaving the subnets.

Q: Your organization requires an isolated environment within the VPC for running sensitive workloads. How would you set up this isolated environment?

A: To set up an isolated environment within the VPC, we can create a subnet with no internet gateway attached. 
   This subnet, known as an "isolated subnet," will not have direct internet connectivity. We can place the sensitive workloads in this subnet, ensuring that they are protected from inbound and outbound internet traffic. 
   However, if these workloads require outbound internet access, we can set up a NAT Gateway or NAT instance in a different subnet and configure the isolated subnet's route table to send outbound traffic through the NAT Gateway/instance.

Q: Your application needs to access AWS services, such as S3 securely within your VPC. How would you achieve this?

A: To securely access AWS services within the VPC, we can use VPC endpoints. VPC endpoints allow instances in the VPC to communicate with AWS services privately, without requiring internet gateways or NAT gateways. 
  We can create VPC endpoints for specific AWS services, such as S3 and DynamoDB, and associate them with the VPC. 
  This enables secure and efficient communication between the instances in the VPC and the AWS services.

Q: What is the difference between NACL and Security groups ? Explain with a use case ?

A: For example, I want to design a security architecture, I would use a combination of NACLs and security groups. At the subnet level, I would configure NACLs to enforce inbound and outbound traffic restrictions based on source and destination IP addresses, ports, and protocols. NACLs are stateless and can provide an additional layer of defense by filtering traffic at the subnet boundary.
  At the instance level, I would leverage security groups to control inbound and outbound traffic. Security groups are stateful and operate at the instance level. By carefully defining security group rules, I can allow or deny specific traffic to and from the instances based on the application's security requirements.
  By combining NACLs and security groups, I can achieve granular security controls at both the network and instance level, providing defense-in-depth for the sensitive application.

Q: What is the difference between IAM users, groups, roles and policies ?

A: IAM User: An IAM user is an identity within AWS that represents an individual or application needing access to AWS resources. IAM users have permanent long-term credentials, such as a username and password, or access keys (Access Key ID and Secret Access Key). IAM users can be assigned directly to IAM policies or added to IAM groups for easier management of permissions.
   IAM Role: An IAM role is similar to an IAM user but is not associated with a specific individual. Instead, it is assumed by entities such as IAM users, applications, or services to obtain temporary security credentials. IAM roles are useful when you want to grant permissions to entities that are external to your AWS account or when you want to delegate access to AWS resources across accounts. IAM roles have policies attached to them that define the permissions granted when the role is assumed.
   IAM Group: An IAM group is a collection of IAM users. By organizing IAM users into groups, you can manage permissions collectively. IAM groups make it easier to assign permissions to multiple users simultaneously. Users within an IAM group inherit the permissions assigned to that group. For example, you can create a "Developers" group and assign appropriate policies to grant permissions required for developers across your organization.
   IAM Policy: An IAM policy is a document that defines permissions and access controls in AWS. IAM policies can be attached to IAM users, IAM roles, and IAM groups to define what actions can be performed on which AWS resources. IAM policies use JSON (JavaScript Object Notation) syntax to specify the permissions and can be created and managed independently of the users, roles, or groups. IAM policies consist of statements that include the actions allowed or denied, the resources on which the actions can be performed, and any additional conditions.

Q: You have a private subnet in your VPC that contains a number of instances that should not have direct internet access. However, you still need to be able to securely access these instances for administrative purposes. How would you set up a bastion host to facilitate this access?

A: To securely access the instances in the private subnet, you can set up a bastion host (also known as a jump host or jump box). The bastion host acts as a secure entry point to your private subnet. Here's how you can set up a bastion host:
      Create a new EC2 instance in a public subnet, which will serve as the bastion host. Ensure that this instance has a public IP address or is associated with an Elastic IP address for persistent access.
      Configure the security group for the bastion host to allow inbound SSH (or RDP for Windows) traffic from your IP address or a restricted range of trusted IP addresses. This limits access to the bastion host to authorized administrators only.
      Place the instances in the private subnet and configure their security groups to allow inbound SSH (or RDP) traffic from the bastion host security group.
      SSH (or RDP) into the bastion host using your private key or password. From the bastion host, you can then SSH (or RDP) into the instances in the private subnet using their private IP addresses.


S3 BUCKET:
S3 is used to store the data of any type like files, folders, videos, images, CSV files etc.
1. The upload file size should not be exceed 5TB.
2. S3 is also supports versioning that means it will keep the previous version of the file if it’s been modified.
3. S3 is used to host the static web sites.
4. S3 buckets are globally accessible using HTTP protocol.
5. S3 supports various storage classes based on your requirements you can choose it.
6. The reliability of S3 is 99.11 9’s

The five major advantages of S3 are:
1. Availability and durability that means it will not goes down. 
2. Scalability that means you can upload as many files you want to the s3 bucket.
3. Security that means you strict the bucket who can access etc.
4. Cost effective that means based on your requirements you can choose the storage account so that you will not charge more.
5. Performance that means we can easily access this bucket anywhere


Cloud Watch:
Cloud watch is a gatekeeper or watchmen to the aws cloud account and which help in understanding and implementing the monitoring, alerting, reporting and logging. With the help of this features, we can keep track of activities that are happening in aws account.

Monitoring - it is used to see whether how much the resource utilised for example ec2 instance how much cpu it is consumed.

Alerting: it is used to give notification to the user.
In the above example, if the ec2 instance reaches 60 then you want to get notified then you need to do using alerting.

Logging - it will provide the insight to you like when you logged into the instance and etc. This logs are present in the console. 

Custom metrics: if you to get track of the memory of the ec2 instance, it will not provide. If you want to track of this custom things then you need to use custom metrics to send data to the cloud watch.

Lambda:
AWS Lambda is a compute, server less and event driven architecture.

Let’s compare with ec2 instance, ec2 is also compute. You can define the details like size ram etc it will automatically creates and deletes when the job is done. Whereas with the ec2 instance, we have to manual stop the instance.

Most common task using lambda is to remove the unused resources in AWS like removing the snapshots, volumes etc.

Cloud Front:
Cloud Front is used for Content Delivery Network(CDN) that means if the application is deployed in some other region, if you try to access the application you will face latency. To reduce the latency, you have use CDN that will cache the content to your nearest locations. So that you will not face any latency during accessing the application.

Most common use-case is hosting the static website in s3 and accessing through the cloud front.

ECR - Elastic Container Registry:
ECR is a container registry which is used to store the container images in it.

Difference between ECR And DOCKER HUB:
Docker hub is public registry. If you create any repository in it, by default it is in public and only one private repository you can create it for free. For storing non sensitive images which is best option.
ECR is AWS container registry. If you create any repository in it, by default it will in private repository. Which is safe and secure to store the organisation images of the application. If you have an AWS account, you can create the ECR for free.

Secrets Management and Systems Management:
Systems Management:
It is used to store the insensitive information like username, registry url etc.

Secrets Management:
It is used to store the sensitive information like passwords and it will provide you the functionality like rotation that means for every 30 days or some period of time, it will automatically ask you to change the password for the security purposes.

In realtime, it is very good practice to use both pf them together for cost optimisation.

Hashi Corp Vault is also secrets management which is not specific to one cloud provider.

##############################################################################################################################################

1.what is ALB and NLB.
ALB (Application Load Balancer)
Works at Layer 7 (Application Layer) of the OSI model.
ALB is ideal for HTTP/HTTPS traffic and can make decisions based on content (like the URL, headers, or cookies).
It’s typically used for web applications or APIs, where the server needs to route traffic based on specific content within the request.

When to Use ALB:
You have web applications or REST APIs.
You need to make routing decisions based on content of the HTTP request (like URL, headers, etc.).
You want to manage SSL encryption (HTTPS) at the load balancer level.

NLB (Network Load Balancer)
Works at Layer 4 (Transport Layer) of the OSI model.
NLB is ideal for TCP/UDP traffic and handles high-throughput and low-latency use cases.
It is optimized for scenarios where speed and reliability are critical, such as handling large amounts of network traffic or distributing connections to databases or microservices.
TCP/UDP Load Balancing: Routes traffic based on IP address and port.

When to Use NLB:
You need to route TCP/UDP traffic (like database connections, FTP, or gaming).
You require low-latency and high-throughput performance.
You need a static IP for your application or service.

2.what is Elastic BeanStalk.
Elastic Beanstalk is a PAAS i.e Plateform As A Serivce and fully managed service that simplifies the deployment and management of applications in the cloud. It automates the setup of infrastructure and lets you focus on writing code rather than managing servers. With support for a wide range of programming languages, easy scaling, and integrated monitoring, it’s an excellent option for developers who want to quickly deploy and scale web applications and services without worrying about the underlying infrastructure.

How to Deploy an Application on Elastic Beanstalk:
Let’s say you have a simple Node.js application you want to deploy to Elastic Beanstalk.

Create an Application:

Open the Elastic Beanstalk Console in the AWS Management Console.
Click on Create Application and select Node.js as the platform.
Deploy Code:

Package your Node.js app into a .zip file (including your app.js and package.json).
Upload the .zip file through the Elastic Beanstalk console.
Elastic Beanstalk Setup:

Elastic Beanstalk automatically provisions the necessary EC2 instances, a load balancer, and other resources for your app to run.
Monitor and Scale:

Once deployed, you can monitor your app using CloudWatch, view logs, and configure automatic scaling.
Elastic Beanstalk scales your application automatically based on traffic (e.g., adding more EC2 instances if the app gets more requests).

3.What is ECS.
Amazon ECS (Elastic Container Service) is a managed service that makes it easy to run and scale Docker containers in the cloud. It allows you to focus on building your applications while AWS handles the operational overhead of provisioning, scaling, and managing the infrastructure. ECS is highly flexible, supports both EC2 instances and Fargate (serverless containers), and integrates seamlessly with other AWS services. It’s a great choice for running microservices, web applications, and batch jobs in containers without worrying about the underlying infrastructure.

How to Use ECS:
Create a Cluster:

In the ECS console, create a new ECS cluster. You can choose to use EC2 instances or Fargate to run your containers.
Define a Task Definition:

Write a task definition that specifies your container image (e.g., a Docker image for your app) and configuration details like how much CPU and memory it should use.
Run the Task or Service:

Deploy the task by running it directly or as part of a service that will ensure your app is always running.
Monitor and Scale:

Use CloudWatch to monitor your containers’ health, performance, and logs. If needed, ECS can automatically scale your containers up or down.

4.what is RDS.
Amazon RDS (Relational Database Service) is a fully managed service provided by AWS that makes it easy to set up, operate, and scale relational databases in the cloud. A relational database stores data in tables with rows and columns, and it is commonly used for applications that require structured data (like customer information, orders, and transactions).

What Does Amazon RDS Do?
RDS helps you manage relational databases like MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB without the need to worry about the infrastructure, patches, backups, and scaling.
AWS handles the heavy lifting for you, such as:
Provisioning the database instances (virtual servers).
Patching the database software to keep it up-to-date.
Backups and restore capabilities for disaster recovery.
Scaling up the database when traffic increases.

How Amazon RDS Works:
Launch a Database Instance:

You choose a database engine (e.g., MySQL, PostgreSQL), a specific version, and a desired instance size (how much CPU, memory, etc.) for your database.
AWS provisions the database instance, which runs on an EC2 instance, and manages the underlying infrastructure.
Connect to Your Database:

Once your database instance is set up, you can connect to it just like any other database using standard database connection tools and drivers.
Manage Data:

You can perform typical database tasks like creating tables, inserting data, running queries, and more, using SQL or the management tools supported by your database engine.
Backup and Recovery:

RDS automatically takes backups of your database and allows you to restore it to a specific point in time if needed.
Scaling and Maintenance:

You can scale the database up or down without worrying about underlying hardware.
RDS handles software patching and updates to ensure that the database remains secure and up-to-date.

5.what is NAT gateway and explain how to configure it.
How to Configure a NAT Gateway
Follow these steps to configure a NAT Gateway in AWS:

1. Set Up a VPC with Private and Public Subnets
If you don’t already have a VPC, you’ll need to create one with both public and private subnets:

Public Subnet: This subnet will have a route to the Internet Gateway (IGW), allowing resources in this subnet to access the internet.

Private Subnet: This subnet will not have a direct route to the internet.

Steps:

Open the VPC Dashboard in the AWS Management Console.
Create a new VPC (or use an existing one) and ensure it has at least two subnets: one public and one private.
Ensure the public subnet has a route to an Internet Gateway (IGW).
The private subnet will not have any route to the internet by default.
2. Create a NAT Gateway in the Public Subnet
Allocate an Elastic IP (EIP):

In the VPC Console, go to the Elastic IPs section and click Allocate new address.
This EIP will be used by the NAT Gateway for communication with the internet.
Create the NAT Gateway:

Go to the NAT Gateways section in the VPC Dashboard and click Create NAT Gateway.
Choose the Public Subnet where you want the NAT Gateway to reside.
Assign the previously allocated Elastic IP (EIP) to the NAT Gateway.
Click Create NAT Gateway.
3. Update Route Table for Private Subnet
Create or Modify the Route Table for the Private Subnet:

In the VPC Console, go to Route Tables and select the route table associated with the private subnet.
Click Edit Routes, and add a route:
Destination: 0.0.0.0/0 (this represents all internet-bound traffic).
Target: Select the NAT Gateway you just created.
Save the changes.
This route tells AWS that any outbound internet traffic from instances in the private subnet should go through the NAT Gateway.

4. Update Security Groups and Network ACLs
Security Groups: Ensure that the security group for your EC2 instances in the private subnet allows outbound traffic to the internet on necessary ports (e.g., port 80 for HTTP, port 443 for HTTPS).

Network ACLs: Ensure that the Network ACLs for the private and public subnets are configured to allow appropriate traffic. By default, they allow all outbound traffic, but you may need to adjust inbound rules for specific requirements.

5. Verify Configuration
Launch EC2 Instance in Private Subnet: Launch an EC2 instance in the private subnet that does not have a public IP address.

Test Internet Access: SSH into an EC2 instance in the private subnet (via a bastion host or other secure method) and try to access the internet. For example, use ping to test connectivity:

6.launch template vs launch configuration.
Launch Template:
A Launch Template is a more advanced and flexible way to define the configuration of an EC2 instance. It's a newer service that has more features and options compared to Launch Configurations.

Key Features of Launch Template:
More Flexible: You can specify additional options, such as Elastic IPs, instance types, AMI IDs, key pairs, user data, and more, in a single template.
Versioning: Launch Templates allow you to create multiple versions of a template. This means you can have different configurations for your instances, and you can specify which version to use when launching instances. This makes managing instance configurations much easier over time.
Supports More Features: Launch Templates support Spot Instances, Capacity Reservations, and more advanced features.
Used with Both Auto Scaling and EC2 Launching: Launch Templates can be used with Auto Scaling Groups, EC2 RunInstances API, and other EC2 features, giving you more control over instance management.

Launch Configuration:
A Launch Configuration is an older and simpler way to define the configuration of EC2 instances in an Auto Scaling Group.

Key Features of Launch Configuration:
Less Flexible: Launch Configurations are simpler and allow you to define only the essential settings needed to launch an EC2 instance (like instance type, AMI, security groups, and key pairs).
No Versioning: Unlike Launch Templates, Launch Configurations do not support versioning. If you need to make changes to your configuration, you must create a new launch configuration and update the Auto Scaling Group.
No Advanced Features: Launch Configurations do not support more advanced features like Spot Instances, Capacity Reservations, or advanced configurations like Elastic IPs.
Limited to Auto Scaling: Launch Configurations can only be used with Auto Scaling Groups. They cannot be used directly with EC2 instances or other AWS services.

7.stateful vs stateless firewalls in AWS.
Stateful Firewalls (like Security Groups in AWS) track active connections and automatically handle inbound and outbound traffic for established sessions, making them easier to use and manage.
Stateless Firewalls (like NACLs in AWS) treat each packet independently and require explicit rules for both inbound and outbound traffic, offering more control but needing more detailed configuration.

8.what is the standard port of RDP.
standard port is TCP 3389

9.what are types of routing policies in route 53.
Amazon Route 53 offers several types of routing policies to control how traffic is routed to your resources. Here are the main types:

1. Simple Routing
Use case: For a single resource (e.g., a web server).
How it works: Routes traffic to a single record without any conditions. It’s the default routing policy.
2. Weighted Routing
Use case: To distribute traffic across multiple resources based on assigned weights.
How it works: Traffic is split according to the weights you assign to each record. For example, if one resource has a weight of 70 and another has a weight of 30, 70% of the traffic goes to the first resource.
3. Latency Routing
Use case: To route traffic to the resource with the lowest latency.
How it works: Routes traffic to the AWS region with the lowest latency for the user’s location.
4. Failover Routing
Use case: For high availability, routing traffic to a backup resource in case the primary fails.
How it works: Routes traffic to a primary resource and fails over to a secondary resource if the primary becomes unavailable.
5. Geolocation Routing
Use case: To route traffic based on the user's geographic location.
How it works: Routes traffic to different resources based on the country or region of the user’s IP address.
6. Geoproximity Routing
Use case: To route traffic based on both geographic location and resource proximity.
How it works: Routes traffic to resources based on the geographic location of both the user and the AWS resource, with the option to bias traffic to be closer to a specific resource.
7. Multivalue Answer Routing
Use case: To return multiple IP addresses for a DNS query, improving availability and fault tolerance.
How it works: Returns multiple values (IP addresses), and the client can choose one to connect to. It works similarly to weighted routing but with multiple values.
Summary:
Simple: Single resource.
Weighted: Distribute traffic based on weights.
Latency: Route based on lowest latency.
Failover: Failover to a backup if the primary fails.
Geolocation: Route based on geographic location.
Geoproximity: Route based on location and proximity.
Multivalue: Return multiple IPs for fault tolerance.

10.let's have a scenario you are experiencing high cpu utilization on your ec2 instances how do you use cloudwatch to troubleshoot the issue.
Check CPUUtilization metric in CloudWatch to identify if CPU usage is high.
Set up a CloudWatch Alarm for proactive monitoring.
Review other EC2 metrics like disk I/O, network activity, and memory usage.
Analyze EC2 instance logs for application or system issues.
Use CloudWatch Logs Insights to query and analyze detailed logs.
Investigate application-level behavior (using top or htop).
Check for any Auto Scaling or instance health issues.


























