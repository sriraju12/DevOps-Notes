Kubernetes:
            Kubernetes automates operational tasks of container management and includes built-in commands for deploying applications,
            rolling out changes to your applications, scaling your applications up and down to fit changing needs, monitoring your applications,
            and more—making it easier to manage applications.

            * While Docker is a container runtime, Kubernetes is a platform for running and managing containers from many container runtimes.


    Master:kube API server:
                             * main hero, handles all the requests and enables communications across stack services.
                             * component in the master that exposes the kubernetes API.
                             * it is the front-end for the kubernetes control plane.
                             * admin connects to it using kubectl CLI
                             * web dashboard can be integrated with this API.


   Master:ETCD server:
                        * stores all the information.
                        * consistent and high available key value store used as kubernetes backing store for all the cluster data
                        * kube API stores retrieves info from it.
                        * should be backup regularly.
                        * stores current state of everything in the cluster.


   Master:kube scheduler:
                           * watches newly created prods that have no node assigned and select a node from them to run on.


  Master : controller manager:
                                * logically, each controller is a separate process.
                                * to reduce complexity,they are all compiled into a single binary and run in a single process.
                                * these controllers include:
                                                              1.Node controller:
                                                                                 responsible for noticing and responding when nodes go down.
                                                              2.Replication controller:
                                                                                       responsible for maintaining the correct number of pods for 
                                                                                       every replication controller object in the system.
                                                              3.Endpoints controller:
                                                                                       populates the endpoints object(i.e joins services and pods)
                                                              4.Service Account and Token controller:
                                                                                                      create default accounts and api access tokens
                                                                                                      for new namespace. 


    worker: kublet:
                    an agent that runs on each node in the cluster.it makes sure that containers are running in a pod.

    worker: kube proxy:
                        * network proxy that runs on each node in your cluster.
                        * network rule:
                                      rules allow network communication to your pods inside or outside of your cluster.

    worker: container runtime: kubernetes supports several container runtime,
                                                                                * Docker
                                                                                * containerd
                                                                                * kubernetes CRI(container runtime interface)

minikube setup:
                1.open powershell as admin
                2.setup chocolaty
                3.install minikube with chocolaty( choco install minikube kubernetes-cli)
                4.open powershell and run(minikube start)
                5.minikube configuration is present in the current working directory(.kube/config)


kops setup:
            1.domain for Kubernetes DNS records(i.e GoDaddy.com)
            2.create a Linux vm and setup(kops,kubectl,ssh keys,awscli)
            3.login to aws account and setup(s3 bucket,IAM user for awscli,route53 hosted zone)
            4.login to domain registry(GoDaddy.com) and create NS records for subdomain pointing to route53 hosted zone NS servers.

note:
      kubectl -> Kubernetes.io/docs/tasks/install-kubectl
      kops -> GitHub.com/Kubernetes/kops/releases


nslookup -type=ns domainname(here kopskubevpro.hkhpro.life) -> it will list all the nameservers.
               


* command for storing the configuration for creating the cluster in s3 bucket:

kops create cluster --name=kopskubevpro.hkhpro.life(domain-name) \        here backward slash means continue command in nextline
--state=s3://kopsbucket1237(s3 bucket-name) --zone=us-east-1a,us-east-1b \
--node-count=2 --node-size=t3.small --master-size=t3.medium --dns-zone=kopskubevpro.hkhpro.life(domain-name) \
--node-volume-size=8 --master-volume-size=8

* if you make any changes to kops configuration then you update the changes by running the command:

     kops update cluster --name kopskubevpro.hkhpro.life(domain-name) --state=s3://kopsbucket1237(s3 bucket-name) --yes --admin

* kops validate cluster --state=s3://kopsbucket1237(s3 bucket-name) -> it will show that your cluster is ready 

* kubectl get nodes -> it will show all the nodes.

* kops delete cluster --name=kopskubevpro.hkhpro.life --state=s3://kopsbucket1237(s3 bucket-name) --yes -> it will delete the cluster



kubernates objects:
                     1.pod -> containers are present inside the pod
                     2.service -> to have an static endpoint to the pod like load balancer.
                     3.replica set -> to create the cluster of pods.
                     4.deployment -> which work similar to replica set and you can deploy new image tag by using deployment(most used)
                     5.config map -> to store our variables and configuration
                     6.secret -> variable and some information to store that are not in clear text.
                     7.volumes -> we can attach different types volumes to pod.



KubeConfig file:
                  use kubeconfig file to organize information about
                                                                     1.clusters
                                                                     2.users
                                                                     3.Namespaces
                                                                     4.Authencation mechanisms 

Namespaces:
            In Kubernetes, namespaces provide a mechanism for isolating groups of resources within a single cluster.
            Names of resources need to be unique within a namespace, but not across namespaces 

            * kubectl get ns -> it will display all the namespaces    
            * kubectl get all -> it will display all the objects that are present in the default namespaces.
            * kubectl get all --all-namespaces -> it will display all the objects of all namespaces. 
            * kubectl create ns kubekart(namespace-name) -> it will create the namespace with the name kubekart.
            * kubectl run ngnix --image=nginx --namespace=kubekart -> it will run the nginx in the namespace.



Pods:
       pod is the basic execution unit of a kubenetes application the smallest and simplest unit in the Kubernetes object model that you 
       create or deploy.A pod represents processes running on your cluster. 

       * Pods that run a single container:
                                           1.The one-container-per-pod model is the most common Kubernetes use case.
                                           2.Pod as a wrapper around a single container.
                                           3.Kubernetes manages the pods rather than containers directly.

      * Multi container pod:
                             1.Tightly coupled and need to share resources.
                             2.One main container and other as sidecar(supporting containers) or init container.
                             3.Each pod is meant to run a single instance of a given application(for example run only tomcat server).
                             4.Should use multiple pods to scale horizontally.

     * kubectl create -f pod-setup.yml -> to create the pod using yaml file
   
                the yaml file looks like this,      apiVersion: v1
                                                    kind: Pod
                                                    metadata:
                                                      name: webapp-pod(pod-name)
                                                      labels:                      //labels are like tags
                                                        app: frontend
                                                        project: vprofile
                                                    spec:
                                                      containers:
                                                        - name: httpd-container(container-name)
                                                          image: httpd
                                                          ports:
                                                            - name: http-port(port-name)
                                                              containerPort: 80

     * kubectl get pod -> it will display the pod details(i.e name of the pod,status,age)

     * kubectl describe pod webapp-pod -> it will display entire details of the pod
  
     * kubectl get pod webapp-pod -o yaml -> it will display the details of pod in yaml format.

     * kubectl edit pod webapp-pod -> to edit the pod but most of the things in pod is non-editable.

     * kubectl apply -f web.yaml -> it will apply the changes to the pod.
  
     * kubectl logs webapp-pod -> it will display logs of that pod.


Service:
         way to expose an application running on a set of pods as a network service(similar to load balancer).

         1.nodeport:
                     exposing(mapping the ports) the pods to outside network(not for production).
        2.clusterIp:
                     you dont want to expose to external world but for the internal communication.
        3.load balancer:
                         expose the pod to external network for the prodution.

        * kubectl create -f service-defs.yaml -> it is used to create the service using yaml file.

        * kubectl get svc -> it will display all the services.

        * kubectl describe svc webapp-service(service-name) -> it will display the details of the service.
      
        * kubectl delete svc webapp-service(service-name) -> it is used to delete the service.

         Note:
                * while creating the service two things are important 1.selector should be match with pod label name
                  and target port should be match with pod ports name or pod ports name.


        let's create a service using nodeport:
                                                1.first create the pod

                                                      apiVersion: v1
                                                      kind: Pod
                                                      metadata:
                                                        name: webapp-pod(pod-name)
                                                        labels:                      //labels are like tags
                                                          app: vproapp
                                                      spec:
                                                        containers:
                                                          - name: vpro-container(container-name)
                                                            image: sriraju12/vprofileapp
                                                            ports:
                                                              - name: vpro-port(port-name)
                                                                containerPort: 8080

    
                                                           
                                                2.create the service
 
                                                     apiVersion: v1
                                                     kind: Service
                                                     metadata:
                                                       name: helloworld-service
                                                     spec: 
                                                       ports:
                                                       - port: 8090 (internal port)
                                                         nodePort: 30001(external port to access and nodeport start from 30000)
                                                         targetPort: vpro-port or 8080 (you can give portnumber or portname of the pod)
                                                         protocol: TCP
                                                       selector:
                                                         app: vproapp
                                                       type: NodePort



      
          let's create a service using loadbalancer:
                                                    1.first create the pod

                                                      apiVersion: v1
                                                      kind: Pod
                                                      metadata:
                                                        name: webapp-pod(pod-name)
                                                        labels:                      //labels are like tags
                                                          app: vproapp
                                                      spec:
                                                        containers:
                                                          - name: vpro-container(container-name)
                                                            image: sriraju12/vprofileapp
                                                            ports:
                                                              - name: vpro-port(port-name)
                                                                containerPort: 8080

    
                                                           
                                                   2.create the service
 
                                                     apiVersion: v1
                                                     kind: Service
                                                     metadata:
                                                       name: helloworld-service
                                                     spec: 
                                                       ports:
                                                       - port: 80 (loadbalancer port to access)
                                                         targetPort: vpro-port or 8080 (you can give portnumber or portname of the pod)
                                                         protocol: TCP
                                                       selector:
                                                         app: vproapp
                                                       type: LoadBalancer



Replica set:
             in simple words replica set means autoscaling.If the pod is goes down then replica will automatically creates new one for us.
            
              
* kubectl get rs -> it will get all the replica sets.

* to scale down two ways -> 1.directly edit the replica yaml file.
                            2.kubectl scale  --replicas=1 rs/frontend(replica-name that is mention in yaml file) => not recommended in production


* to scale up two ways -> 1.directly edit the replica yaml file.
                          2.kubectl edit rs frontend or kubectl edit rs/frontend => not recommended in production.


let's see how we can write replica set yaml file,

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:          // this template information is about pod
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5




Deployment:
             1. A deployment controller provides declarative updates for pods and replicasets.
             2. define desired state in a deployment, and the deployment controller changes
                 the actual state of the desired state at a controlled rate.
             3. deployment creates replicaset to manage number of pods.


* kubectl set image deployment/nginx-deployment(deployment-name mention in yaml file) nginx(image-name)=nginx:1.16.1(mention different version of nginx).
  this is not recommended for production.Best way is directly edit the yaml file and apply the changes.

* kubectl get deploy -> it will display all the deployments.

* whenever you make deployments,then the all the pods will delete one by one and create a new pods one by one by replica set.

* kubectl rollout undo deployment/nginx-deployment(deployment-name mention in yaml file) -> it will rollback to previous version on the image.

* kubectl rollout history deployment/nginx-deployment(deployment-name mention in yaml file) -> it will display the revision numbers of rollout.

* kubectl rollout undo deployment/nginx-deployment(deployment-name mention in yaml file) --to-revision=2 -> it will rollback to revision 2 of the image.

* kubectl scale deployment/nginx-deployment(deployment-name mention in yaml file) --replicas=6 -> it will scale up the pods.

* kubectl delete deploy nginx-deployment(deployment-name mention in yaml file) -> it will delete the deployment.


let's see how we can create the deployment yaml file,

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80




Commands and Arguments:
                        how to pass commands and arguments to pods.


let's see how we create the pod using Commands and Arguments in yaml file

apiVersion: v1
kind: Pod
metadata:
  name: command-demo
  labels:
    purpose: demonstrate-command
spec:
  containers:
  - name: command-demo-container
    image: debian
    command: ["printenv"]
    args: ["HOSTNAME", "KUBERNETES_PORT"]
  restartPolicy: OnFailure



Config Map:
             set and inject variables in the pod. A ConfigMap is an API object used to store non-confidential data in key-value pairs


* kubectl create configmap db-config --from-literal=MYSQL_DATABASE=accounts \
  --from-literal=MYSQL_ROOT_PASSWORD=raju12 (not recommended for production).  -> it will create db-config file of configmap
                                                                                  and inject these variable in the pod
* kubectl get cm -> it will display all the configmaps.

* kubectl get cm db-config(configmap-name mention in yaml file) -o yaml -> it will display the details of the file in yaml format.

* kubectl describe cm db-config(configmap-name mention in yaml file) -> it will show indetails of the file.

let's see how we can create the configmap in yaml,

apiVersion: v1
kind: ConfigMap
metadata:
  name: db-config 
data:
  MYSQL_ROOT_PASSWORD=raju12
  MYSQL_DATABASE=accounts

lets see how we can inject these configmap file to pod,

  * this will inject the entire configmap variables to the pod.


apiVersion: v1
kind: pod
metadata:
  name:db-pod
  labels:
    app: db
    project: vprofile
spec:
  containers:
    - name: mysql-container
      image: mysql:5.7
      envFrom:
        - configMapRef:
            name: db-config
      ports:
        - name: db-port(port-name)
          containerPort: 3306


  * if you want to inject specific variables from configmap file to the pod.


apiVersion: v1
kind: pod
metadata:
  name:db-pod
  labels:
    app: db
    project: vprofile
spec:
  containers:
    - name: mysql-container
      image: mysql:5.7
      env:
        - name: MYSQL_ROOT_PASSWORD(variable name this will store in the container.it will take value from the key and store in this variable)
          valueFrom:
            configMapKeyRef:
              name: db-config
              key: MYSQL_ROOT_PASSWORD
      ports:
        - name: db-port(port-name)
          containerPort: 3306

this will inject only MYSQL_ROOT_PASSWORD variable to the pod.


  * if you want to inject specific variables from configmap file through volumes to the pod.

apiVersion: v1
kind: pod
metadata:
  name:db-pod
  labels:
    app: db
    project: vprofile
spec:
  containers:
    - name: mysql-container
      image: mysql:5.7
      env:
        - name: MYSQL_ROOT_PASSWORD(variable name this will store in the container.it will take value from the key and store in this variable)
          valueFrom:
            configMapKeyRef:
              name: db-config
              key: MYSQL_ROOT_PASSWORD
      volumeMounts:
      - name: config(volume name)
        mountPath: "/config" (below mention path file will be stored in the location)
        readonly: true
  volumes:
  - name: config(name of the volume)
    configMap:
      name: db-config(name of the configmap)
      items:
      - key: "MySQL_ROOT_PASSWORD"
        path: "MySQL_ROOT_PASSWORD" (the value of the key will be stored in this file)
 
      ports:
        - name: db-port(port-name)
          containerPort: 3306

NOTE:****

* kubectl exec --stdin --tty db-pod(pod-name) --bin/sh -> to login into the pod


Secrets:
         store and manage sensitive information such as password.


* kubectl create secret generic db-secret(secret-name) --from-literal=MYSQL_ROOT_PASSWORD=raju12 -> it will create the secret in imperative which bad way
  and password will be stored as encrypted password.

* let's see how we can create secret using declarative way in yaml file

 first you need to encode the password for that use -> echo -n "raju12" | base64  => it will encode the password


apiVersion: v1
kind: Secret
metadata:
  name: dbpass_secret
data:
  username: eghegaehipofugbjda24(by using this echo -n "raju12" | base64 => you will get encoded password)
  password: uwfgoisehoifvbseise24
type: Opaque

kubectl create -f yamlfilename -> it will create the secret.
  

lets create the pod for the secret.
      

apiVersion: v1
kind: Pod
metadata:
  name: secret-pod
spec:
  containers:
  - name: secret-container
    image: sriraju12/vprofiledb
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: dbpass_secret(secret-name)
            key: username
            optional: false

      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: dbpass_secret
            key: password
            optional: false

  restartPolicy: Never



Ingress:
          An API object that manages external access to the services in a cluster, typically HTTP.

          Ingress may provide load balancing, SSL termination and name-based virtual hosting.

          Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster.
          Traffic routing is controlled by rules defined on the Ingress resource

=====================================================================================================================================================================

Kubernetes:

Disadvantage of docker:
1. Docker is a ephemeral that means short lived.The nature of the docker is scope to one host because of this in docker, containers are getting affected. Like you have installed docker and you are running 100 containers and 1st container  is taking more memory and resources because of this other container will affect it may not be created because of single host.
2. Docker does not have auto healing mechanism. Like if someone has killed your container and the application running in the container will not accessible and if you want to access the application again we need to start the container. It does not have the mechanism to automatically heal the container by itself own. This is major disadvantage.
3. Docker does not have the autoscaling mechanism that means whenever the traffic is high the containers should increase automatically and distribute the traffic among them using load balancer. This is missing in the docker and this also major drawback of it.
4. Docker is very simple and minimalist. It will not support the enterprise level support that means it does not support the load balancer, firewall, autoscaling, auto heal etc .
5. These problems are solved using Kubernetes.

Let’s see how we can resolve the above problems using Kubernetes:

1. By default Kubernetes is a cluster that means a group of nodes. If one container is getting affected by another then Kubernetes immediately takes that and put it in another node. So that the problem will be resolved because Kubernetes maintain cluster and in that there are different nodes in it.
2. Kubernetes solves the problem of autoscaling in two ways. First way is there a replica set yaml file in that file modify the replica set from 1 to 10 because you know that tomorrow due to festivals the load will increase this is manual way. Second way is using HPA that means Horizontal Pod Autoscaling. Whenever the traffic is high it will automatically increase the node and we can configure it using threshold if it reaches 80% then increase the load that’s it.
3. Kubernetes solves the problem of auto healing. Kubernetes will controls the damage and fix the damages that means there is a API server in Kubernetes, whenever it receives the information about the container is somehow going to down then the Kubernetes automatically create the new one before the container is getting down so that you will not know that the container is gone down.
4. Kubernetes solves the problem of enterprise level support that means it supports the load balancer, auto healing etc.

Architecture of Kubernetes:

Request —> control plan(api server) —> worker node

The request is goes through the control plan i.e master and then it will goes to the worker i.e slave. In the worker nodes, we will have a pod just like container in docker that is running. 
Kubelet is the one which manages the pod whether is running or not. If it is not running then it will inform to the master. 

Flow of Kubernetes:

For suppose user send the request to create the pod then that request will be authenticate, validate if that request is valid then that request is received by the API server and then sends to the ETCD data store to make the entry for the pod creation and after making the entry it will sent information to the API server that entry has been made and then the scheduler continuously monitoring the nodes and inform the availability and then the API server goes to that node and sent the request to the kubelet to create the pod and then kubelet send back the response that the pod has been created and then API server sent that information to the ECTD store that the pod is created in that node and then ECTD stores the information and sent the response to the API server that the information is stored and finally API server sends the response to the user that the pod has been created successfully. This is how it works.

Data plan / worker node:

Container run time which actually runs the container. In docker, in order to run the container we need to have container runtime like java etc. in Kubernetes support various container runtimes like docker, containerd etc.

In docker we have a docker zero which is used to manage the networking. In Kubernetes, we have a kube proxy, it will provide the networking that means every pod that you are creating will have the ip address for it and it has to be provided with load balancing capabilities because we have a autoscaling, instead one replica to a pod we have two replica sets to a pod then we have to do load balancing for it. In simple we can say, kube proxy provides networking, ip address and also the default load balancing. Kube proxy uses Ip tables for networking.

These are the three components that are present inside the worker node or data plane. i.e container runtime, kubelet, kube proxy.

Control plan:

What is the use of control plan:
If you want to create the pods then who decides in which nodes it will get created like node 1, node 2 etc. it will decide by api server that is present inside the control plan and it also exposes the Kubernetes to the outside world. Api server takes the request from the external or outside world. Api sever is the heart of the Kubernetes.

Let’s see you want to create the pod and you try to access the api server and api server decides that node 1 is free and in order to schedule the process there is a component in the control plan that schedules this process in the node that is scheduler. The main responsibility of the scheduler is scheduling the pods or the resources in the Kubernetes. Api server decides where to be scheduled. Scheduler acts on it.

ETCD is used for backing store that stores the all information of the cluster. It stores information in the form of key-value pair.

Controller manager ensures that controller like replica set are running.  It is manager for inbuilt controllers.

The Cloud Controller Manager is a component that interacts with the cloud provider's API. It abstracts cloud-specific logic away from the core Kubernetes components. It allows Kubernetes to work seamlessly across different cloud environments. Key responsibilities include:
* Managing cloud-specific resources (like Load Balancers).
* Watching for changes in the cloud infrastructure, such as the addition or removal of nodes.
* Managing persistent volumes and cloud storage.

The components that comes under control plan are api server, scheduler, ECTD, controller manager, cloud controller manager(CCM)

Control plan is the one which controls the actions and data is the one which executes actions on it.


NOTE:
For development environment and staging environment we can use Minikube but production environment we can not use Minikube because it does not support the enterprise level support.

For production environment we can use KOPS(Kubernetes Operations)
By using KOPS, we can create, modify and delete clusters and also we can upgrade changes.This is the Kubernetes lifecycle.

Pre-requisites for installing KOPS are:
1. AWS CLI
2. python3 (to install AWS CLI python3 is needed)
3. Kubectl - command line for Kubernetes 
4. KOPS
KOPS configuration files are stores in s3 bucket.

PODS:
1. Pod is a wrapper or smallest unit of Kubernetes that is used to create and deploy the applications in container  inside the pod . 
2. The Kubernetes assign the cluster ip to pod, by using this pod cluster ip we can access the container that is present inside the pod.  
3. Pods that can contain single container or multiple containers inside the pod.
4. In mostly cases, we will create only one container inside the pod.
5. The other container are supporting container or init containers. These containers supports the main container.
6. The difference between docker container and pod are in docker we will run the docker command to start the container and provide the details like name of the container, port number, networking etc in the command itself whereas in Kubernetes pods we will write everything in the pod.yaml file. That is only the difference 
7. To initially start with pod and to add the functionalities like auto healing, auto scaling  we will use the deployment yaml file.
8. In real time, we will not use pod yaml to create the container instead we will use the deployment yaml file which provides the features of auto healing and auto scaling.
9. Deployment yaml file is the way to deploy the applications in the Kubernetes.
10. Kubectl explain pod -> this command is used to tell the which version of pod should we use in the yaml file.
11. kubectl get pods -o wide -> it will display the ip address of pod.
12. kubectl get pods -v=7 -> it will display what exactly happening in the pod. V means verbosity you can increase or decrease the value but maximum is 9.

NOTE:
To login into the pod use below command 
kubectl exec -it name-of-the-pod — sh

How do you debug your pods:
We can get the logs and the information about the pods using this command kubectl describe pod name of the container and kubectl logs name of the pod.

Difference between container, pod and deployment:

Container: 
if you want to run the container, everything you need to pass as argument i.e command in command line like docker run -lt mention port mention volume mention networking etc parameters.

Pod:
Pod is also similar like container but instead passing parameters in command line, here we will write the pod yaml file and in that file we will mention all the parameters.

Deployment:
We came to the Kubernetes because it’s support the auto healing, auto scaling but in pod we can not do this features. In deployment we can implement this feature and at the end deployment also deploy the pod only and also we can implement auto healing and auto scaling in deployment.

Deployment:

Let’s see how deployment works:

When the deployment is deployed, it will create the replica sets which is Kubernetes controller that means it maintains state between the deployment(actual) and desired(cluster) in worker node. The replica sets creates the pods. In deployment file you have mentioned replica sets as 2 then it will create the 2 pods and it maintains 2 pods . If someone deletes one pods then replica sets automatically creates new one for you this is called auto healing mechanism because replica set is controller.

Services:

Let’s see what problems are there without services:

Let’s say you have deployed the deployment and in that deployment you have mention replica sets are three. Deployment create the replica sets and the replica set create the three pods. You have given the 3 pod ip address to the three user to access it. For now everything is fine and working. Due to internet issue one pod goes down and replica set has auto healing mechanism, it will create the new pod even before or at the same time of termination of pod. Then the ip address of that pod changes then the user can not access the pod because ip address is changed. This is major problem if we are not using the services.

Let’s see how we can resolve the above problem using services:

On top of the deployment, we will create the services which will act as load balancer and with the help of kube proxy we will distribute the traffic among the pods.instead of directly accessing the ip address of the pod, we will give the ip address of load balancer to the users to access the load balancer. The load balancer with the help of kube proxy it will distribute the traffic to the pods.

Another problem is, what if the pod goes down due to some issue and the replica set will create the new pod and for that pod will have new ip address. That new ip address even the load balancer does not know about that new ip address. The problem is still there. This problem is resolved by service discovery with the help of labels and selectors. The services will identifies the pod with the help of label instead of ip address so that even the pod goes down and it will create the new one, ip address will change but not the label. It should be same because that is mention in the deployment file and replica set uses that deployment file to create the pods.This is how we can resolve the problem.

Let’s see the overall flow:

When you deploy the deployment file, in that file , in the meta data you have write the label as app(here label name is anything) then it will create the replica set and the replica set will create the pods that you you mention in the deployment file . Let’s see you have mention 3 replica sets then it will create the replica sets with the label name as app in the pod. For some reasons pod will gone down and replica set will create the new pod and it will get the new ip address but the label name should be same i.e app. On the top of deployment we will create the service will take the label name instead ip address so that our problem will be resolved. How many times the pod goes down it does not matter because label name is fixed and we are mapping to label. This mechanism is called service discovery. 

If you want to expose your application to the outside world then 

The service is of three types:
1. Cluster ip
2. Node port
3. Load balancer 

Cluster IP:
If you use service as cluster lP type then you can access the application in the cluster itself. Cluster IP comes with load balancing and the service discovery that mention above.

NodePort:
If you use service as NodePort then whoever has the access to the worker nodes they can access the application. NodePort comes with load balancing and the service discovery that mention above.

Load Balancer:
If you use service as Load Balancer then you can access the application from anywhere with the help of the load balancer ip address. Load balancer comes with load balancing and the service discovery that mention above.

Namespaces:
In Kubernetes, namespaces provides a mechanism for isolating the group of resources within a single cluster. Basically it will act as capsule that means within the cluster you want to create different namespaces and you can put different resources on one namespaces. Names of resources need to be unique within a namespace, but not across namespaces.
1. kubectl get all -> it will display all the objects that are present in default namespaces.
2. kubectl get ns -> it will display all the namespaces. 
3. kubectl get -n namespace-name -> it will display the all objects present in namespace.
4. kubectl get —namespace=namespace-name -> it is same as above command.
5. kubectl create ns namespace-name -> it will create the namespace.
6. kubectl delete ns/namespace-name -> it will delete the namespace.
7. From one namespace to another namespace, we can access the pods directly using ip addresses. i.e curl ip address.
8. From one namespace to another namespace, we are not able to access the services(svc) like we are getting unable to resolve the name. Command is curl service-name.
9. To resolve the above problem you need to use FQDN that means Fully Qualified Domain Name. like curl name-of-the-service.FQDN.
10. You can use FQDN by using this command
         cat /etc/resolv.config (login into pod and    
          type this command). 
  11.  FQDN always like this name-of-the-  
        namespace.svc.cluster.local

Cron job:
Cron job is used to execute the task or anything at particular time and date.

Syntax for cron job:
* * * * * *  
* First star is minutes (0 to 59)
* Second star is hours (0 to 23)
* third start is days of month (1 to 31)
* Fourth star is month (1 to 12)
* Fifth star is days of week (0 to 6 sun to sat) 

Job:
Job is used to execute the task one time only. Like installing everything. i.e one time activity

Note:
1. Static pods are stored in single directory and the kubelet monitors that directory.
2. Control plan itself act as a pod and those pods are called static pods. 
3. Scheduler is used to schedule pods and is not responsible for scheduling the control plan pod because scheduler does not meant for static pod since control plan is a static pod.
4. In control plan also kubelet is present and it monitors these pods in the control plan.
5. /etc/kubernetes/manifests this is where the static pod configuration are stored. And these files are present in the control plan. So login into control plan and go to this path then you will find the configuration files.
6. If you remove the scheduler yaml file for this location and if you try to create the new pod then it will not create pod because scheduler is the one who is responsible for creating the pod. If you remove the yaml file of schedular then new pods will not create but existing pods will keep on running. 
7. Scheduler will keep on looking on the pods that are in pending state. In that pending state also, it will only schedule pods that have no node name because if node name is present then it will create that pod in that node. Scheduler only focus on the pods that have pending state as well as no node name present.
8. We can manually schedule the pods and without need of scheduler. For that you need to mention the node name in the pod yaml file. Here node name is, in which worker node this pod needs to be created.

Labels and Selectors:
Labels are just like tags to the resources, we can easily search the resources by using the labels.

Selectors are like just grouping them together to manages the resources into it. By using selectors we will match the label, so that they are managed easily.

INGRESS:

The problems with services:
1. The services does not provide the enterprise  TLS(https) load balancing. That means in services load balancer support only round robin to distribute the traffic and does not support various options.
2. If you have 100’s of services and then you have to pay more for the cloud providers to maintain the load balancer because it uses static ip address.

Let’s see how we can use Ingress:
By using ingress we can resolve above problems, for that first you need to create the ingress resource using yaml file in the cluster and also you need to deploy the ingress controller for it. Ingress controller is the load balancer type like nginx etc . Ingress controller manages the ingress resource just like kubelet manages the pod and kube proxy manages the services using ip tables.

Using ingress we can have, host based routing, path based routing and also wildcard based routing.

Host based routing:
Host based routing is based on the domain name. Using the domain name we can access the service.

Path based routing:
Path based routing is based on the domain name and followed by path like /home. Using this we can access the service. We can define multiple paths in the ingress resources to access the different services.

Wildcard based routing:
Wildcard based routing is also same as host based but only different is, here we don’t define the complete domain name like foo.bar.com instead we use wildcard like *.bar.com. If anything matches with this domain name then that request will be forward to the service.

RBAC:
RBAC stands for Role Based Access Control, in kubernetes RBAC is used to give Access to the users like what kind of user has what permission in the cluster and also provide Access for the Services that are running in the cluster. 
The two primary purpose of the RBAC are Access for Users and services that are running in the cluster.

Kubernetes offloads(give) user management to identity providers.

RBAC looks like:

Service account(manages users)   Role(permission)   Role binding(binding).

Role binding attach the roles to the service accounts.

Role is applied to namespaces only like resources inside the namespace such as pods, deployment, services etc.

ClusterRole which is applied to the cluster only like Nodes, namespaces such as get nodes, get namespaces etc.

If you want to see, what are the resources at namespace level then 
kubectl api-resources —namespaced=true

If you want to see, what are the resources are there at cluster level then 
kubectl api-resources—namespaced=false


Custom Resources:
 Custom resources definitions(CRD) are used to extend the behaviour of Kubernetes and also validate the  custom resource yaml file.

Let’s say, you have create the deployment yaml file which is a resource and mention all the details like api version, kind, metadata and spec etc. inside Kubernetes cluster, there is a resources definitions file which will validate the deployment yaml file like whether mention details are correct or not. If any unwanted details are mentioned then it will throw an error.
Similarly custom resources definitions are provided by third party to extends the behaviour of Kubernetes. There is custom resource definitions(CRD) which validate and extends the behaviour of kubernetes. Custom resources is similar to deployment yaml file. Resources definitions is similar to the custom resource definitions(CRD). when you deploy the deployment yaml file it will validate with the resource definitions and then the deployment controller will start creating the replica set and then pods. Similarly when you deploy the custom resource, it will validate with the custom resource definitions(CRD) and the custom controller will start creating the custom resource. Custom controller is used for managing the custom resource.


In Kubernetes, taints and tolerations are mechanisms used to control which pods can be scheduled on which nodes, providing a way to manage node allocation based on specific criteria.
Taints
A taint is a property of a node that prevents pods from being scheduled on that node unless those pods have a matching toleration. Taints are applied to nodes and consist of three parts:
1. Key: A string that identifies the taint.
2. Value: A string that provides additional information about the taint.
3. Effect: Defines what happens to pods that do not tolerate the taint. Possible effects are:
    * NoSchedule: Prevents pods from being scheduled on the node.
    * PreferNoSchedule: Tries to avoid scheduling pods on the node but may do so if necessary.
    * NoExecute: Evicts existing pods that do not tolerate the taint.
Tolerations
A toleration is applied to a pod and allows (but does not guarantee) the pod to be scheduled onto nodes with matching taints. Tolerations also consist of key, value, and effect, enabling the pod to "tolerate" the taint on the node.

The difference between taints, toleration and nodeSelector:

Taints
* Definition: A property applied to a node that restricts which pods can be scheduled on that node.
* Function: If a node has a taint, only pods with matching tolerations can be scheduled on it.
* Types: Taints have three effects:
    * NoSchedule: Pods without matching tolerations cannot be scheduled on the node.
    * PreferNoSchedule: Kubernetes will try to avoid scheduling those pods, but it’s not guaranteed.
    * NoExecute: Existing pods without matching tolerations are evicted, and new pods cannot be scheduled.
Tolerations
* Definition: A property applied to a pod that allows it to be scheduled on nodes with matching taints.
* Function: Tolerations "allow" a pod to tolerate a node's taint, meaning it can be scheduled even if the node has specific restrictions.
* Usage: Tolerations are specified in the pod's configuration and correspond to the taints present on nodes.
Node Selectors
* Definition: A simpler mechanism used to specify criteria for selecting nodes based on labels.
* Function: Node selectors allow you to restrict pod scheduling to nodes that have specific labels.
* Usage: You define a set of key-value pairs in the pod specification. Only nodes with matching labels will be considered for scheduling.

Taints are applied to the nodes. Nodes decides which types of pods should be accepted.

NodeSelector are applied to the pods and pods decide, in which node should be created or run the pods.

Node Affinity:

Node affinity in Kubernetes is a set of rules used to determine which nodes a pod can be scheduled on, based on the labels assigned to those nodes. It provides a more expressive and flexible way to control pod placement compared to node selectors.
Types of Node Affinity
1. RequiredDuringSchedulingIgnoredDuringExecution:
    * This is a hard requirement. If the specified node affinity rules are not met, the pod will not be scheduled.
    * It ensures that the pod is only placed on nodes that meet the criteria.
2. PreferredDuringSchedulingIgnoredDuringExecution:
    * This is a soft requirement. It specifies preferences for scheduling, but if no nodes match the criteria, the pod can still be scheduled on any available node.
    * Kubernetes will try to place the pod on nodes that meet the preference, but it won't block scheduling if none do.
In taints and toleration, it does not support the multiple conditions but node affinity will support various conditions like not equal, in etc operator.

Taints and toleration does not guarantee that the pods will not schedule in another nodes. Let’s say you have three nodes and you have applied taints i.e gpu= true applied to the node 1 and another two nodes you have applied label as disktype= ssd. In the pod yaml file, you have written the toleration like gpu= true. When you deploy this file, the scheduler decides check the nodes. Let’s say scheduler first checks the  node 2 and it will not matched the label still it will deploy the pod in the node. In order to prevent, only matched toleration should be deployed into the pod then we have to use both taints, toleration and node affinity.

In the pod yaml file, we have to mention both toleration and node affinity. Like node taint is gpu=true, node affinity is disk type is ssh. In the node 1 taints is gpu= true and node 2 label is disk type is hdd. When you deploy the pod then scheduler first check node 2 then it will match the label, in our case the label is not matched then it will go for node 1, here the taints are matched and so it will deploy the pod in node 1.
This is how we can prevent the pod to deploy in unmatched taints or label  nodes.

ConfigMaps:
ConfigMaps are used to store the insensitive data that are used for later point of time like database port, database name etc.

ConfigMaps can be applied to the pods in two ways: 1. Using environment variables 
            2. Using volumes.

1. Using environment variable, we will import the values from ConfigMap to pods. In the container image level we will write the env variables.
 The drawback of using environment variable is, when you change the values in ConfigMap, you have to restart the pods due to that some traffic may be lost. To avoid this we can volumes as ConfigMap.

2. Using Volumes, we can resolve above problem. In container level, create the volume and also mention the ConfigMap. In image level mention volume mount and in that mention name of the volume and path where this volume should present.

Secrets:
Secrets are used to store the sensitive data or information that are used for later point of time.

Resources, Requests and Limits:

For example, there are two nodes and each node contains 4 cpu and 4gb memory. Each pod can take 1 cpu and 1 gb memory to create. And there are 4 pods deployed in node 1 as well as on node 2 also. There is no cpu’s or memory left in the node and if you try to create another pod then it will throw an error called insufficient resources. This is one problem.

Another problem is, let’s there is only one node and it’s capacity is 4 cpu and 4 gb memory. You have created a 1 pod, it occupies the 1 cpu and 1 gb memory. Lot of requests are coming then the pod is occupying more cpu’s and memory and at some point the cpu’s and memory is not sufficient to handle request then it crash the pod and throws an error called out of memory error. And also it crash the node. This is also big issue.

Auto Scaling:
Auto scaling is of two types:
1. Horizontal Pod Autoscaling(HPA):
         It is used to scale the pods when there is a 
         heavy traffic or load.
         HPA is applied to pod and HPA is also     
        applied to node that is called Cluster Auto 
        Scaler.

 2. Vertical Pod Autoscaling(VPA): 
      It is used to increase or decrease the pod 
      capacity like cpu’s, memory etc when there                                    
      is  a heavy traffic or load. VPA is used when 
     you not bother about downtime, high latency.
    VPA is applied to pods and VPA is also applied                 to the nodes as well that is called Node AutoProvisioning.


EKS: how to deploy application in EKS

What are the problem with manual way:
If you deploy the application using EC2 instance then you need to install the control plane components and data plane components as well. You need to manage all the components in control and data plane. Due this there is a chance of manual errors occurs. If  the api server is down, ETCD is crashed, scheduler is not responding for all these problem you have troubleshoot the problem.This is very hectic task for us. To resolve this problem we will use EKS.

EKS is a managed control plane. EKS will manage all the control plane components. For data plane components, EKS provides two options. One is using ec2 instance, you can install data plane components in EC2 and connect with EKS control plane. Second option is using far-gate it should automatically creates everything for you. You don’t need to manage the data plane components. Which is the best option.

Let’s see how EKS deploy application and access it from outside world:
Using kubectl we will deploy the deployment yaml file, it will create the pods in the data plane or worker nodes. We will create the service for it. Service provides three options. One is clusterIp which is accessible within the cluster. Node-port which is accessible within the organisation. Load balancer is used to access from the outside world but it uses the static ip due to this cost will be huge. Instead of using the load balancer, we will use either node-port or clusterIp with the ingress. In the ingress we will define the rules. In order to manage the ingress we will create or deploy the ingress controller which manages the ingress and also create the load balancer which is accessed by the user like by using this load balancer and the load balancer send it’s request to the service and service to the pod. 

The best way to create the cluster in EKS is using EKSCTL command tool. 
 etsctl create cluster —name demo-cluster —region us-east-1 —fargate    -> it will create the cluster for you. Both control plane and data plane. Also create the VPC, inside it will create the public and private subnets.


DEPLOYMENT STRATEGIES:

Why we need deployment strategies:
Let’s say you have deployed a payment application in kubernetes using deployment yaml file and replica set as 4 then it will create the 4 pods. After some days you have to upgrade the version of the payment application. Then you have deleted the existing pods which takes 2 mins let’s say and to deploy the new version of the application will take 5 mins. Totally it takes 7mins to takes the request again. This is very bad experience for the organisation and the user as well. In order to reduce the downtime we will use the deployment strategies.

They are three deployment strategies mainly used: 
1. Rolling update
2. Canary
3. Blue Green 

1. Rolling update deployment strategy:
 It is the default deployment strategy in Kubernetes. When you have deployed four pods using the deployment yaml file then you have decided to upgrade the version of the application then you have changed the new image in the deployment yaml file then it will create  25% of the running pods first. If it’s okay then it will replace the 25% of the existing pods and then it will keep on doing this until it replaces all the existing pods with the latest pods. Due to this, the downtime comes to nearly zero. By default, creating new pods and replacing the existing pods is 25%. If you want to increase or decrease we can do in deployment yaml file. In that mention strategy and maximum unavailable(replacing pods) and maximum surge(new pods).

2. Canary deployment strategy:
You have deployed application of version V1 in kubernetes. After some days, you want to upgrade the version of the application then you have deployed the V2 version and tell the load balancer to distribute the traffic for V1 is 90% and for V2 is 10%. We will test the V2 version with limited number of users and after observing the V2 version for some days, if everything is good then you keep on increasing the load on the V2 version like 20%, 30%..100%. If everything is good then distribute the traffic to V2 version. Continuously we will take feedback from the users of V2 version. From this deployment strategy less people get affected. The main advantage of using canary is, we have the control over it. This is the most preferred one.

Difference between canary and rolling update is:
In canary, we can decide how much traffic should it goes to the new version of the application. If something goes wrong less users will get effected.
Whereas in rolling update, we don’t have that control. If one pods successful created then within a minute all the new pods created and it replaces with the new existing pods.

3. Blue Green deployment strategy:
It is very simple and easy to use and also very costly as well.
You have deployed the application of version V1 in the kubernetes with 4 pods using deployment yaml and these pods are pointing to the service. You have decide to upgrade the version of the application to V2 version. You need to create the same setup that is similar to V1 version like pods services, ingress. In the ingress, you just need to replace the new service that we have just now and that’s it. The load balancer will point to the new service and serves the traffic. If anything goes wrong then again we will go to the ingress and then we will mention the previous service name then load balancer again points to the previous version of the application. It’s just like link will remove from service1 to service2, if anything goes wrong again we can change from service2 to service1.


Health Probes:
Health probes is the process of monitoring the resources and it ensures the application are running.
Health probes are of these types:
1. Liveness probes: it restarts the application when it’s fails
2. Readiness probes: ensures the application is in ready, then only it will serves the traffic.
3. There two are mostly used health probes.

Note:
Liveness probe health is failed then the kubelet will restart the pod. If this health check is pass then it will be in running state. 

Service Mesh with ISTIO:
Let’s say, you have deployed the micro service  application in the Kubernetes cluster with micro services like login, catalog, payment and notification. If this application needs to access from the outside, so you will use either ingress or node port or load balancer as type. If the traffic flows from outside world to the application in cluster or vice versa then it is called north- south traffic. If the services talk to each other then it is called east-west traffic. ISTIO is used for traffic management that is the traffic between the services between the Kubernetes cluster. If the services are talking to each other then what is the use of ISTIO here. ISTIO provides additional features like mTLS(mutual TLS) when you install the istio in the namespace then istio provide the secure communication between the services. Each services will have their certificates that are generated by authority of ISTIO automatically. When services want to talk each other then both services show their certificates then both are validate then it will make a connection. ISTIO provides the deployment strategy like canary, blue green etc. ISTIO also support Observability i.e kiali. You need to install it and it is integrated easily.

How it will implement:
In the clusters,  for every container it will create a new container which sits next to the container. This new container is called sidecar container. Which contains the envoy proxy server. Which is responsible for the traffic coming and going out to the container like if any request coming from outside it will go to the sidecar container first and then it will goes to the main container and vice versa. The sidecar container will collect information about the container and sends to the ISTIOD and it will keep track of the metric and everything. This is how we can make use of Observability.
How does the api server notifies the ISTIO that incoming request receives and that request need to transfer to ISTIO and ISTIO need to create the sidecar container for it. It uses the admission controller.

NOTE:
Governance means set of rules of organisation to implement in the project. In Kubernetes it is like for the pods we need to have resource quota, resource limits and tags etc need to present. If not present then they should be created. For this to implement, we need admission controller. For writing admission controller is very difficult because it should write in go language most of the people don’t know the go language . Instead we use dynamic admission controller  like kyverno. In kyverno we will write the yaml file. The kyverno will convert into admission controller. It will provide the security by enforcing the things.


Troubleshooting of kubernetes:

1. ImagePullBackOff:
ImagePullBackOff words has two meanings,first ImagePull means it occurs due to the invalid image name or non-existing image or if the image is private. For these reason we will get the  ImagePullBackOff error. BackOff means at first it will throw an error called ErrImagePull, it will continuously trying to get the image, if it fails then after some time it will convert into ImagePullBackOff error. These are the reason we will get this error.

NOTE:
if you want to fetch the private image then you need to mention this in yaml file. i.e imagePullSecrets and inside mention  the secret. This is container level word.

If you want create the secret for docker-hub private image then use this command

kubectl create secret docker-registry demo --docker-server=<your-registry-server> --docker-username=<your-name> --docker-password=<your-pword> --docker-email=<your-email>

If you want create the secret for AWS ECR private image then use this command

kubectl create secret docker-registry  \
  --docker-server=${AWS_ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com \
  --docker-username=AWS \
  --docker-password=$(aws ecr get-login-password) \
  --namespace=default

2. CrashLoopBackOff:
The word CrashLoopBackOff contains two words. One is CrashLoop that means when you deploy the deployment yaml file, the api server takes that request and talk to the scheduler on which node this deployment should deploy and kubelet is the one who is responsible for creating the pod. At first the pod goes to the container creating state and after that two things can happen either it goes to the running or to the error state. Due to the some reason the pod got crashed. The kubelet will again try to restart the container and then it goes to the container creating status with same loop. Like
Container creating —> running/error —>crashed—>restart the container —>container creating. This is called CrashLoop.
BackOff means if the pod get crashed then it will keep trying to restart the pod with some time like after 10sec, 12 sec etc. This is called BackOff.
The most common reason for CrashLoopBackOff is,
1. Due to misconfiguration: like wrongly reading the env variable or volumes.
2. Out Of Memory:  out of resources like memory or cpu’s.
3. Wrong CMD command: like to start the application wrong CMD command is given in Dockerfile.
4. Liveness probe: if the health check of Liveness probe failed.

StatefulSets:
StatefulSets are similar to deployment but only difference is, in deployment if you mention replica set as 2 then it will try to schedule the 2 pods like 0/2 in pending state at first. Whereas in StatefulSets at first it will schedule only one pod, if it’s successfully created then only it will schedule the second pod i.e at first 0/1 in pending state.

Workflow of StatefulSet:
StatefulSet -> persistent volume claims -> storage class -> provisioners -> persistent volume.
StatefulSet requires the persistent volume claims and PVS require the storage class and storage class need the provisioner to create persistent volume.

3. StatefulSet with persistent volume not working after cloud migration:
It is because we need to mention the correct storage class for the specific cloud providers.

Network policies:
In Kubernetes, network policies are used to restrict the access one pod accessing  to the another pod using ingress(inbound traffic) and egress(outbound traffic). 
Network policies are applied to pod using ingress by using ip address, namespace selectors( using namespace we can strict access) and using pod selectors(by using label names we can restrict the access)

For example: if you have deployed three tier application in the Kubernetes like frontend, backend and database. If you dont want to access the frontend directly to database then you can apply network policies to restrict it from accessing using pod selectors(using label names).

How to secure api server:
1. Using RBAC, we can restrict the access to the resources in Kubernetes.
2. By using certificates. For you need to edit the api server yaml file and add the certificate, private key and client.



Real time challenges scenario problems:
How do you distribute the resources(memory and cpu) among the different teams in a cluster like if you take e commerce application they are payments team, products team etc:

Problem 1:
For a cluster you have 100gb memory and 100 cpu’s. How do you distribute these resources to the different teams. For example, you have created a different namespaces for the different teams like to say 5. And you have deployed the application into the namespaces. For example the first requires only 5gb memory due memory leaking i.e consuming more memory like 20gb then the other namespace may not get the required sufficient  resources to run the application then it will crash the pod and then it will throw an error called OOMkilled that means OutOfMemory error. How do you resolve this problem.

Solution:
50% of the problem resolve using this:
For the namespaces, we can allocate the Resource Quota i.e limit. For for that namespace, it should takes only whatever memory and resources we have mention that only it should take, not more than that. How can you decide the resources for the application. For that you need to sit with the development team and discuss with them. The development team will run the performance testing for their application and then they will tell you that this much resources are enough to run this application. And you mention those details.
Resource Quota is a limit that can be set on the namespace.

 We have reduced the blast radius from cluster to the namespace. By using Resource Quota you have reduce blast radius to namespace. In that namespace, you have allocated 10gb of memory and the applications running inside are let’s 5 applications. One application is consuming more memory(memory leaking) due to that another application may not get enough resource to run application again it will go into OOM i.e Out of memory error. To resolve this problem, we will use Resource limit. Resource Limit is a limit that can be set on the pod. By using this we can set the limit for the pod. Again if one pod consuming more memory, that pod only crashes it will not impact the other application that are running in the namespace.

Overall:
At first, the entire cluster is getting crashed, so we have used Resource Quota for namespace and then again it will impact the namespace, for that we have used the Resource limit for the pod. If anything happens then it will affect only that pod. This is how we have reduced the blast radius and This is very easy to identify the issue that means which application causes the issue.

Problem 2 and solution :
For the above scenario, how do you handle the OOMKilled error for the pod:
As a devops engineer, you will log into that pod and takes the heap dump and Thread dump and share it to the development team. The development team will check which part of the application is leaking the memory and then they will fix and they will give you the new version of the application and you need to deploy that application that’s it. But you need to take care of Resource Quota and Resource limit.

Problem 3 is with upgrades. Like moving from one version to another version.


DNS: Domain Name System 
DNS is used to resolve the ip address into names.
Let’s see how DNS works:
Whenever first you visited the website in browser, it will write the DNS query to get the response. So basically, browser first sends the request to the DNS server and matches the name with the ip address and get that ip address to the browser and using this ip address, browser will make the request. It actually caches the data. So whenever first time using the website, it will make the dns query and get the response and caches the data in the browser. So from next time onwards, it will not make the dns query because it already cached the data and it simply uses that ip address to make the request. Due to this latency problem will resolved.

Let’s see what are commonly used DNS Records:
1. A record: which is used to map the ipv4 address to the domain name.
2. AAAA record: which is used to map the ipv6 ip address to the domain name.
3. CNAME record: which is used to map dynamically another website to this domain name.
For example, if you have domain name www.raju.xyz and in the cname we have given google.com. So whenever you access this domain, it will point to the google.com

Like when you access this domain, first it will search for CNAME record and then it will take the google.com and then it will again go to the google dns server and search for A record and then that ip address is taken and keep in the cname record and it will open the google page for us.


Note:
Whenever we do drain the node that means two things must happen 
   1. Drain the node: evicting the node. 
2. Cordon: unschedule of pods.

Whenever the pods are drain then the node will be evicted and also unscheduled of pods.

Uncordon: that means pods can be schedulable.

Upgradation: 
Whenever you do upgradation, first thing you need to keep in mind is, you can not skip the version that means you can not directly jump from 1.28.1 to 1.30.3. Instead first you need to upgrade from 1.28.1 to 1.29.2  and then 1.29.2 to 1.30.3. That how you need to upgrade the versions.

Here 1.30.3 are 1 is major release, 
                           30 is minor release
                            3 is patch
Minor release will release 2-3 months and patch are frequent of bug fixes, it will come any time.

Kubernetes supports only three latest minor releases. Previous minor releases, you will not get any upgrades or support.

Let’s see how we can upgrade the version:
Three things are very important for upgradation:
1. Drain the node and then
2. Upgradation and then
3. Uncordon

Upgradation strategy are:
1. All at once:  That means all the node will be upgraded at a time. Which causes downtime for the users.
2. Rolling update: That means one node will be evicted and then we will do upgradation and then it will create a new node and then one by one it will upgrade all the nodes.
3. Blue green: For this, we need to create the exact existing infrastructure with new version and then we will attach the new nodes to the master node one by one and then we will evict the existing nodes or pods. Downtime is also very less for this. This strategy is good when you use managed Kubernetes like GKS, AKS etc. if you use own database then which is not recommended because we need to take lot of approvals to create the new vm’s.

Note:
While upgradation, keep this is mind otherwise you will face comparability issues.
Api server needs be with the latest version
Remaining components in control plan, can be latest or one low version of latest version.
Data plane components like kubelet, kubectl can go upto max 2 low versions.

To upgrade follow this:
1. First upgrade the primary control plane.
2. Then upgrade the additional control plane.
3. Then upgrade the worker nodes.

Steps:
1. First you need to change the packing repository to the version you want to upgrade. Location is /etc/apt/sources.list.d/kubernetes.list
2. Determine which upgrade needs to be upgraded. For that run ->  sudo apt-cache madison kubeadm. 
3. It will show all the available versions.
4. Upgrade the control plane first. For Command refer official document.
5. Then upgrade the worker nodes.


Backup of ETCD:
To backup the ETCD, you need to first install the ETCDCTL which is used to interact with the ETCD and we can take backup’s, restore etc.

Whenever you run ETCDCTL commands pass this as environment variable ETCDCTL_API=3 because it will take the latest version. If you don’t specify this then it will take the deprecated version.

Steps:
1. Cat on the ETCD yaml file. Location is /etc/Kubernetes/manifests. Below is the command for the backup of ETCD.
2. ETCDCTL_API=3 etcdctl —endpoints =mention the url of listen-client-url in the ETCD yaml file \
         —cacert= mention the trusted-ca-file in the     
         ETCD yaml file  \
         —cert= mention the cert-file in ETCD yaml 
         file \
         —key= mention the key-file in ETCD yaml    
         file \
          snapshot save /opt/backup.db(mention the 
          location where you need to store the 
          backup of this file and extension should be 
          db).

To restore the backup of ETCD:

To restore, steps to follow:
1. Stop the api server
2. Restore it from backup file
3. Start the api server
4. Below is the command to restore the ETCD

          ETCDCTL_API=3 etcdctl —endpoints 
          =mention the url of listen-client-url in the 
          ETCD yaml file \
         —cacert= mention the trusted-ca-file in the     
         ETCD yaml file  \
         —cert= mention the cert-file in ETCD yaml 
         file \
         —key= mention the key-file in ETCD yaml    
         file \
          snapshot restore /opt/backup.db(mention 
         the location where you stored the 
         backup file) —data-dir=/var/lib/etcd-
         restore-from-backup-file.

    5. After running the above command, you 
        need to edit the ETCD yaml file and in the 
        change the data-dir location to the what we 
        have created above and also change the 
        mount path and host path to this location 
       and that’s it.
    6. Restart the api server. For that you need to 
         move the yaml file to the tmp location and 
         then again from tmp location to the current 
         location and that’s it. It automatically 
         restart the api server.
   7. Restart the kubelet like systemctl restart 
       kubelet and also systemctl  daemon-reload
  8. This how we can restore the ETCD.

Note:
After Kubernetes 1.3 update, the default runtime changed from docker to containerd. So in order to interact with it, you need to use crictl tool for it.

$HOME/.kube  —> This is the default location of kube config file.

JsonPath:
In Kubernetes, JSONPath is used to extract and manipulate specific fields from JSON-formatted output, such as data from Kubernetes objects retrieved using kubectl. It allows you to query and format results effectively by specifying a JSONPath expression. For example:

kubectl get pods -o jsonpath='{.items[*].metadata.name}'


upgradation of EKS(Managed Kubernetes control plan cluster):

the pre-requisites:
1.first cordon the nodes -> it will make the node as unschedulable
2.read the release notes -> to know about the changes made on the new version.
3.for EKS, we can not downgrade the version -> if you upgrade the version from 1.30 to 1.31 then you can not downgrade it to 1.31 to 1.30
4.first upgrade in lower environments like dev,staging and observer for 1-2 weeeks like everything is good then only go for prod.
5.control plan and data plan should be in same version.
6.kubelet and cluster auto scaler should compatable with new version. if not then upgrade those.
7.make sure that 5 available ip address in the subnet.

process:
1.first upgrade the control plan component using the eksctl updrade command.
2.upgrade the data plane components. here in EKS, there are either nodegroup(using launch templates), nodes(manual creation of ec2 instances),
  fargate.
3.if you are using nodegroup using launch templates then in the compute section on the ami version you will get update button to upgrade the
  data plane components.
4.update the AddOns like CNI etc.

to check upgradation:
to verify whether the upgraded version is working or not, you need to run the script i.e end-to-end testing or functional testing.this will
check everything is working fine or not.

############################################################################################################################################

interview questions:

1.what is the difference between stateful and stateless in kubernetes.
Stateless Applications:
What it means: Stateless means that the application doesn’t remember anything between sessions or requests. Each time it gets a request, it starts fresh without any memory of previous requests.
Example: Imagine a web server that shows a webpage. Every time you visit, it doesn’t remember anything about your last visit—just a clean page every time.
How it works in Kubernetes: If a stateless app's pod (container) crashes or gets replaced, Kubernetes doesn’t need to worry about restoring any data because the app doesn’t store anything locally. You can easily add or remove pods without worrying about losing data.
Stateful Applications:
What it means: Stateful means the application needs to remember things across requests. It keeps track of data or information even if it restarts.
Example: A database like MySQL. It needs to remember your data (like customer information) even if the server goes down and comes back up.
How it works in Kubernetes: Kubernetes has to make sure that if a stateful app's pod restarts or scales, it can still access its data. Kubernetes ensures that each pod has its own "identity" and access to persistent storage (so the data doesn’t get lost when the pod restarts).

In Kubernetes:
Stateless apps are managed by a Deployment, which makes it easy to add/remove pods without worrying about data.
Stateful apps are managed by a StatefulSet, which ensures that each pod has its own identity and access to data, so it can “remember” information across restarts.

2.what are kubernetes services and explain each of them.
n Kubernetes, Services are used to expose and manage access to your applications (running in pods) in a way that's reliable and consistent. They allow communication between different parts of your application or with external users, even as your pods are created, destroyed, or moved around.

ClusterIP = A private, internal address to talk to services in your cluster.
NodePort = A way to expose your app on a specific port on all the machines in the cluster.
LoadBalancer = A fancy, cloud-managed service that gives your app a public IP and distributes traffic to it.

3.explain kubernetes architecture.
Kubernetes is a platform for managing and automating the deployment, scaling, and operation of containerized applications.
Main Components of Kubernetes Architecture
1.Master Node (Control Plane)
2.Worker Nodes

1.Master Node (Control Plane)
The Master Node is like the "brain" or the manager of your Kubernetes cluster. It makes all the important decisions (like scheduling and monitoring) and keeps track of what's happening inside the cluster.

Main parts of the Master Node:

API Server: This is like the front desk of the Kubernetes control plane. It receives all the requests (from users or other parts of Kubernetes) and processes them. It communicates with all the other components.
Scheduler: Think of the scheduler as a "traffic manager" who decides which worker node (machine) should run which container (pod). It assigns workloads to the nodes based on availability and resources.
Controller Manager: It ensures that the desired state of the system matches the actual state. For example, if you need 5 instances of a service running and one fails, the controller manager makes sure a new one gets started automatically.
etcd: This is a key-value store where all the important cluster data (like configuration and state information) is saved. It’s like the "memory" of Kubernetes, storing everything about your cluster’s state.

2.Worker Nodes
Worker Nodes are the machines (physical or virtual) that actually run your applications (in containers) and do the work. They receive instructions from the master node.

Main parts of a Worker Node:

Kubelet: The "worker" that listens to the master and ensures that the containers (pods) are running properly on the node. It takes orders from the master node and keeps the containerized apps healthy.
Kube Proxy: It manages network communication between pods, ensuring that traffic is correctly routed to the right application. It helps with things like load balancing and service discovery.
Container Runtime: This is the software that runs your containers. For example, Docker or containerd. It’s responsible for running the actual application inside the container.

4.what are resource requests and limits in kubernetes.
A resource request is the minimum amount of resources (CPU and memory) that a container needs to run properly. It’s like telling Kubernetes, "This is the minimum I need for this container to run."
A resource limit is the maximum amount of resources (CPU and memory) that a container can use. It’s like telling Kubernetes, "This is the maximum I allow this container to use."

Request is like the minimum amount of resources the container needs to run.
Limit is like the maximum amount of resources the container can use.

Real-Life Analogy:
Imagine you're running a small office with multiple employees (containers) working on different tasks:

The request is like telling the office manager how much desk space (memory) and how many computers (CPU) you need to do your work comfortably. The manager uses this information to assign you an appropriate desk.

The limit is like telling the manager that even if you really want to, you're not allowed to take up more space than a certain amount. If you start taking over more than your share, the manager will stop you (or kick you out of the office if necessary!).

5.difference between secrets and configmaps in kubernetes.
A ConfigMap is used to store non-sensitive configuration data that your application might need, such as environment variables, configuration files, or command-line arguments.
A Secret is used to store sensitive data such as passwords, API keys, or any information that needs to be kept private and secure.

6.difference between deployment,stateful and replica set.
In Kubernetes, Deployments, StatefulSets, and ReplicaSets are all controllers used to manage the deployment and scaling of application pods.

A Deployment is a higher-level controller used for managing stateless applications that can be easily scaled, updated, and replaced without worrying about maintaining the state.
Key Features:
Stateless Applications: It is used for applications where the pods do not need to persist data. For example, web servers or API servers.
Rolling Updates: Deployments automatically handle rolling updates, meaning you can update your application without downtime by gradually replacing old pods with new ones.
Scaling: You can easily scale the number of pod replicas (copies) up or down.

A StatefulSet is a controller used for managing stateful applications that need to maintain identity, stable storage, and persistent state across pod restarts.
Key Features:
Stateful Applications: It is used for applications that need to remember data or have unique identities (like databases or message queues). Each pod gets a stable network identity (e.g., my-statefulset-0, my-statefulset-1).
Persistent Storage: StatefulSets can be used with Persistent Volumes (PVs), so that data is preserved even if the pod is restarted.
Ordered Deployment: Pods in a StatefulSet are created and terminated in a specific order, which is important for some stateful applications (like a database cluster that needs pods to be started in a specific order).

A ReplicaSet is a lower-level controller that is responsible for maintaining a specified number of replicas (pods) of a pod running at all times. It is often used indirectly through Deployments, as a Deployment automatically creates and manages a ReplicaSet.
Key Features:
Basic Scaling: It ensures that a specific number of identical pods are running at any given time.
No Updates or Rollouts: Unlike Deployments, ReplicaSets don't handle rolling updates or versioning of the pods. They simply ensure the number of pods is as requested.
No State Management: ReplicaSets are for stateless applications, just like Deployments, but they don't have features like ordered deployment or stable network identities.

7.what is the role ETCD in kubernetes.
etcd is a key-value store that holds all the configuration and state data for your Kubernetes cluster.
It helps Kubernetes track and manage the desired and actual state of resources (pods, services, deployments).
etcd is critical for ensuring the stability and reliability of the Kubernetes cluster, acting as the central database for the entire cluster's state.
Without etcd, Kubernetes wouldn't be able to remember what should be running, where it should be running, or what the current state of the cluster is.

8.how rolling update works in a deployment.
In Kubernetes, a Rolling Update is a strategy used in a Deployment to update the application without causing downtime. It gradually replaces the old version of your app with the new version, one pod at a time (or in small batches). This allows your application to continue serving traffic while it’s being updated.

How Rolling Update Works in a Deployment:
Let's say you have an application running with 5 replicas (pods), and you want to update the application to a new version.

Start with the Current Version:
Initially, the Deployment has your app running with 5 replicas, each running the old version of the application.

Update the Deployment:
When you update the Deployment (for example, changing the container image to a new version), Kubernetes will begin a rolling update to replace the old pods with new ones that use the updated container image.

Gradual Replacement:

Kubernetes will not replace all pods at once. Instead, it will replace them one by one or in small batches. This ensures that there are always some running pods to handle traffic, avoiding downtime.
For example, if you have 5 pods, Kubernetes might stop and replace 1 pod at a time. So, there will always be 4 pods running the old version and 1 pod running the new version during the update.
Health Checks:

Kubernetes checks if the new pods are running successfully (healthy) before replacing the next pod. If a new pod is not healthy (it fails a readiness probe), Kubernetes will stop and wait for it to become healthy before proceeding.
If something goes wrong with the new pods, Kubernetes can rollback to the previous version to ensure your application remains stable.
Complete the Update:
Once all the old pods are replaced by the new ones, the update is complete. All the pods are now running the new version of the application.

9.what are the different types of kubernetes volumes.
PersistentVolume (PV) and PersistentVolumeClaim (PVC)
What it is: This is a more advanced and flexible storage option.
A PersistentVolume (PV) is a piece of storage provisioned by an administrator or dynamically created based on storage classes.
A PersistentVolumeClaim (PVC) is a request by a user for a specific amount of storage from a PV. It’s like asking for a certain amount of space on a disk.
When it’s used: It’s used when you need persistent storage that will survive pod restarts and even pod deletions. The data is stored outside the pod, often on network storage, and can be reused by different pods even if the pod is rescheduled to another node.
Use case: For databases, file storage, or any other application that needs data persistence across pod restarts.

ConfigMap and Secret Volumes
What they are:
ConfigMap Volumes store configuration data (non-sensitive) that your app can use at runtime, like environment variables or configuration files.
Secret Volumes store sensitive information, such as passwords, API keys, or certificates, and provide an encrypted way to store this data securely.
When they’re used: These volumes are useful for storing application configuration or secrets that your containers need to function but want to keep separate from the container image itself.
Use case: Use a ConfigMap to store configuration files for your app or a Secret to securely store API keys and passwords.

10.if a pod is in crash loop, what might be the reason and how to recover it.
When a pod is in CrashLoopBackOff, it means it keeps failing to start up. This happens for various reasons, such as:
Errors in the application code (bugs or crashes),
Missing or wrong configurations (environment variables, files),
Resource problems (using too much memory or CPU),
Connectivity issues (can't reach other services it depends on).

To recover:
Check the logs to find out why the pod is failing.
Inspect the pod configuration (resources, environment variables, volume mounts).
Fix the error (update the code, adjust configuration, increase resources).
If needed, delete the pod and let Kubernetes create a new one.

11.what is the difference between statefulset and daemon set.
In Kubernetes, both StatefulSet and DaemonSet are controllers used to manage the deployment and scaling of pods
A StatefulSet is used when you need to run pods that require stable, persistent storage and maintain unique network identities across restarts. It’s typically used for applications like databases or any application that needs to remember its identity (like Redis, MySQL, Cassandra).

A DaemonSet ensures that a copy of a specific pod runs on every node in the cluster (or a subset of nodes). It's useful when you need certain types of pods (like logging agents, monitoring agents, or network proxies) running on every node in your cluster.
Use Case: If you need to deploy a pod on every worker node in the cluster, such as a log collection agent (like Fluentd), monitoring agent (like Prometheus Node Exporter), or network proxy.

12.what is sidecar container in kubernetes and whar are it's usecases.
A sidecar container in Kubernetes refers to a helper container that runs alongside the main container within the same Pod. The sidecar container is not the primary application but performs supporting tasks that enhance or extend the functionality of the main application container.
Real-World Example of Sidecar:
Imagine you're running a web application in a Kubernetes pod. You have a main container that serves the website, but you also want to:
Log user requests: You could run a Fluentd sidecar container to collect and send the logs to a logging system like ELK or Splunk.
Monitor application performance: You could run a Prometheus exporter sidecar container to collect metrics like response time and request count, which can then be scraped by Prometheus for monitoring.
Secure traffic with TLS: You might use a proxy sidecar (e.g., Envoy) that manages TLS encryption and ensures all traffic to and from your web app is secure.

13.what is kubernetes and why it is used.
Kubernetes (often abbreviated as K8s) is an open-source platform that helps you manage and orchestrate (automate) the deployment, scaling, and operation of containerized applications.
why it is used:
It’s used because it makes deploying, scaling, and managing applications (especially complex ones) much easier and more reliable. It also helps ensure that your applications stay up and running without manual intervention.

14.what is pod in kubernetes.
A Pod is the smallest unit of work in Kubernetes.
A Pod can hold one or more containers that share the same network and storage resources.
Pods are temporary and can be automatically replaced by Kubernetes if they fail.
Pods allow you to manage containerized applications in a scalable and automated way in Kubernetes.

15.what are kubernetes ingress.
Ingress is like a gatekeeper that manages how external traffic (from the internet) gets into your Kubernetes cluster and how it is routed to different services inside.
Ingress is a way to manage external HTTP/HTTPS traffic to services inside the cluster.
It allows you to define routing rules (path-based, host-based), handle SSL termination, and provide load balancing.
How Does Ingress Work?
Ingress works by defining rules for how traffic should be routed. It usually relies on an Ingress Controller (like NGINX or Traefik) to actually implement the rules.

Here’s a basic flow:
Ingress Resource: You define an Ingress resource, which specifies how external traffic should be routed to services. For example, you might route traffic coming to example.com/api to one service and traffic coming to example.com/login to another service.
Ingress Controller: The Ingress Controller listens for incoming requests and uses the rules in the Ingress resource to decide where to send the traffic (which service and which port).

16.explain the concept of service mesh in kubernetes.
A Service Mesh is a dedicated infrastructure layer that helps manage and secure the communication between microservices in a Kubernetes cluster. It provides features like traffic management, security, observability, and reliability for the services running inside the cluster, without needing to modify the application code.

How Does a Service Mesh Work?
A Service Mesh works by using sidecar proxies that are deployed alongside each microservice. These proxies intercept the communication between services and handle tasks like routing, load balancing, and security.
Here's how it works in simple terms:
Sidecar Proxy: A sidecar proxy is a small helper process that runs alongside your application containers in the same Pod (just like sidecar containers). This proxy intercepts and manages all the network traffic to and from the service it’s running with.
Control Plane: The control plane is responsible for configuring and managing the mesh. It decides how traffic should be routed, how services should be secured, and gathers data on the performance of the services. It pushes these configurations to the proxies.
Data Plane: The data plane consists of the sidecar proxies deployed with the services. The proxies handle the actual traffic and enforce the policies set by the control plane.

17.what is namespace and why it is used.
A Namespace is like a folder or container where you can organize and manage your Kubernetes resources, helping to avoid conflicts and manage access for different teams or projects.
Namespace is a way to organize and isolate resources in a Kubernetes cluster.
It helps you separate different environments (dev, staging, prod), teams, or applications within the same cluster.
Namespaces provide resource isolation, access control, and avoid name conflicts.
By using namespaces, you can better manage and secure large-scale Kubernetes clusters.

18.how does kubernetes handles pod failures.
Kubernetes monitors the health of Pods and automatically restarts or replaces failed Pods to ensure the desired application state.
ReplicaSets and Deployments ensure the right number of Pods are running, even if some Pods fail.
Kubernetes can reschedule Pods to different nodes if a node fails.
Kubernetes can scale the number of Pods if needed to handle increased load.

19.what are the different workloads in k8's.
pod,deployment,relica sets,daemon sets,statefulsets,job and cronjob.

20.what is the difference between replicationController,replicasets and deployment.
The ReplicationController is the oldest controller in Kubernetes for managing a set of identical Pods. Its main job is to ensure that a specific number of Pods (replicas) are running at all times. If a Pod fails, the ReplicationController will create a new one to maintain the desired number of replicas.
A ReplicaSet is a more advanced and flexible version of the ReplicationController. It has the same basic function of ensuring that a specified number of identical Pods are running, but with added features and better support for newer Kubernetes features.
A Deployment is the most advanced and recommended way to manage Pods in Kubernetes. It builds on top of ReplicaSets and provides even more functionality, including rolling updates, rollbacks, and version management.

21.how does kubernetes manages internal and external networking.
Internal Networking in Kubernetes:
Internal networking refers to communication within the Kubernetes cluster between different Pods and services.
1.Pod-to-Pod Communication:
Pods in Kubernetes can communicate with each other directly using their IP addresses. Each Pod gets its own unique IP address in the cluster, and Pods can reach each other using these IPs.
example:If you have two Pods running on different nodes, they can still talk to each other directly using their IP addresses.
2.Services:
A Service in Kubernetes is an abstraction that allows Pods to communicate with each other without needing to know their exact IP addresses. A Service creates a stable DNS name or IP address that can be used to access a set of Pods.
example:A web application running in one Pod can access a database running in another Pod by referring to the Service name (e.g., database-service) instead of the Pod's IP address.
3.DNS in Kubernetes:
Kubernetes includes a DNS server that allows services and Pods to be discovered by name. This means you don’t need to worry about IP addresses — you can use the Service name, and Kubernetes will resolve it to the right IP address.
For example, if you create a Service named frontend-service, other Pods in the cluster can reach it by using the DNS name frontend-service.
4.Kubernetes allows you to define Network Policies to control which Pods can communicate with each other. This is useful for implementing security and isolation between different parts of your application.
Example: You can set up a Network Policy that only allows Pods in the frontend namespace to access Pods in the backend namespace, and deny access to Pods in other namespaces.

External Networking in Kubernetes:
External networking refers to how your Kubernetes cluster communicates with the outside world (internet or other systems outside the cluster).
1.nodeport
2.load balancer
3.ingress controller

Internal Networking: Kubernetes allows Pods and Services to communicate with each other easily using unique IPs and DNS names. Services like ClusterIP help Pods find each other and talk to one another without worrying about IP addresses.
External Networking: To allow external traffic to reach your cluster, Kubernetes uses NodePort, LoadBalancer, and Ingress. These methods expose your services to the outside world, whether it's through a port on your node, a load balancer, or HTTP routing based on rules.

22.what are persistent volumes and persistent volume claims.
A Persistent Volume (PV) is a piece of storage in your cluster that has been provisioned by an administrator or dynamically by Kubernetes. It represents the actual storage resource that can be used by your applications.
Example: You could have a 100GB disk (like an AWS EBS volume) that you allocate as a Persistent Volume in your cluster. This volume could be used by your database Pod to store its data permanently, even if the Pod is restarted.

A Persistent Volume Claim (PVC) is a request made by a user (or Kubernetes workload) for a specific amount of storage from the available Persistent Volumes in the cluster.
Example: If your database needs 10GB of storage, you can create a PVC requesting 10GB of storage. Kubernetes will either assign an existing PV or create a new one to fulfill the claim.

23.explain the process of binding PV to a PVC in k8's.
In Kubernetes, Persistent Volumes (PVs) and Persistent Volume Claims (PVCs) work together to manage storage for your applications. The PVC is a request for storage by the user, and the PV is the actual storage resource that gets allocated. 
steps:
1. Create a Persistent Volume (PV)
Before a PVC can be bound to a PV, the PV must first exist. The administrator typically creates a Persistent Volume (PV), which represents the actual storage resource in the Kubernetes cluster. This PV can be backed by a variety of storage solutions like cloud storage (e.g., AWS EBS, Google Persistent Disk), NFS, or even local disks.
2.Create a Persistent Volume Claim (PVC)
A Persistent Volume Claim (PVC) is created by the user (or the Kubernetes application) to request a specific amount of storage. It specifies the desired storage size, access mode, and optionally, a storage class that matches the PV.
3.Once both the Persistent Volume (PV) and Persistent Volume Claim (PVC) are created, Kubernetes handles the binding process automatically. Here’s how it works:
1.Matching Criteria which includes storage size,access mode and storage class.
2.Binding: Once a suitable match is found, Kubernetes binds the PVC to the PV. This means the storage is now allocated to the PVC, and the PVC can be used by a Pod.
3.Pod Access: Once the PV and PVC are bound, the Pod that needs persistent storage can use the PVC. The PVC is then mounted into the Pod, allowing the Pod to read from and write to the storage.
4.What Happens After Binding
PV and PVC remain bound: Once the PVC is bound to a PV, they stay bound until the PVC is deleted.

24.what is HPA in k8's.
The Horizontal Pod Autoscaler (HPA) is a feature in Kubernetes that automatically adjusts the number of Pods in a deployment, replica set, or stateful set based on metrics like CPU utilization, memory usage, or custom metrics (like request rates, error rates, etc.). In simple terms, HPA scales the number of Pods up or down based on the workload, ensuring that your application can handle varying amounts of traffic or load.

25.difference between cluster autoscaler and HPA.
Cluster Autoscaler (CA):
Scales nodes in the Kubernetes cluster.
Purpose: When there are not enough resources (CPU, memory) in the cluster to schedule the required Pods, Cluster Autoscaler can add more nodes to the cluster.
Scaling Action: Adds or removes worker nodes (VMs or physical machines) to/from the cluster.
Horizontal Pod Autoscaler (HPA):
Scales Pods in a deployment, replica set, or stateful set.
Purpose: HPA automatically adjusts the number of Pods based on observed metrics (like CPU or memory usage).
Scaling Action: Adds or removes Pods to/from the application based on resource demand.

26.what is RBAC in kubernetes.
RBAC stands for Role-Based Access Control. It's a way of managing and controlling who can access and perform actions in a Kubernetes cluster, and what actions they can perform on different resources. In simple terms, RBAC lets you define roles and permissions for users and services within a Kubernetes environment.

RBAC is like a permissions system that defines who can do what in a Kubernetes cluster.
You can define roles (like "admin", "read-only", etc.), then bind those roles to users or service accounts.
It helps ensure that only the right people or applications can access or modify Kubernetes resources, based on their assigned role.

27.what is difference between Role and clusterRole.
In Kubernetes, both Role and ClusterRole are used in Role-Based Access Control (RBAC) to define what actions users or service accounts can perform on resources. 
Role: Think of it as a local permission system, which works only inside a specific namespace.
ClusterRole: Think of it as a global permission system, which works across all namespaces or for cluster-level resources.

28.what is service account and why would you use it.
A Service Account is like a digital identity for your application (running in a Pod) to interact with Kubernetes. Just like a person needs an ID card to prove who they are, a Service Account lets the application prove its identity to Kubernetes and ask for permission to do things like read data or create resources.
Why Do You Need It?
Applications (running inside Pods) often need to access Kubernetes resources like:
Secrets (passwords or keys)
ConfigMaps (configuration settings)
Pods, Services, or Deployments
But, Kubernetes needs to know who is asking for access and what they are allowed to do. A Service Account helps manage this securely.

29.what is node affinity in k8's.
Node Affinity is a way to tell Kubernetes where (on which nodes) you want your Pods to run, based on certain labels on the nodes.
In simple terms, Node Affinity allows you to control or restrict which nodes a Pod can run on by matching labels attached to the nodes.
In Kubernetes, you have different nodes (machines) that can run your Pods. Sometimes, you might want to run a Pod only on a specific node, or a certain type of node, for reasons like:
Hardware requirements (e.g., you need a node with more memory).
Special configurations (e.g., you want a Pod to run on a node that has a GPU).
Node-specific tasks (e.g., you want certain workloads to run only on nodes in a particular zone or region).
Node Affinity helps you ensure that your Pods are scheduled on the right nodes based on your needs.

30.what is static pods.
A Static Pod in Kubernetes is a Pod that is not managed by the Kubernetes control plane (like the Deployment or ReplicaSet controllers). Instead, it is directly managed by the Kubelet (the agent running on each node).
Node-Specific: Static Pods run only on the node where the configuration is placed.
No Kubernetes Controller: They're not managed by Kubernetes controllers like Deployments, so if the Pod is deleted or crashes, it won't be recreated unless the Kubelet detects the manifest file again.
Used for System Components: Commonly used for essential system Pods (e.g., kube-proxy, DNS) that need to run on specific nodes.

31.what is taint and toleration in k8's.
In Kubernetes, Taints and Tolerations are a way to control which Pods can be scheduled onto which nodes. They help to restrict or allow Pods to run on certain nodes based on specific conditions.
A Taint is a property that you can apply to a node, which prevents Pods from being scheduled on that node unless the Pod has a matching Toleration. Taints are like "labels" on nodes, but instead of describing a node's characteristics, they prevent Pods from being scheduled there unless they explicitly tolerate the taint.
Why use Taints? Taints are used to mark nodes with special conditions (e.g., nodes with limited resources, nodes that are reserved for specific workloads, etc.). They ensure that only specific Pods can run on those nodes.

A Toleration is a property that you can apply to a Pod, which allows the Pod to be scheduled on nodes that have a matching Taint. Tolerations allow Pods to "tolerate" or "ignore" the taints on certain nodes and be scheduled there.
Why use Tolerations? Tolerations are used to make sure that a Pod can be scheduled on a node that has a Taint. They ensure that only Pods that are meant for a specific node can be placed on that node.

32.explain how CNI works in kubernetes.
CNI (Container Network Interface) is a standard that defines how networking should be configured for containers in Kubernetes. It ensures that containers can communicate with each other, the outside world, and the Kubernetes cluster.

Here’s how CNI works in Kubernetes:

1. Pod Networking
In Kubernetes, each pod (a group of one or more containers) is assigned a unique IP address. This allows containers within the same pod to communicate with each other using localhost (127.0.0.1), but it also enables communication between pods across the cluster.

2. CNI Plugins
CNI is a plugin-based architecture. Kubernetes uses CNI plugins to manage pod networking.
Popular CNI plugins include Calico, Weave, Flannel, and Cilium. These plugins define how pods should connect to each other, how they get IP addresses, and how to route traffic between pods.
3. How CNI Works in Kubernetes:
When a pod is created, Kubernetes asks the CNI plugin to set up the network for that pod.
The CNI plugin allocates an IP address to the pod and configures network interfaces to allow communication. It also configures routing, firewall rules, and other network settings.
Once the network is set up, the pod can communicate with other pods and external systems, depending on the network policies defined.
4. Key CNI Features in Kubernetes:
IP Addressing: Each pod gets its own unique IP address from a predefined pool.
Network Policies: CNI plugins can enforce rules about which pods can communicate with each other.
Routing: CNI handles how traffic should be routed between pods and from pods to external services.
Overlay Networks: Some CNI plugins (e.g., Flannel, Weave) use overlay networks to connect pods across different nodes in a Kubernetes cluster.
In Simple Terms:
CNI in Kubernetes is responsible for giving each pod its own IP address, allowing pods to talk to each other, and managing how network traffic flows in and out of the cluster. It uses plugins to set up and control the network according to the cluster’s needs.


########################################################################################################################################

1. explain CNI and CRI interfaces

CNI(Container Network Interface):

* Container Network Interface (CNI) is a specification and set of libraries for configuring network interfaces in Linux containers.

*  CNI's primary job is to provide network connectivity for pods. It handles assigning IP addresses to pods and setting up the 
   networking rules that allow pods to communicate with each other, with external services, and with the host machine.


CRI(Container Runtime Interface):

* The Container Runtime Interface (CRI) is an API that allows the kubelet (the primary node agent in Kubernetes) to use different container runtimes.

* CRI decouples the kubelet from the container runtime, making Kubernetes more flexible. It defines the communication between the kubelet
  and the container runtime, handling container lifecycle management like starting, stopping, and deleting containers.


2. Explain the core components and benefits of a service mesh like istio

A "service mesh" is a networking layer that helps different parts of a large application talk to each other.

* Traffic Management: A service mesh provides fine-grained control over traffic flow. You can easily set up advanced deployment strategies 
  like canary releases and A/B testing, where a percentage of traffic is routed to a new version of a service. It also offers features
  like retries, timeouts, and circuit breakers to improve network resilience and prevent cascading failures.


* Security: It handles security at the network level, independent of the application code. A service mesh can automatically enable 
  mutual TLS (mTLS) to encrypt all service-to-service communication within the mesh and enforce strong identity-based authentication and 
  authorization policies. This helps establish a "zero-trust" network model.

* Observability: Because all traffic flows through the proxies, a service mesh is an ideal place to collect telemetry data.
  It automatically generates metrics (like latency and request volume), distributed traces, and access logs for all service interactions.
  This provides deep insights into the behavior and health of your microservices, making it much easier to monitor and troubleshoot issues. 

Istio is logically divided into two main parts: the data plane and the control plane:

Data Plane: This is the part that handles all network traffic between services. It's composed of a set of intelligent proxies, 
            most often the Envoy proxy, which are deployed as sidecars alongside each microservice. A sidecar proxy intercepts all incoming 
            and outgoing network traffic for its corresponding service.


            Envoy Proxy: A high-performance proxy that handles tasks like traffic routing, load balancing, health checks, and security. 
            Because it's a sidecar, it can manage the network communication for a service without the service needing to be aware of it.

Control Plane: This is the "brain" of the service mesh. It doesn't handle any network traffic itself, but instead manages and configures the
               proxies in the data plane.


               Istiod: In Istio, the control plane is a single binary called istiod. It provides service discovery, configuration,
               and certificate management. It takes high-level routing rules and security policies defined by the user and translates
               them into configurations that the Envoy proxies can understand and enforce at runtime.


3. describe how traffic management like canary or A/B testing is implemented using service mesh

Using Istio, traffic management for a canary or A/B testing deployment is done by configuring VirtualService and DestinationRule resources. 
These rules tell the service mesh's sidecar proxies exactly how to route traffic to different versions of a service.

Step-by-Step Implementation:

* Deploy Multiple Versions: You first deploy both the stable version (v1) and the new version (v2) of your service to the cluster.
  Each version is typically distinguished by a label, like version: v1 and version: v2.

* Define Subsets with DestinationRule: A DestinationRule is used to group the pods of your service into named subsets.
  You create a subset for each version (e.g., v1 and v2) based on the labels you defined in the deployment. This tells Istio which pods 
  belong to which version.

apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: my-app
spec:
  host: my-app
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2

* Route Traffic with VirtualService: A VirtualService is the core component for traffic routing. It defines rules that tell Istio how to route
  requests to the different service subsets.

Canary Deployment: To start a canary, you define a rule to send a small percentage of traffic (e.g., 5%) to the new version and the rest to 
                   the old one.

A/B Testing: For A/B testing, you can use more specific rules, like routing traffic based on an HTTP header. 
             For example, all requests with the header user-type: beta-tester go to v2, while everyone else goes to v1.      


apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: my-app
spec:
  hosts:
  - my-app
  http:
  - route:
    - destination:
        host: my-app
        subset: v1
      weight: 95
    - destination:
        host: my-app
        subset: v2
      weight: 5


4. discuss the challenges of implementing network policies and how would you troubleshoot network connectivity issues within a k8s cluster.

 Challenges of Implementing Network Policies:

 * Default Deny: Many network policy implementations are based on a "default deny" model, which means all pod-to-pod communication is blocked 
   unless explicitly allowed. This can be complex to manage as you must create a policy for every single connection, which can be 
   time-consuming and prone to errors. 

* Lack of Granular Control: Kubernetes NetworkPolicies have limitations in their granularity. For example, they can control traffic based on
  pods and namespaces, but they can't natively control traffic based on HTTP methods (like GET or POST) or specific URLs.   

* Third-Party CNI Plugin Dependency: Network policies are not implemented by Kubernetes itself but by the Container Network Interface (CNI) 
  plugin. Different CNI plugins (like Calico, Cilium, or Flannel) have varying levels of support for network policies, and a policy that 
  works with one plugin may not work with another  

* Complexity and Misconfiguration: As the number of microservices and network policies grows, managing them becomes a huge challenge.
  A single misconfigured policy can unintentionally block critical traffic, leading to unexpected application failures.


Troubleshooting Network Connectivity Issues in Kubernetes:

1. Check Pod Status and Logs

  * Verify Pods are Running: The first step is to check if all pods are in the Running state and if they are ready to serve traffic.
    Use the command kubectl get pods --all-namespaces.

  * Inspect Pod Logs: Look for any connectivity errors in the application's logs using kubectl logs <pod-name>. 
    The logs may reveal failed connection attempts or DNS resolution issues.


2. Test Basic Pod-to-Pod Connectivity

  * Ping Test: Use a debug pod with network tools to test connectivity. Run kubectl run -it --rm --image=busybox:1.28.4 --restart=Never debug-pod -- ping <destination-pod-ip>.
    This verifies if a pod can reach another pod at the IP layer.

  * Service Connectivity: Check if a pod can connect to a service. You can use a tool like netcat to verify if the pod can reach the service's
    cluster IP and port.

3. Analyze Network Policy Behavior

  * Check Policies: List all network policies in the namespace with kubectl get networkpolicies.

  * Inspect and Understand Policies: Use kubectl describe networkpolicy <policy-name> to see what a policy is actually doing. 
    Verify if the podSelector and namespaceSelector are correct and if the ingress and egress rules allow the necessary traffic.

  * Temporarily Disable Policies: As a last resort, if you suspect a network policy is the cause, you can temporarily disable 
    it to see if the connectivity is restored.

4. Inspect Kubernetes Networking Components

  * Verify CNI Plugin: Confirm the CNI plugin is correctly installed and running. Check the CNI plugin's pods (e.g., Calico or Cilium pods)
    for any errors in their logs.

  * Inspect kube-proxy: The kube-proxy service handles service routing. Ensure it's running correctly on all nodes.
    You can inspect its logs for any issues with updating IPtables rules.

5. Advanced Troubleshooting

  * Use a Network Diagnostics Tool: Tools like kubectl-debug or ksniff can help you capture network traffic inside a pod to see what's
    happening at the packet level.

  * Check Node-Level Networking: If all else fails, the issue might be on the host node. Check the node's firewall rules (iptables -L)
    and network interfaces to ensure they are configured correctly and not blocking traffic.


5. what is PDB in kubernetes.

   A Pod Disruption Budget (PDB) is a Kubernetes resource that limits the number of pods of an application that can be simultaneously unavailable 
   during voluntary disruptions. It helps ensure your application remains highly available during planned events like node maintenance or 
   cluster upgrades. The PDB specifies either a minimum number of available pods or a maximum number of unavailable pods.

6. how do you make a k8s cluster highly available

To make a Kubernetes cluster highly available, you need to ensure the control plane has no single point of failure. 
This is primarily achieved by running multiple master nodes, typically three or five, with a replicated etcd database for state consistency.
A load balancer is used to distribute traffic across the API servers on these masters. Additionally, having multiple worker nodes and 
using a persistent storage solution ensures that applications and their data remain available even if a node fails.

7. how do you stop a pod in k8s

In Kubernetes, there's no native "stop" command for a pod. Instead, you effectively stop a pod by deleting it. 
i.e kubectl delete pod pod_name

if it is a deployment then it will create the new pod when you delete it then use this command.
i.e kubectl scale deployment deployment-name --replicas=0


8. What is PDB(Pod Disuption Budget).

A PDB is a Kubernetes resource that lets you specify the minimum number of pods for an application that must be available at all times. 
The main purpose of a PDB is to ensure that your application remains highly available during voluntary disruptions

9. How does k8s DNS resolution work for pods and services.

Kubernetes DNS resolution enables pods and services to find and communicate with each other using names instead of dynamic IP addresses. 
This is handled by an in-cluster DNS service, typically CoreDNS, which is deployed in the kube-system namespace. Each pod is configured 
to use this DNS service for name lookups.

Kubernetes DNS resolution relies on an in-cluster DNS service, typically CoreDNS. Each pod's /etc/resolv.conf file is automatically configured 
to use CoreDNS as its nameserver. When a pod makes a DNS query for a service or pod name, CoreDNS resolves it to the correct internal IP address

for pods:
Pods can also get DNS names, although this is less common for general communication since services provide a more stable endpoint. Pod DNS names are typically used in stateful workloads (e.g., StatefulSets) or for direct pod-to-pod communication.

The DNS name for a regular pod is typically its IP address, with dashes replacing the dots, and then appended with the namespace and .pod.cluster.local.

Example: A pod with IP 10-0-0-50 in the default namespace would have a DNS name of 10-0-0-50.default.pod.cluster.local.

For pods in a StatefulSet, the DNS name is more stable and is in the format:
<pod-name>.<service-name>.<namespace>.svc.cluster.local

Example: A pod named web-0 managed by a StatefulSet with a service named web in the prod namespace would have a DNS name of web-0.web.prod.svc.cluster.local


for services:

Services are assigned an A record that resolves to the service's ClusterIP. This allows pods to connect to the service without knowing the IP addresses of the individual pods behind it. The DNS name format is:

<service-name>.<namespace-name>.svc.cluster.local

Example: A pod in the web namespace trying to connect to a service named database in the backend namespace would use the FQDN: database.backend.svc.cluster.local.

Pods can also use shorter names if the target service is in the same namespace, e.g., simply database. This works because Kubernetes automatically appends the pod's namespace and other suffixes to resolve the name.



10. how does kubernetes garbage collection work including finalizers and termination grace periods.

Kubernetes garbage collection automatically cleans up resources based on owner references, which establish parent-child relationships between objects.
When an owner is deleted, its dependents are also removed. Finalizers are special keys that prevent an object's immediate deletion until a controller performs
necessary cleanup tasks. The termination grace period gives a pod a specific amount of time to shut down gracefully after a deletion request before being forcefully 
terminated. These mechanisms ensure a clean and controlled teardown of resources.


11. they are 200 pods and 100 pods fails due to IP shortage. how would you fix it.

An IP shortage for pods means your Pod CIDR range is too small. To fix it, you need to change the cluster's network configuration, which depends on your deployment.
For managed services like EKS, you can add a secondary CIDR range to your VPC and configure your cluster's CNI plugin to use it. On self-managed clusters,
you'd modify core files like the kube-controller-manager manifest to specify a larger CIDR block. Another solution is to adjust CNI settings to prevent
IP pre-allocation on nodes. Ultimately, migrating to IPv6 is the most robust, long-term fix, as it provides an almost unlimited address space.


12. what is OIDC(OpenID Connect). where it is used and alternatives.

OpenID Connect (OIDC) is an authentication protocol built on top of OAuth 2.0. It uses JSON Web Tokens (JWTs), called ID tokens, to securely verify a user's identity.
While OAuth 2.0 is for authorization, OIDC provides the authentication layer, confirming who the user is. It's widely used for social logins (e.g., "Sign in with Google")
and Single Sign-On (SSO) in web and mobile applications. Alternatives include SAML, which is an older, XML-based protocol often used in enterprise environments, 
and various proprietary solutions.






















