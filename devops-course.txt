Disadvantages of agile:
                      1.Cost is more when you want to change any thing at later stages.
                      2.No product is released or visible till the end of the process.

Main process of deployment the code in production is: 
                                                      Devlopers devlop the code and push into
the github.after that code is moved into the bulid server there the code is bulid,tested and
evaluated then it produces software or artifact.This software or artifact is moved into 
repository based on the programmming language like war or jar files.After that,this artifact
is moved into the further testing.If everything is okay then it moved into the production.

problem:
       devlopers devlops some piece of code continuosly for 3 weeks and then pushes the code
into the github and then it is moved into the bulid server,there if any bulid issues occurs.
It is very difficult for the devlopers to resolve many issues at that time and the bugs are 
detect at final stage rather than earlier stage.

solution:
         Whenever the devlopers devlops the code,they will push the code into the github and
and the code is pushed into the bulid server continuously if there are any issues occured at
bulid server,devlopers will notify the issue and at the earlier stage the problem will be sovled.
This process is called continuous integration(CI). 

CI ==> code -> fetch --> bulid --> test --> notify --> feedback -->code(cyclic way) 

tools --> 1.code --> eclipse,vscode,pycharm etc
          2.version control system --> Git
          3.bulid tools --> maven,ant,gradle etc based on programming language
          4.repositories(to store artifacts) -->sonatype nexus,jfrog artifactory,archiva
                                                cloudsmith package,grunt
          5.CI tool --> jenkins

The main goal of CI is to detect the bugs at earlier stage.

Continuous Delivery(CD): 
                        Continuous delivery automates the entire software release process.
Every revision that is committed triggers an automated flow that builds, tests, and 
then stages the update. The final decision to deploy to a live production environment
is triggered by the developer.(after bulid to production everything is automated but the
live production need approval from developer).

The main difference between Continuous Delivery and Continuous Deployment:

         Continuous Delivery after build release,everything is automated expect final stage
i.e to move the software into live production(this is manual process.It need approval from
the developer)

        Continuous Deployment after bulid release,everything is automated till the last stage
i.e move the software into the live production(It does not need approval from the developer).


virtualization:
                It is used to create virtual representation of storage,server and physical
machines(os) that run on one physical os.



virtualization manual setup:                                virtualization automated setup:

1.Oracle Virtual VM (tool)                                    1.Oracle Virtual VM (tool)

2.ISO file (installation file                                 2.vagrant(automatically creates
 which we attached to our vm)                                   vm's for us with vagrant file)     
                                                                   

3.Login tool (git bash and Putty)                             3.commands(vagrant up)


manual vm problems:
                   1.OS Installation
                   2.Time Consuming
                   3.since it is manual process there is a chance of human error.

Vagrants commands :
                     1.vagrant up -> To bring up the vm
                     2.vagrant halt -> To power off the vm
                     3.vagrant destroy -> To delete the vm
                     4.vagrant status -> To see whether vm is running or not
                     5.vagrant global-status -> To see all the vms status 
                     6.vagrant ssh -> To login into linux
                     7.vagrant box list -> To see all the downloaded vagrant box files in the local system.
                     8. vagrant init box-name -> It will create the Vagrant file.
                     9. cat Vagrantfile -> to print the Vagrantfile content.
                     10.vagrant reload -> to reboot the vm
                     11.vagrant global-status --prune -> to remove the old entries of vm's.

Note:
     To move into different file directories
     1.pwd ->present working directory (current folder)
     2.cd /c/vagrantvms -> this will move into the vagrantvms folder
     3. cd .. -> this will come to the previous folder (i.e c folder)
     4. mkdir centos -> this will create a folder called centos

Some important directories in linux:
                                     1.Home Directories -> /root for root user home directory
                                                           /home/username for normal user home directory
                  
                                     2.User Executables -> /bin the commmand that we run
                                                            like ls,pwd are present here (store here)
                                                                    
                                                                     or here 

                                                           /usr/bin,/usr/local/bin.
                                                           These commands are executed by normal user

                                     3.System Executables -> /sbin,/usr/sbin,/usr/local/sbin
                                                             These commands are executed by root user
                                                              like installing software,adding user etc.

                                     4.Other MountPoints -> /media,/mnt
                                                           If you connect any external devices
                                                           like pen drive,usb it will mounted here.

                                     5.Configuration  -> /etc 
                                                         Configuration data is store here like
                                                         network configurations,server configurations etc.

                                     6.Temporary Files -> /tmp
                                                           To store any temporary data.it will delete
                                                           data whenever its reboot.

                                     7.Kernels and Bootloader -> /boot


                                     8.Server Data -> /var,/srv
                                                      If you are running web server,web site files
                                                      are put here or sql data.

                                     9.System Information -> /proc,/sys


                                     10.Shared Libraries -> /lib,/usr/lib,/usr/local/lib
                                                            popular libraries stored here. 


Linux:

     Basic Commands:
                    1.whoami -> to see what user your(root user or normal user)
                    2.pwd -> to see the present working directory(current path)
                    3.ls -> to see the files in current folder
                    4.cat /etc/os-release -> to see which operating system we are using and its version. 
                    5.cd / -> means root directory(in this directory all the directories(folders) are present)
                    5.cd /root -> root user directory
                    6.cd or cd ~ -> whenever you want to come to home directory.
                    7.touch touchfile1.txt -> touch command is used to create the empty file
                    8.touch devopsfile{1..10}.txt ->it will create the 10 devops file(like devops1,devops2..devops10)
                    9.cp devopsfile1.txt dev/ -> it will copy the devopsfile1.txt file to dev folder(here source is devopsfile1 and destination is dev folder)
                    10.cp -r dev backup/ -> This is used to copy the entire directory or folder to another directory(source is dev and destination is backup)
                    11.mv devopsfile3.txt ops/ -> this is used to move the file or folder to another folder(source is devopsfile3.txt and destination is ops)
                    12.mv ops dev/ ->here we are moving entire ops directory or folder into dev directory(source is ops and destination is dev).
                    13.mv devopsfile2.txt devopsfile22.txt -> this rename the file name from devopsfile2.txt to devopsfile22.txt.
                    14.mv *.txt dev/ -> this command is used to move the all files that ends with .txt will be moved into the dev directory
                    15.rm devopsfile2.txt -> it will remove that file(only files not folder)
                    16.rm -r dev -> this will remove the entire folder.
                    17.rm -rf * -> it will remove the all the files from the current working directory
                    18.ip addr show -> to show the ip address 
                    

Note:
     1.absolute path means complete path of that directory or folder.
     2.relative path means just that directory or folder name 
     3.sudo yum install vim -y -> to install vim editor in linux(centos machines).

vim editor:
           It has three modes: 
                               1.command mode
                               2.insert mode(edit mode we can edit changes in this mode) i.e click i to enter into the edit mode
                               3.extended mode(if you want to do more operations like save or quit from vim editor) i.e : is used to extended mode like :w to save and :q to quit

By default it will in command mode.
if you are in insert mode you want to come back to command mode use esc key.

vim devopsfile1.txt -> this command is used to enter into vim editor and make changes in the file.
cat devopsfile1.txt -> this is used to print the content that is present in the file.
:q! -> this is forcefully quitting from the vim editor without saving the changes.
:wq -> this will save the content and quit from vim editor.
cat command is used to print the content.
:se nu -> in vim editor to see the numbering for the lines. 

Note:
      while working with directories or folder(like move,copy, delete, cd, operations):

      1.if you are in same directory then file name is enough.
      2.if you are in different directory and trying to do operations in different directory
        you need to give absolute path(complete path).


vim editor shortcuts:
                      1.gg -> to move from last line to first line
                      2.G -> to move from first line to last line
                      3.yy -> to copy single line
                      4.p -> small p (p) paste the line below and captial p (P) paste the line above.
                      5.4yy -> to copy multiple line(4 lines)
                      6.dd -> to delete the single line
                      7. u -> to undo
                      8.5dd -> to delete 5 lines 
                      9./anything -> to search the content 

To do any operation you should be in command mode.

Types of files in Linux:
                         1.- is for regular files
                         2.d is for directory
                         3.l is for link
                        
1.ls -lt -> it will sort latest one to first according to the time stamp.
2.ls -ltr -> it will sort the latest one to the bottom
3.vim /etc/hostname -> By this we can change the hostname after that we need to run another command -> hostname typehostname
4.file filename -> this command is used to check the type of file


Filters:
        1.grep -> This command is used to search the content in a file
                   i.e grep firewall anaconda-ks.cfg

                  In this firewall is word to be search in anaconda-ks.cfg file.


Note:
     1. In Linux,everything is case-sensitive.

     2.If you want to search anything in a file without case-sensitive use -i in above
       i.e grep -i firewall anaconda-ks.cfg.here i indicates without case-sensitive. 

     3.If you want to search anything in the current directory use -> grep -i firewall *
       it will search only files not directories (folder).

     4.If you want to search anything in the current directory irrespect of files and folders
       use -> grep -iR firewall *.this -R indicate that search in both files and directories. 

     5.If you dont want to search for that particular word (show content except this word)
       use -> grep -vi firewall *.this -v indicates show content except this word.


2.less commmand -> This is used to print the content just like cat command but 
                    only difference is it looks like vim editor.we can search content etc.
                    use -> less anaconda-ks.cfg

3.more command -> This is just like less command only difference is, in less command you can move
                  the content using up and down arrow and in more you should use enter button.
                  Content percentage also it will show. use -> more anaconda-ks.cfg

4.head command -> if you want to see first 10 lines of the file use -> head anaconda-ks.cfg.
                                          or
                  if you want to use first 20 lines use -> head -20 anaconda-ks.cfg

5.tail command -> if you want to see last 10 lines use -> tail anaconda-ks.cfg
                                      or
                  if you want to use last 20 lines use -> tail -20 anaconda-ks.cfg

6.cut command -> if you want to filter username or anything based on delimiter
                 use -> cut -d: -f1 /etc/passwd.here f1 is column one and : is delimiter.

7.if you want any thing to search and replace the word:
                                                         2 ways

                 1.using vm editor (in extended mode) -> %s/coronavirus/covid19
                   this will replace one time in single line. 
 
                   if cornovirus word present in more than one time in the same line then use -> %s/coronavirus/covid19/g 
                
                2.sed command -> sed -i 's/coronavirus/covid/g' samplefile.txt


I/O Redirections:
                  i/o redirection means input/output is redirect to some files rather than showing on the screen.

                  1. > -> this is the symbol for output redirection.
                  2. < -> this is the symbol for input redirection. 
                  3. If you don't want to override the content i.e just append(add) use -> >>
                  
echo -> this command is used to print content what ever we give( echo "hello" -> it will print hello).


Piping:
       1.You can use the wc command to count a file's word, line, character, or byte count.
       2. wc -l /etc/passwd -> wc -l is used to count no.of lines in a file  or folders in directory.
       3. ls | wc -l -> here ls gives output and pipe takes that output as input to the wc -l command and print
          the total no.of lines
       4. head anaconda-ks.cfg | grep by -> here head anaconda-ks.cfg will print first 10 lines as output
          the pipe takes that output as input and grep is used for searching it will search for by and 
          it will print the output.

find command -> This command is used to locate or search for the files i.e find /etc -name host*
                here /etc is the path and -name is search by name and host* is that starts with.



Users and Groups:
                  users is of three types: 
                                           1.root
                                           2.regular
                                           3.service(no login)

         
root:x:0:0:root:/root:/bin/bash -> here root is username,x is shadow encrypted password,0 is userid,0 is groupid
                                   root is comment,/root is home directory,/bin/bash is login shell.

1.adduser username -> This command is used to add the user.
2.groupadd groupname -> This command is used to add the group.
3.passwd ansible(here ansible is the username) -> This command is used to reset or set the password for user.
4.su - ansible(username) -> This command is used to switch from one user to another user.
5.userdel aws(username) -> This command is used to delete the user but still home directoy is present,
                           if you want to delete user with home directory use -> userdel -r aws.
6.groupdel devops(groupname) -> This command is used to delete the group..
                           

If you want to add users to the group they are two ways:
                                                         1.usermod -aG devops ansible
                        (here -aG is the supplementory group , devops is groupname and ansible user).

                                                         2.vim editor(just edit the file add name in the group).


File Permissions:
                 In this first character is file type,next three is for user,next three is for group,last three is
                 for others.

                 -rw-------. 1 root root 2026 Feb 16 07:21 anaconda-ks.cfg
                 lrwxrwxrwx. 1 root root   37 Feb 16 07:35 cmds -> /opt/dev/ops/devops/test/commands.txt
                 -rw-r--r--. 1 root root   94 Feb 18 05:55 example.txt
                 -rw-------. 1 root root 1388 Feb  5 15:50 original-ks.cfg
                 drwxr-xr-x. 2 root root    6 Feb 18 05:53 redir


here d -> file type
rwx -> user                        r -> read, w -> write, x -> execute
r-x -> group
r-x -> other

1.To change the ownership of the file use -> chown ansible(username):devops(group) redir(filename)

2.To change the read,write and execute permissions for others use -> chmod o-x redir(filename)
  (here o is others amd redir is filename)

                drw-r--r-x. 2 jenkins devops 25 Feb 18 08:20 redir

3.To change the read,write and execute permissions for group use -> chmod g+r redir(filename)
  (here it will add read permissions)

                                    or


Change Permissions using numeric methods:
                                         4(for read r)
                                         2(for write w)
                                         1(for execute x)


drw-r--r-x. 2 jenkins devops 25 Feb 18 08:20 redir

to change permissions,for user and group full acess and others no acess use -> chmod 770 redir(filename).
(here 1st digit for user, 2nd digit for group and last digit for others).



Services:
         services that are running in our machine and know their status.

         1.systemctl status httpd -> To see the status whether it is active or not.  i.e here httpd is the service
         2.systemctl start httpd -> To start the service
         3.systemctl stop httpd -> To stop the service                                           
         4.systemctl restart httpd ->To restart the service
         5.systemctl reload httpd ->To reload the changes without even restarting it.
         6.systemctl enable httpd -> modifies the service configuration to tell systemd that the service needs to start up automatically on boot.

provising -> provising in vagrant means executing script or commands when vm is up.
provising executes only one time.

Steps to deploy website manually:
                                 1.first install the dependencies.
                                 2.start the server(using systemctl)
                                 3.manage the configurations if needed
                                 4.deploy the website(download the website and unzip it and restart the server)

Note:
     tooplate.com -> ready-made websites are available.

Steps to deploy website automatically:
                                       1.In vagrantfile add the provising(scripts or commands)then everything get ready.









Networking: 
            OSI(Open Systems Interconnection) Model : 
                       physical layer(0's & 1's) -> data link layer(frames) -> network layer(packets) -> transport layer -> session layer ->

                       presentation layer -> application layer. 


Whenever the Physical layer receives the signal it converts into bits (0's & 1's) and it sends to Data Link layer.
The data link layer make sure that the signal is error free from node to node over Physical layer, in this layer
data is in frames.Then the data will pass to Network layer,the data is in packets and then data will pass to
Transport layer,in this data will check end to end connections.session layer is to establish,manage and terminate
session .presentation layer is to encrypt and decrypt the data.application layer is to allow acess to network resource
 



OSI Model             DOD Model            Protocols             Devices/Apps


5,6,7 layers         Application           dns,https,ssh etc      web server, mail server etc
                     layer                 


layer 4              host to host          tcp/udp                gateway


layer 3              internet              ip                     router,firewall layer 3 switch


layer 2              network acess         MAC Address            bridge layer 2 switch


layer 1              network acess         eternet                hub



Note:

     1.ipv4 range -> 0.0.0.0 to 255.255.255.255
     2.ip is of two types -> public ip is internet
                             private ip is local network

private ip is classfied into 3 classes and their ranges are
                                                            1.class A => 10.0.0.0 to 10.255.255.255
                                                            2.class B => 172.16.0.0 to 172.31.255.255
                                                            3.class C => 192.168.0.0 to 192.168.255.255


Protocols & port numbers:

 
Label on column        Service name                      UDP and TCP port numbers

1.DNS                  Domain name service-UDP           UDP 53

2.DNS-TCP              Domain Name Service-TCP           TCP 53

3.HTTP                 Web                               TCP 80

4.HTTPS                Secure Web(SSL)                   TCP 443

5.SMTP                 Simple Mail Transfer Protocol     TCP 25

6.POP                  Post Office Protocol              TCP 109,110

7.SNMP                 Simple Network Management         TCP 161,162 UDP 161,162

8.TELNET               Telnet Terminal                   TCP 23

9.FTP                  File Transfer Protocol            TCP 20,21

10.SSH                 Secure Shell(terminal)            TCP 22

11.AFP IP              Apple File Protocol/IP            TCP 44,548



Network Commands:
                  1.ifconfig -> it will show all active network interfaces with names
                                    or
                    ip addr show
                
                  2. ping ip(type total ip address) -> to check whether this ip is connected or not
                  
                  3.netstat -antp -> to see the open ports.



Containers:
            1.Containers offer Isolation and not virtualization.
            2.For virtualization we can use Virtual Machine.
            3.For understanding we can think containers as OS virtualization.

Note:
      Virtualization enables you to run multiple operating systems on the hardware of a single physical server,
      while containerization enables you to deploy multiple applications using the same operating system on a single virtual machine or server.

Docker:
       1.It is a container runtime environment for developing,shipping and running the applications.
       2.Docker registry is place where Docker Container Images are stored.
       3.Docker containers are created from Docker Images

Note:
      1.docker run hello-world -> here docker run means create a container.
      2.docker images -> this will show you available images on your computer.
      3.docker ps -> this will show you running containers.
      4.docker ps -a -> this will show you all the containers(both running and stopped containers).
      5.docker run --name web01 -d -p 9080:80 nginx -> here web01 is the container name
                                                            -d means run in background
                                                            -p means port
                                                             9080 means hostport and 80 means container port.
        here instead -p(small p),if you give -P(captial P) then portnumbers automatically created.

      6.docker inspect web01(container name) -> it will show the ip address.
      7.docker stop web01(container name) -> it will stop the container.
      8. docker rm web01(container names) -> this will delete the containers.
      9. docker rmi image-id -> it will remove the docker images.
      10.if you want to run multiple containers together their is a concept called docker-compose.
      11.docker compose up -d -> this will start the docker all containers
      12.docker compose down -> to stop all the containers
      13.docker system prune -a -> to remove all the stopped containers.







Bash Scripting: 
                In bash script file start with -> #!/bin/bash.

variables:
          These are temporary storing of data in the memory.

          1.Skill="Devops":
                        if you want to acess the values use -> $ (echo $Skill it will print Devops).
        
          2.PACKAGE= "wget unzip httpd":
                                         we can use like -> sudo yum install $PACKAGE -y.
                                         it will download the dependencies.

command line arguments:  
                        means passing the data or value at the time of executing the file.

For example,if you have created a file name with scripts.sh and inside it has the following data

                    #!/bin/bash
                    echo "the value of 0 is :"
                    echo $0

                    echo "the value of 1 is :"
                    echo $1

                    echo "the value of 2 is :"
                    echo $2

when you execute this file it will print, value of 0 is scripts.sh(file name)
                                         value of 1 is empty line
                                         value of 2 is empty line.

if you give argument while executing like ./scripts.sh aws jenkins

output is : 
            value of 0 is ./scripts.sh(file name)
            value of 1 is aws
            value of 2 is jenkins.

Note:
     1.$0 is always the name of the bash script
     2.$1 to $9 are the command line arguments
     3.echo $USER -> this will print the username
     4.echo $HOSTNAME -> it will print the hostname
     5.echo $RANDOM -> it will print the random number.
     6.echo $? -> this is the exit status of last command(0 means sucessful and other than 0 means unsucessfull).



command substitution:
                      Command substitution is method of storing OUTPUT of a command into variable.
                       
                       Two ways to do command substitution:
                                                           1.``
                                                           2.$()
                     1. If you want to store any output to a variable use ->` `
                        UPTIME_1=`uptime`
                        echo $UPTIME_1 ->this will gives the uptime status

                    2.CURRENT_USER=$(who)
                      echo $CURRENT_USER ->This will print the username.


Exporting Variables:
                     variables are temporary stored,if you exit from the shell the data will be gone.

If you want to store the variables permanently then,
  
                         Two ways:
                                   1.export variable available only for that user then edit .bashrc file(this is available in home directory ls -a)
                                   2.export variable available globally(all users) then edit /etc/profile file.


User Input:
           If you want to take input from the user while executing script use -> read name(any variable).
       
    In user-script.sh file,
                          
                            #!/bin/bash
                      
                            read skill
                            echo "your skill is : $skill"

                            read -p user:usr          -> here -p means prompt
                            read -sp password:pwd     -> here -s means suppress(hide)

                            echo "welcome $usr".


Conditional statements:(if and if-else)
                      
                        #!/bin/bash

                       read -p "Enter the number:" NUM
                       echo

                       if [ $NUM -gt 100 ]   => here gt means greater than
                       then
                           echo "you have entered if block"
                           sleep 3
                           echo "number is greater than 100"
                       else
                           echo "number is less than 100"

                       fi  => here fi means end of condition.


                     (if-elseif)

                     #!/bin/bash

                     value=$(ip addr show | grep -v LOOPBACK | grep -ic mtu)

                     if [ $value -eq 1 ]
                     then
                         echo "one active network interface"

                     elif [ $value -gt 1 ]
                     then
                         echo "found more than one network interface"

                     else
                         echo "no active network interface"

                     fi


Looping:
         for loop:
                   #!/bin/bash
                   for VAR1 in java devops react linux   => here VAR1 is variable name
                   do
                     echo "loop starting"
                     sleep1
                     date
                     echo
                   done     => here done means end of for loop.



                  #!/bin/bash
                  MYUSER="alpha beta gamma"
                  for usr in $MYUSER
                  do
                    echo "adding user $usr"
                    adduser $usr
                    id $usr
                  done


While loop:
           It will run the loop as long as condition is true.
        
        
             #!/bin/bash

            counter=0
            while [ $counter -lt 5 ]
            do
              echo "loop starting"
              echo "the value of counter is $counter"
              counter=$(( counter + 1 ))
              sleep 1
           done

          echo "out of loop"



        Infinite loop:
  
                       #!/bin/bash

                       counter=3
                       while true
                       do
                         echo"starting loop"
                         echo "the value of counter is $counter"
                         counter=$(( counter * 3 ))
                         sleep 1
                       done

                       echo "out of loop"



Remote command execution:   This means from one vm we can connect to other vm's.

                          1.first change the hostname of all the vm's
                          2.In one vm add the ip address and hostname (i.e 192.168.10.13 web01) in /etc/hosts
                          3.check whether the ip is connected or not( ping web01(hostname).
                          4. ssh vagrant@web01 then it will ask for password(password is username)
                          5.add the user using adduser devops and passwd devops
                          6. add permission to user in visudo file.(devops ALL=(ALL)  NOPASSWD: ALL)
                          7.ubuntu by default doesnot allow to login through password.we need login into web03(ubuntu)
                            and change the permissions i.e enable password login (/etc/ssh/sshd_config ) and
                            then restart ssh(systemctl restart ssh).
                          8. again login into one vm , do ssh vagrant@web03 now it will ask for password 
                             login and add user devops,password devops and edit permission in visudo file
                             (export EDITOR=vim and then visudo)
                          9.now try ssh devops@web01 uptime -> it will ask for password and then displays uptime.
                            without even login into web01 vm we are executing the commands.


SSH Key Exchange:
                
                  It is another way to login remotely without password(it is key based login)

                  1.ssh-keygen -> it will generate the key-pair(/root/.ssh/id_rsa it will store here).
                  2.ssh-copy-id devops@we01 -> it will ask for the password and it will copy the lock to this vm.
                  3.ssh devops@web01 uptime -> now it will not ask for password to login, it will directly execute
                                               the command

Note:
 
         for host in `cat remhosts` 
         do
            echo devops@$host
            date 
         done

         here remhosts -> a file which contains hostnames(web01,web02,web03) and it will print the date



         #!/bin/bash

#PACKAGE="wget unzip httpd"
TEMPDIR="/tmp/webfiles"
URL=" https://www.tooplate.com/zip-templates/2108_dashboard.zip"
ART_NAME="2108_dashboard"
#SVC="httpd"

yum --help &> /dev/null

if [ $? -eq 0 ]
then
    PACKAGE="wget unzip httpd"
    SVC="httpd"

    echo "running on centos"


    sudo yum install $PACKAGE -y > /dev/null

    sudo systemctl start $SVC
    sudo systemctl enable $SVC

    mkdir -p $TEMPDIR
    cd $TEMPDIR
    wget $URL > /dev/null
    unzip $ART_NAME.zip
    sudo cp -r $ART_NAME/* /var/www/html/

    systemctl restart $SVC

    rm -rf $TEMPDIR

    systemctl status $SVC

else
    PACKAGE="wget unzip apache2"
    SVC="apache2"

    echo "running on ubuntu"

    apt update
    sudo apt install $PACKAGE -y > /dev/null

    sudo systemctl start $SVC
    sudo systemctl enable $SVC

    mkdir -p $TEMPDIR
    cd $TEMPDIR
    wget $URL > /dev/null
    unzip $ART_NAME.zip
    sudo cp -r $ART_NAME/* /var/www/html/

    systemctl restart $SVC

    rm -rf $TEMPDIR

    systemctl status $SVC

fi 



               #!/bin/bash

               USR="devops"

               for host in `cat remhosts`
               do
                  echo "connecting to $host"
                  echo "pushing the script to $host"
                  scp mulos_setup.sh $USR@$host:/tmp/    => here scp is used to push the file into others vm's.
                  echo "executing script on $host"
                  ssh $USR@$host /tmp/mulos_setup.sh
                  ssh $USR@$host rm /tmp/mulos_setup.sh
                  echo
              done









AWS:

1.EC2:
       Elastic Compute Cloud provides web services api for provisioning,managing and deprovisioning virtual server
       inside amazon clouds.

Ec2 pricing:

             1.On-Demand(pay per hour or even seconds)
             2.Reserved(Reserve capacity for 2-3 years for discounts)
             3.Spot(Bid your price for unused ec2 capacity)
             4.Dedicated Hosts(physical servers dedicated for you)



Components inside ec2 instance:
                               
                                1.AMI(Amazon Machine Image):
                                                            AMI provides the information requried to launch an instance
                                                            which is a virtual server in cloud(this like vagrant box list)

                                2.Instance Type:
                                                when you launch an instance,the instance type that you specify determines
                                                the hardware of the host computer used for your instance.
                                                (how much size and memory required)

                                3.EBS(Amazon Elastic Block Store):
                                                                   Amazon ec2 provides you with flexible,cost effective and
                                                                   easy to use data storage options for your instances.

                                                                   EBS is like virtual hard drive in which you can store 
                                                                   your OS and your data.

                               4.Tags:
                                      Tag is simple label consisting of customer-defined keys and optional values that can
                                      make it easier to manage,search for,and filter resources.

                               5.Security Groups:
                                                  A Security Group acts as a firewall that controlls the traffic for one
                                                  or more instances.

                               6.Amazon EC2 uses the public-pair cryptography to encrypt and decrypt the login information.


Note:
     1.In ec2,whenever the ec2 instance stopped,the public ip will be gone and private ip
       will be there.
     2.when you start the ec2 instance again,the new public ip will generate and private ip
       remain same.

     3.if you want static ip(fixed ip) then you can use elastic ip's.In right menu search for
        elastic ip and select the region and click on create
     4.it will create the elastic ip's and we can associate that ip to our ec2 instance.


AWS CLI:
         if you want to connect aws through command line use follwing steps:
         
         1.install aws cli in your machine.
         2.create user or existing user also fine, in user setting search for acess keys
           and create acess keys 
         3.open gitbash and type aws configure, it will ask for acesss key,secret acess key
           output format is json and region. then configuration is done


  aws sts get-caller-identity -> this command shows the account ID and account number you are
                                 using.



EBS(Amazon Elastic Block Store):
                                 Amazon ec2 provides you with flexible,cost effective and
                                 easy to use data storage options for your instances.

                                 EBS is like virtual hard drive in which you can store 
                                 your OS and your data in the form of volumns.
                                
                                 snapshot is backup of a volumn.


          EBS Types:
                     1.General purpose(SSD-Solid State Drive) - most work loads

                     2.Provisioned IOPS - Large Databases.

                     3.Throughput Optimized HD - big data and data warehouse.

                     4.Cold HDD(Hard Disk Drive) - file servers.

                     5.Magnetic - backups and Archieves.


If you want to store anything separately then follow the below steps:
 
 1.first you need to create a volume(choose the size based on your requirment).
 2.Attach that volume to the instance(just make sure that volume and instance is in same zone).
 
 fdisk -l -> which will show all the disks.

 3.volume we have created for this one,partition is not present we need to create it.

             first step is to create the partitioning.

 4.fdisk /dev/xvdf(hard disk path) -> it will open the hard disk utility.
   click m -> for help
 5. n -> to create the partition.and then click enter upto last step.
 6. p -> to print the disk partition.
 7. w -> to write the partitions(partitions are created).

            next step is to formatting it.

8.mkfs and click the tab button 2 times -> it will show all the available utilities.
9.choose any utility you want and mkfs.ext4 /dev/xvdf1(hard disk path) click enter.
10.the partition is formatted with ext4 format.

           next step is to mount it.

11.create a directory  in temp directory and moves all images file to created directory.
   here mkdir /tmp/img-backups
        mv /var/www/html/images/* /tmp/img-backups/
12.now the images directory is empty.
13.This is a temporary mount -> mount /dev/xvdf1(hard disk path) /var/www/html/images/(where you want to mount i.e path).
14. run df -h command to see the mounted.(how much is used and how much is available)
15.if you want unmount it(delete it) -> umount /var/www/html/images/(path)

now lets see the permanent mount

16.open vim /etc/fstab -> add /dev/xvdf1	/var/www/html/images/	ext4	defaults	0 0
17. mount -a -> this will mount all the entries from fstab file.
18. next is to move the images from temp directory to mounted directory.
    mv /tmp/img-backups/* /var/www/html/images/
19.restart the service -> systemctl restart httpd
20.check the status of the service if mounting is failed then the service will not run -> systemctl status httpd.
21.check in the browser whether it shows images or not.



EBS Snapshots:
               Snapshots are usually to backup's and restores the data.

 
              we will use the previous instance for the backup

      for that first, change the name of the instance and deattach the volume from this instance.
      unmount the previous partition -> umount /var/www/html/images/.
	
 create  a new volume and attach that volume to ec2 instance.


 fdisk -l -> which will show all the disks.

 3.volume we have created for this one partition is not present we need to create it.

             first step is to create the partitioning.

 4.fdisk /dev/xvdf(hard disk path) -> it will open the hard disk utility.
   click m -> for help
 5. n -> to create the partition.and then click enter upto last step.
 6. p -> to print the disk partition.
 7. w -> to write the partitions(partitions are created).

            next step is to formatting it.

8.mkfs and click the tab button 2 times -> it will show all the available utilities.
9.choose any utility you want and mkfs.ext4 /dev/xvdf1(hard disk path) click enter.
10.the partition is formatted with ext4 format.
11.mkdir -p /var/lib/mysql -> this is where mysql database is present.

           now mount it.

16.open vim /etc/fstab -> add /dev/xvdf1	/var/lib/mysql	ext4	defaults	0 0
17. mount -a -> this will mount all the entries from fstab file.

18 now lets install mysql -> yum install mariadb-server -y (since it is a centos).
19.start the service -> systemctl start mariadb
20. ls /var/lib/mysql/ -> here you can see the downloaded files.

Note:
      if you want to restore the existing partitioning then snapshot will not do that instead
      of that snapshot will create a volume for it and store the data.


     Snapshots backups and restores:

     if you loose the data and already taken the snapshot then,

                                     1.unmount partition.
                                     2.deattach volume
                                     3.create a new volume from snapshot
                                     4.attach the created volume from snapshot
                                     5.mount it back.

21.create a snapshot from volume(in volume section).
22.go to /var/lib/mysql directory delete the mysql data from it  -> rm -rf *
23.stop the service -> systemctl stop mariadb.
24.unmount the partition -> umount /var/lib/mysql/
25.deattch the volume(in volume section).
26.go to snapshot section and select the snapshot and click on create volume.
27.now attach the recovered volume to ec2 instance.
28.mount -a -> this will mount all the entries.
29.now check the data whether it came or not -> ls /var/lib/mysql/.




ELB-Elastic Load Balancer:
                          Elastic load balancing distributes incoming applications or network
                          traffic across multiple targets such as ec2 instances or containers etc.

      Elastic load balancer supports 3 types of load balancers:
                     
                                  1.Application load balancer - which supports only web traffic
                                  2.network load balancer - which is very high performing load balancer and expensive too.
                                  3.classic load balancer - which is simplest one.


     Main flow of elastic load balancer is:
                                            1.create a ec2 instance with website.
                                            2.create AMI for that instance.
                                            3.create launch template and launch it.
                                            4.create target groups.
                                            5.create load balancer.



CloudWatch:
            it monitors the performance of aws environment-standard infrastructure metrics.


            Metrics:
                    Aws cloud watch allows you to record metrics for services such as EBS,EC2
                    Amazon RDS,ColudFront etc.

            Events: 
                  Aws events delivers a near real time stream of systems that describe change
                  in amazon web services resources.

            Logs:
                you can use amazon cloudwatch logs to monitor,store and acess your log files
                from amazon ec2 instance(Elastic Compute Cloud) and other resources.

Note:
     1.alarms monitors cloudwatch metrics for instances.

      SNS(Simple Notification Services) is a web service that co-ordinate and manage the
      sending of messages to subscribing endpoints.

 flow:
       ec2 instance ----> amazon cloudwatch------> Alarms------------>SNS(email notifications).



EFS(Amazon Elastic File System):
                                 It is a shared file system for data storage.

    1.It is similar to EBS(Elastic Block Store) the only difference is, in EBS we can store data
      for single ec2 instance only.
    2.But in EFS(Elastic File System) it shared file system i.e common storage for multiple
      ec2 instances.

      flow:
           1.create the ec2 instance
           2.in EFS,create the file sytem
           3.create the acess points to acess that file system
           4.we need to mount this in /etc/fstab file.



Auto Scaling:
              Auto scaling is a service that automatically monitors and adjust compute
              resource to maintain the performance for applications hosted in the aws.


       flow:
            we need to create the auto scaling group which will provide in launch configuration
            template to launch instances based on the load (cpu utilization) alarm will be triggered
            if it cross the threshold and scaling policy will trigger the launch of new instances
            in the auto scaling group or even reduce the instances based on the scaling policies.



Amazon S3(Simple Storage Service):
                                   Amazon S3 is a storage for the internet.you can use
                                   S3 to store and retrieve any amount of data at any time
                                   from anywhere on the web. i.e it is just like google drive.


         S3 Storage Classes:
                             1.S3 Standard: 
                                            General purpose storage of frequently accessed data.
                                            Fast acess and object replication in multi available zones.

                             2.S3 Infrequent Acess:
                                                    Long lived,but less frequently accessed data.
                                                     Slow access,object replication in multi available zones.

                            3.S3 One Zone-Infrequent Access:
                                                             It is for data that is accessed less frequently,
                                                             but requires rapid access when needed.slow access,
                                                             no object replication.

                           4.S3 Inteligent Tiering:
                                                    Automatically moves data to most cost effective tier.

                           5.S3 Glacier:
                                        Low cost storage for data archiving.

                           6. S3 Glacier Deep Archive:
                                                       Lowest cost storage,retrival time of 12hrs.

Note:
      1.By default everything in S3 Bucket is private.
      2.Bucket versioning: 
                           1.If you disable bucket versioning, if you delete anything in the bucket, you can not retrive it.
                           2.If you enable the bucket versioning,you delete anything in bucket we can recover it but the size 
                             of the bucket will grow(size).if you keep on recovering the objects, the size will increases.

      3.while uploading the objects to S3 bucket,you can choose the storage classes.



If you want to host static website in S3 Bucket:
                                                  1.download the static website from tooplate.com
                                                  2.upload the files to S3 Bucket.
                                                  3.In permission,enable the public acess and make the files as public by selecting the files.
                                                  4.enable the static website hosting in properties option.
                                                  5.use the endpoint to display the content in browser(endpoint present static website hosting in properties).

   
    4.By mistakenly you override the object then follow these steps:
                                                                      
        select the file you want to recover and go to versioning there you can find the previous version of your object 
        and download it and then upload the file.make it public then it will display in browser.This is how you can recover
        the objects in S3 bucket.Just make sure to enable bucket versioning.


    5.If you delete the objects in S3 Bucket you want to recover it follow these steps:

       In the S3 Bucket,you will see the list of objects option,you just enable it.there you can see
       the all versions of yours object.The delete files also shown here.select the delete file and click on the delete option
       then it will come to S3 Bucket.


Flow of project setup Vprofile in aws:

                                       1.Login into aws account.
                                       2.Create key pairs.
                                       3.Create security groups.
                                       4.Launch ec2 instances with user data(bash scripts).
                                       5.Update IP to name mapping in route 53.
                                       6.Bulid application from source code.
                                       7.Upload to s3 bucket(refer point 1 to point 3 in note).
                                       8.Download the artifact to tomcat ec2 instances(refer point 4 onwords for this).
                                       9.Setup Elastic Load Balancer(ELB) with https(certificate from amazon certificate manager).
                                       10.Map Elastic Load Balancer endpoint to website name in Godaddy DNS.
                                       11.Verify the entire setup.
                                       12.Bulid auto scaling group for tomcat instances.(1.AMI
                                                                                         2.launch template 
                                                                                         3.autoscaling group).

Note:
      To upload the artifact into amazon S3 bucket follow these steps:
   
       1.first we need to configure the IAM role in AWS CLI(commmand -> aws configure)
       2.create the s3 bucket using command -> aws s3 mb s3://hkh-code-artis(Bucket name).
       3.we copy the artifact to S3 bucket -> aws s3 cp target/vprofile-v2.war s3://hkh-code-artis/

       4.create the role for the s3 bucket and attach that role to the tomcat instance.
       5. aws s3 ls -> it will show all the s3 buckets.
       6.copy the artifact to temp folder -> aws s3://hkh-code-artis/vprofile-v2.war /tmp/
       7.delete the default tomcat page -> rm -rf /var/lib/tomcat9/webapps/ROOT
       8.copy the artifact from temp folder to tomcat default page -> cp /tmp/vprofile-v2.war /var/lib/tomcat9/webapps/ROOT.war




Refactoring the above project:
                          

                                 Comparision


 1.BeanStalk                                    1.Tomacat EC2 instance on vm
 2.ELB in beanstalk                             2.nginx LB/ELB
 3.autoscaling                                  3.autoscaling
 4.EFS/S3                                       4.EFS/S3
 5.RDS                                          5.mysql on ec2 instances.
 6.Elastic cache                                6.memcache on ec2 instance.
 7.Active MQ                                    7.Rabbit MQ on ec2 instance.
 8.Route 53                                     8.Godaddy,local DNS
 9.Cloud Front                                  9.Multi delivery content across world.



Flow of execution:  

                                                           monitors by amazon cloud watch
                                                                               ^
                                                                               |
user ---> Route 53 ---> cloud front ---> application load balancer ---->  ec2 instances in bean stalk ----> stores artifact in s3 bucket.
                                                                               |
                                                                               |
                                    RDS(mysql)<-----Elastic Cache<----- Active MQ
                                                                         

1.Login into aws account.
2.create the key pair for beanstalk instances login.
3.create the security group for Elastic cache,Active MQ and RDS.
4.create
         RDS,Amazon Elastic Cache,Amazon MQ.

5.create Elastic Beanstalk environment.
6.update security group for backend to allow traffic from beanstalk security group.
7.update security group for backend to allow internal traffic.

8.launch EC2 instances for DB initializing.
9.login into the instance and initialize the RDS DB.
10.change the health check on beanstalk to /login.
11.add 443 https listener to ELB.

12.build artifact from backend information.
13.deploy artifact to beanstalk.
14.create CDN(Content Delivery Network) with ssl certificate.	
15.update entry in godaddy DNS Zones.
16.test the url.


Amazon CloudFront:
                   Amazon CloudFront is a content delivery network operated by Amazon Web Services.
                   The content delivery network was created to provide a globally-distributed network of proxy servers
                   to cache content, such as web videos or other bulky media, more locally to consumers,
                   to improve access speed for downloading the content.

Note:
      1.Generally whenver the user request some thing on the browser it will route to S3 since the data is present in S3.
      2.When you use cloudfront,for the first time whenever the user request for data from s3, the data is cached to 
        nearest edge location.so again whenever you request the same data then the data will come from the edge location 
        instead of S3.So that we can able to acess the data very fast.







GIT:
     Git is distributed version control system.
     
    Two types of version control system
                                         1.central control system:
                                                                   In these,everybody needs to make changes from the centrol repository that means one repository.
                                                                   due to some error occured and the entire data will be lost here.
                                        
                                         2.Distributed control system:
                                                                       here,everyone has the one local copy of the repository.whenever the some issue occured still
                                                                       data will not be lost.

Note:
      1.git log -> This command is used to show the logs(previous executed commites).
      2.git log --oneline -> This will display the one line commit id.
      3.git show commit-id -> It will show the content of that file.
      4.git branch -a -> This will show all the branches in the current repositories.
      5.git branch -c sprint1 -> it will create the branch called sprint1 in the current repositories.
      6.git rm filename -> To remove the file.
      7.git mv existing_filename updated_filename -> to modify the filename.
      8.git checkout branchname -> it will switch from one branch to another branch.
               or
        git switch branch
      9.git clone https_link -> it will clone the repository to our local system.
      10.git checkout filename -> it will rollback the changes how acutually it was, when the file is not staged.
      11.git diff(it will work will when the file is not staged i.e that means not added) -> it will show the acutual file content and what's been modified.
      12.git diff --cached -> it will work only when the file is staged.
      13.git restore --staged filename -> if you want to rollback the changes when the file is staged.
      14.git revert HEAD -> if you want to rollback the changes when the file is commited (for revert, the history will be saved i.e commit message will be shown).
      15.git reset --hard commit_id -> it will same as above.Only difference is it will not store the history.

  
      Pushing the code from local repository to remote repository(github):

                                                                            1.git init
                                                                            2.git add filename(to add single file)
                                                                                 or
                                                                              git add .(to add all the files).
                                                                            3.git commit -m "message for the commit"
                                                                            4.git add origin https_repository url
                                                                            5.git push origin branch name(i.e main or master)







Maven:
      maven is a build tool for java.

Maven life cycle:
              
                  1.validate:
                              validate the project is correct and all necessary information is correct.

                  2.compile: 
                             compile the source code of the project.

                  3.test:
                          test the compiled source code using suitable unit testing framework.
                          These tests should not require the code to be packaged(i.e jar or war).

                  4.package:
                             take the compiled source code and packaged(i.e jar or war).

                  5.verify:
                            run any checks on results of integration tests to ensure quality criteria are met.

                  6.install:
                             install the packages into the local repository,for use as a dependency in other project locally.

                  7.deploy:
                            done in the build environment,copies the final package to the remote repository for sharing with other developers and projects.









Jenkins:
           

flow of continuous integration pipeline:


devloper ---> github----->jenkins(fetch code from github)------->Build(maven)------>unit test(maven)
                                                                                          |
                                                                                          |
                                upload artifact(nexus sonatype)<--------  code analysis(sonar qube)



Steps to setup CI:

                    1.jenkins setup
                    2.nexus setup
                    3.sonarqube setup
                    4.security group
                    5.plugins(nexus artifact uploader,sonarqube scanner,build timestamp(for versioning artifact),pipeline maven integration,pipeline utility step).
                    6.integrate
                                 i)nexus
                                 ii)sonarqube

                    7.write pipeline script(if anything goes wrong it will not bulid it)
                    8.set notifications.



Declarative Pipeline:
               

pipeline {
    agent any
    tools {
        maven "Maven3"
        jdk "OracleJDK11"
    }
    stages {
        stage('Fetch Code') {
            steps {
                git branch: 'main', url: 'https://github.com/devopshydclub/vprofile-repo.git'

            }
        }
        stage('Build') {
            steps {
                sh 'mvn install -DskipTests'
            }
            post {
                success {
                    echo "Now Archiving it"
                    archiveArtifacts artifacts: '**/target/*.war'
                }
            }
        }
        stage('UNIT TEST') {
            steps {
                sh 'mvn test'

            }
        }
    }
}


if you want to send the junit,jacoco,checkstyle reports to sonarqube using sonarscanner,



pipeline {
    agent any
    tools {
        maven 'Maven3'
        jdk 'OracleJDK11'
    }
    stages {
        stage('Fetch Code') {
            steps {
                git branch: 'main', url: 'https://github.com/devopshydclub/vprofile-repo.git'
            }

        }
        stage('Build') {
            steps {
                sh 'mvn install -DskipTests'
                
            }
            post {
                success {
                    echo "Now Archiving it"
                    archiveArtifacts artifacts: '**/targets/*.war'

                }
            }
        }
        stage('UNIT TEST') {
            steps {
                sh 'mvn test'
            }
        }

        stage('Checkstyle Analysis') {
            steps {
                sh 'mvn checkstyle:checkstyle'
            }
        }
        stage('Sonar Analysis') {
            environment {
                scannerHome = tool 'sonar4.7'
            }
            steps {
                withSonarQubeEnv('sonar') {
                    sh '''${scannerHome}/bin/sonar-scanner  -Dsonar.projectKey=vprofile \
                    -Dsonar.projectName=vprofile \
                    -Dsonar.projectVersion=1.0 \
                    -Dsonar.sources=src/ \
                    -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
                    -Dsonar.junit.reportsPath=target/surefire-reports/ \
                    -Dsonar.jacoco.reportsPath=target/jacoco.exe \
                    -Dsonar.java.checkstyle.reportsPath=target/checkstyle-result.xml'''
                }

            }
        }
    }
}



quality gate: based some metrics(bugs) we can bulid or failure the build.

to create this in sonarqube:
                             1.create the quality gate based on the bugs(mention total number of bugs to build failure).
                             2.then go to project and attch the quality gate(project settings) to project.
                             3.create the webhook, in that mention url of jenkins server.

pipeline {
    agent any
    tools {
        maven 'Maven3'
        jdk 'OracleJDK11'
    }
    stages {
        stage('Fetch Code') {
            steps {
                git branch: 'main', url: 'https://github.com/devopshydclub/vprofile-repo.git'
            }

        }
        stage('Build') {
            steps {
                sh 'mvn install -DskipTests'
                
            }
            post {
                success {
                    echo "Now Archiving it"
                    archiveArtifacts artifacts: '**/targets/*.war'

                }
            }
        }
        stage('UNIT TEST') {
            steps {
                sh 'mvn test'
            }
        }

        stage('Checkstyle Analysis') {
            steps {
                sh 'mvn checkstyle:checkstyle'
            }
        }
        stage('Sonar Analysis') {
            environment {
                scannerHome = tool 'sonar4.7'
            }
            steps {
                withSonarQubeEnv('sonar') {
                    sh '''${scannerHome}/bin/sonar-scanner  -Dsonar.projectKey=vprofile \
                    -Dsonar.projectName=vprofile \
                    -Dsonar.projectVersion=1.0 \
                    -Dsonar.sources=src/ \
                    -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
                    -Dsonar.junit.reportsPath=target/surefire-reports/ \
                    -Dsonar.jacoco.reportsPath=target/jacoco.exe \
                    -Dsonar.java.checkstyle.reportsPath=target/checkstyle-result.xml'''
                }

            }
        }
        stage('Quality Gates') {
            steps {
                timeout(time: 1, unit: 'HOURS') {
                    waitForQualityGate abortPipeline: true
                }
            }
        }
    }
}


To store the artifact in nexus repository:
                                           1.create the repository in nexus(choose maven hosted).
                                           2.add the nexus credentials in jenkins.(manage jenkins --> manage credentials --> click on jenkins ---> global credentials --> add credentials)

Note:
      1.To attach the timestamp for the artifact, we need to first enable build timestamp(in that mention pattern)
        ->path is manage jenkins ----> build timestamp.

pipeline {
    agent any
    tools {
        maven 'Maven3'
        jdk 'OracleJDK11'
    }
    stages {
        stage('Fetch Code') {
            steps {
                git branch: 'main', url: 'https://github.com/devopshydclub/vprofile-repo.git'
            }

        }
        stage('Build') {
            steps {
                sh 'mvn install -DskipTests'
                
            }
            post {
                success {
                    echo "Now Archiving it"
                    archiveArtifacts artifacts: '**/targets/*.war'

                }
            }
        }
        stage('UNIT TEST') {
            steps {
                sh 'mvn test'
            }
        }

        stage('Checkstyle Analysis') {
            steps {
                sh 'mvn checkstyle:checkstyle'
            }
        }
        stage('Sonar Analysis') {
            environment {
                scannerHome = tool 'sonar4.7'
            }
            steps {
                withSonarQubeEnv('sonar') {
                    sh '''${scannerHome}/bin/sonar-scanner  -Dsonar.projectKey=vprofile \
                    -Dsonar.projectName=vprofile \
                    -Dsonar.projectVersion=1.0 \
                    -Dsonar.sources=src/ \
                    -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
                    -Dsonar.junit.reportsPath=target/surefire-reports/ \
                    -Dsonar.jacoco.reportsPath=target/jacoco.exe \
                    -Dsonar.java.checkstyle.reportsPath=target/checkstyle-result.xml'''
                }

            }
        }
        stage('Quality Gates') {
            steps {
                timeout(time: 1, unit: 'HOURS') {
                    waitForQualityGate abortPipeline: true
                }
            }
        }

        stage('Upload Artifact') {
            steps {
                nexusArtifactUploader {
                    nexusVersion: 'nexus3',
                    protocol: 'http',
                    nexusUrl: '172.31.18.21:8081',
                    groupId: 'QA',
                    version: "${env.BUILD_ID}-${env.BUILD_TIMESTAMP}",
                    repository: 'vprofile-repo',
                    credentialsId: 'nexuslogin',
                    artifacts: [
                        [artifactId: 'vproapp',
                        classifier: '',
                        file: 'target/vprofile-v2.war',
                        type: 'war']
                    ]

                }
            }
        }
    }
}


last is send notification whether the build is passed or failure:

For notifications, we are using tge slack notification:
                                                         1.create the slack account and then login in.
                                                         2.create the workspace and then create the channel(i.e group in watsup).
                                                         3.integrate our channel with jenkins (for that search for add apps to slack in browser,
                                                                                               search for jenkins and then click on add to slack,
                                                                                               we need to choose the channel and copy the token).
                                                         4.install slack notification in jenkins.
                                                         5.configure the slack.
                                                           (going through manage jenkins --> slack notifications-->secrete text -> paste the token and give id(name) and description)
                                                         


def COLOR_MAP = [
    'SUCCESS': 'good',
    'FAILURE': 'danger',
]
pipeline {
    agent any
    tools {
        maven 'Maven3'
        jdk 'OracleJDK11'
    }
    stages {
        stage('Fetch Code') {
            steps {
                git branch: 'main', url: 'https://github.com/devopshydclub/vprofile-repo.git'
            }

        }
        stage('Build') {
            steps {
                sh 'mvn install -DskipTests'
                
            }
            post {
                success {
                    echo "Now Archiving it"
                    archiveArtifacts artifacts: '**/targets/*.war'

                }
            }
        }
        stage('UNIT TEST') {
            steps {
                sh 'mvn test'
            }
        }

        stage('Checkstyle Analysis') {
            steps {
                sh 'mvn checkstyle:checkstyle'
            }
        }
        stage('Sonar Analysis') {
            environment {
                scannerHome = tool 'sonar4.7'
            }
            steps {
                withSonarQubeEnv('sonar') {
                    sh '''${scannerHome}/bin/sonar-scanner  -Dsonar.projectKey=vprofile \
                    -Dsonar.projectName=vprofile \
                    -Dsonar.projectVersion=1.0 \
                    -Dsonar.sources=src/ \
                    -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
                    -Dsonar.junit.reportsPath=target/surefire-reports/ \
                    -Dsonar.jacoco.reportsPath=target/jacoco.exe \
                    -Dsonar.java.checkstyle.reportsPath=target/checkstyle-result.xml'''
                }

            }
        }
        stage('Quality Gates') {
            steps {
                timeout(time: 1, unit: 'HOURS') {
                    waitForQualityGate abortPipeline: true
                }
            }
        }

        stage('Upload Artifact') {
            steps {
                nexusArtifactUploader {
                    nexusVersion: 'nexus3',
                    protocol: 'http',
                    nexusUrl: '172.31.18.21:8081',
                    groupId: 'QA',
                    version: "${env.BUILD_ID}-${env.BUILD_TIMESTAMP}",
                    repository: 'vprofile-repo',
                    credentialsId: 'nexuslogin',
                    artifacts: [
                        [artifactId: 'vproapp',
                        classifier: '',
                        file: 'target/vprofile-v2.war',
                        type: 'war']
                    ]

                }
            }
        }
    }

    post {
        always {
            echo 'slack notification.'
            slackSend channel: '#jenkinscicd'.
            color: COLOR_MAP[currentBuild.currentResult],
            message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"

        }
    }
}
                                           

















      


   







         


                                                        




   




	



                                










                    





 
 
          
      
        

 



 



