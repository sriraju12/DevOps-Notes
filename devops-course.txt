Disadvantages of agile:
                      1.Cost is more when you want to change any thing at later stages.
                      2.No product is released or visible till the end of the process.

Main process of deployment the code in production is: 
                                                      Devlopers devlop the code and push into
the github.after that code is moved into the bulid server there the code is bulid,tested and
evaluated then it produces software or artifact.This software or artifact is moved into 
repository based on the programmming language like war or jar files.After that,this artifact
is moved into the further testing.If everything is okay then it moved into the production.

problem:
       devlopers devlops some piece of code continuosly for 3 weeks and then pushes the code
into the github and then it is moved into the bulid server,there if any bulid issues occurs.
It is very difficult for the devlopers to resolve many issues at that time and the bugs are 
detect at final stage rather than earlier stage.

solution:
         Whenever the devlopers devlops the code,they will push the code into the github and
and the code is pushed into the bulid server continuously if there are any issues occured at
bulid server,devlopers will notify the issue and at the earlier stage the problem will be sovled.
This process is called continuous integration(CI). 

CI ==> code -> fetch --> bulid --> test --> notify --> feedback -->code(cyclic way) 

tools --> 1.code --> eclipse,vscode,pycharm etc
          2.version control system --> Git
          3.bulid tools --> maven,ant,gradle etc based on programming language
          4.repositories(to store artifacts) -->sonatype nexus,jfrog artifactory,archiva
                                                cloudsmith package,grunt
          5.CI tool --> jenkins

The main goal of CI is to detect the bugs at earlier stage.

Continuous Delivery(CD): 
                        Continuous delivery automates the entire software release process.
Every revision that is committed triggers an automated flow that builds, tests, and 
then stages the update. The final decision to deploy to a live production environment
is triggered by the developer.(after bulid to production everything is automated but the
live production need approval from developer).

The main difference between Continuous Delivery and Continuous Deployment:

         Continuous Delivery after build release,everything is automated expect final stage
i.e to move the software into live production(this is manual process.It need approval from
the developer)

        Continuous Deployment after bulid release,everything is automated till the last stage
i.e move the software into the live production(It does not need approval from the developer).


virtualization:
                It is used to create virtual representation of storage,server and physical
machines(os) that run on one physical os.



virtualization manual setup:                                virtualization automated setup:

1.Oracle Virtual VM (tool)                                    1.Oracle Virtual VM (tool)

2.ISO file (installation file                                 2.vagrant(automatically creates
 which we attached to our vm)                                   vm's for us with vagrant file)     
                                                                   

3.Login tool (git bash and Putty)                             3.commands(vagrant up)


manual vm problems:
                   1.OS Installation
                   2.Time Consuming
                   3.since it is manual process there is a chance of human error.

Vagrants commands :
                     1.vagrant up -> To bring up the vm
                     2.vagrant halt -> To power off the vm
                     3.vagrant destroy -> To delete the vm
                     4.vagrant status -> To see whether vm is running or not
                     5.vagrant global-status -> To see all the vms status 
                     6.vagrant ssh -> To login into linux
                     7.vagrant box list -> To see all the downloaded vagrant box files in the local system.
                     8. vagrant init box-name -> It will create the Vagrant file.
                     9. cat Vagrantfile -> to print the Vagrantfile content.
                     10.vagrant reload -> to reboot the vm
                     11.vagrant global-status --prune -> to remove the old entries of vm's.

Note:
     To move into different file directories
     1.pwd ->present working directory (current folder)
     2.cd /c/vagrantvms -> this will move into the vagrantvms folder
     3. cd .. -> this will come to the previous folder (i.e c folder)
     4. mkdir centos -> this will create a folder called centos

Some important directories in linux:
                                     1.Home Directories -> /root for root user home directory
                                                           /home/username for normal user home directory
                  
                                     2.User Executables -> /bin the commmand that we run
                                                            like ls,pwd are present here (store here)
                                                                    
                                                                     or here 

                                                           /usr/bin,/usr/local/bin.
                                                           These commands are executed by normal user

                                     3.System Executables -> /sbin,/usr/sbin,/usr/local/sbin
                                                             These commands are executed by root user
                                                              like installing software,adding user etc.

                                     4.Other MountPoints -> /media,/mnt
                                                           If you connect any external devices
                                                           like pen drive,usb it will mounted here.

                                     5.Configuration  -> /etc 
                                                         Configuration data is store here like
                                                         network configurations,server configurations etc.

                                     6.Temporary Files -> /tmp
                                                           To store any temporary data.it will delete
                                                           data whenever its reboot.

                                     7.Kernels and Bootloader -> /boot


                                     8.Server Data -> /var,/srv
                                                      If you are running web server,web site files
                                                      are put here or sql data.

                                     9.System Information -> /proc,/sys


                                     10.Shared Libraries -> /lib,/usr/lib,/usr/local/lib
                                                            popular libraries stored here. 


Linux:

     Basic Commands:
                    1.whoami -> to see what type of user you are(root user or normal user)
                    2.pwd -> to see the present working directory(current path)
                    3.ls -> to see the files in current folder
                    4.cat /etc/os-release -> to see which operating system we are using and its version. 
                    5.cd / -> means root directory(in this directory all the directories(folders) are present)
                    5.cd /root -> root user directory
                    6.cd or cd ~ -> whenever you want to come to home directory.
                    7.touch touchfile1.txt -> touch command is used to create the empty file
                    8.touch devopsfile{1..10}.txt ->it will create the 10 devops file(like devops1,devops2..devops10)
                    9.cp devopsfile1.txt dev/ -> it will copy the devopsfile1.txt file to dev folder(here source is devopsfile1 and destination is dev folder)
                    10.cp -r dev backup/ -> This is used to copy the entire directory or folder to another directory(source is dev and destination is backup)
                    11.mv devopsfile3.txt ops/ -> this is used to move the file or folder to another folder(source is devopsfile3.txt and destination is ops)
                    12.mv ops dev/ ->here we are moving entire ops directory or folder into dev directory(source is ops and destination is dev).
                    13.mv devopsfile2.txt devopsfile22.txt -> this rename the file name from devopsfile2.txt to devopsfile22.txt.
                    14.mv *.txt dev/ -> this command is used to move the all files that ends with .txt will be moved into the dev directory
                    15.rm devopsfile2.txt -> it will remove that file(only files not folder)
                    16.rm -r dev -> this will remove the entire folder.
                    17.rm -rf * -> it will remove the all the files from the current working directory
                    18.ip addr show -> to show the ip address 
                    

Note:
     1.absolute path means complete path of that directory or folder.
     2.relative path means just that directory or folder name 
     3.sudo yum install vim -y -> to install vim editor in linux(centos machines).

vim editor:
           It has three modes: 
                               1.command mode
                               2.insert mode(edit mode we can edit changes in this mode) i.e click i to enter into the edit mode
                               3.extended mode(if you want to do more operations like save or quit from vim editor) i.e : is used to extended mode like :w to save and :q to quit

By default it will in command mode.
if you are in insert mode you want to come back to command mode use esc key.

vim devopsfile1.txt -> this command is used to enter into vim editor and we can make changes in the file.
cat devopsfile1.txt -> this is used to print the content that is present in the file.
:q! -> this is forcefully quitting from the vim editor without saving the changes.
:wq -> this will save the content and quit from vim editor.
cat command is used to print the content.
:se nu -> in vim editor to see the numbering for the lines. 

Note:
      while working with directories or folder(like move,copy, delete, cd, operations):

      1.if you are in same directory then file name is enough.
      2.if you are in different directory and trying to do operations in different directory
        you need to give absolute path(complete path).


vim editor shortcuts:
                      1.gg -> to move from last line to first line
                      2.G -> to move from first line to last line
                      3.yy -> to copy single line
                      4.p -> small p (p) paste the line below and captial p (P) paste the line above.
                      5.4yy -> to copy multiple line(4 lines)
                      6.dd -> to delete the single line
                      7. u -> to undo
                      8.5dd -> to delete 5 lines 
                      9./anything -> to search the content 

To do any operation you should be in command mode.

Types of files in Linux:
                         1.- is for regular files
                         2.d is for directory
                         3.l is for link
                        
1.ls -lt -> it will sort latest one to first according to the time stamp.
2.ls -ltr -> it will sort the latest one to the bottom
3.vim /etc/hostname -> By this we can change the hostname after that we need to run another command -> hostname typehostname
4.file filename -> this command is used to check the type of file


Filters:
        1.grep -> This command is used to search the content in a file
                   i.e grep firewall anaconda-ks.cfg

                  In this firewall is word to be search in anaconda-ks.cfg file.


Note:
     1. In Linux,everything is case-sensitive.

     2.If you want to search anything in a file without case-sensitive use -i in above
       i.e grep -i firewall anaconda-ks.cfg.here i indicates without case-sensitive. 

     3.If you want to search anything in the current directory use -> grep -i firewall *
       it will search only files not directories (folder).

     4.If you want to search anything in the current directory irrespect of files and folders
       use -> grep -iR firewall *.this -R indicate that search in both files and directories. 

     5.If you dont want to search for that particular word (show content except this word)
       use -> grep -vi firewall *.this -v indicates show content except this word.


2.less commmand -> This is used to print the content just like cat command but 
                    only difference is it looks like vim editor.we can search content etc.
                    use -> less anaconda-ks.cfg

3.more command -> This is just like less command only difference is, in less command you can move
                  the content using up and down arrow and in more you should use enter button.
                  Content percentage also it will show. use -> more anaconda-ks.cfg

4.head command -> if you want to see first 10 lines of the file use -> head anaconda-ks.cfg.
                                          or
                  if you want to use first 20 lines use -> head -20 anaconda-ks.cfg

5.tail command -> if you want to see last 10 lines use -> tail anaconda-ks.cfg
                                      or
                  if you want to use last 20 lines use -> tail -20 anaconda-ks.cfg

6.cut command -> if you want to filter username or anything based on delimiter
                 use -> cut -d: -f1 /etc/passwd.here f1 is column one and : is delimiter.

7.if you want any thing to search and replace the word:
                                                         2 ways

                 1.using vm editor (in extended mode) -> %s/coronavirus/covid19
                   this will replace one time in single line. 
 
                   if cornovirus word present in more than one time in the same line then use -> %s/coronavirus/covid19/g 
                
                2.sed command -> sed -i 's/coronavirus/covid/g' samplefile.txt


I/O Redirections:
                  i/o redirection means input/output is redirect to some files rather than showing on the screen.

                  1. > -> this is the symbol for output redirection.
                  2. < -> this is the symbol for input redirection. 
                  3. If you don't want to override the content i.e just append(add) use -> >>
                  4. if you want to redirect the error messages to a file(you don't want to display error messages on screen) -> rcho hi 2> errorfile 
                  
echo -> this command is used to print content what ever we give( echo "hello" -> it will print hello).


   standard input(1)   
           ------> |---------------------- ---------->standard output(1)
                   |       command       |
                   |---------------------- ---------->standard error(2)

* command > output.txt 2> error.txt -> if you execute the command and we want to redirect to the file and if the command throws a error message then
                                       we want to redirect to a file.

* command < input.txt > output.txt -> here we are taking the input from the input.txt file and redirecting the output to a output.txt file.
  cat < input.txt > output.txt

* command < input.txt > output.txt 2> errorfile.txt ->  here we are taking the input from the input.txt file and redirecting the output to a 
                                                        output.txt file and if any error will occur then it will redirect to a errorfile.txt file.

Piping:
       1.You can use the wc command to count a file's word, line, character, or byte count.
       2. wc -l /etc/passwd -> wc -l is used to count no.of lines in a file  or folders in directory.
       3. ls | wc -l -> here ls gives output and pipe takes that output as input to the wc -l command and print
          the total no.of lines
       4. head anaconda-ks.cfg | grep by -> here head anaconda-ks.cfg will print first 10 lines as output
          the pipe takes that output as input and grep is used for searching it will search for by and 
          it will print the output.

find command -> This command is used to locate or search for the files i.e find /etc -name host*
                here /etc is the path and -name is search by name and host* is that starts with.



Users and Groups:
                  users is of three types: 
                                           1.root
                                           2.regular
                                           3.service(no login)

         
root:x:0:0:root:/root:/bin/bash -> here root is username,x is shadow encrypted password,0 is userid,0 is groupid
                                   root is comment,/root is home directory,/bin/bash is login shell.

1.adduser username -> This command is used to add the user.
2.groupadd groupname -> This command is used to add the group.
3.passwd ansible(here ansible is the username) -> This command is used to reset or set the password for user.
4.su - ansible(username) -> This command is used to switch from one user to another user.
5.userdel aws(username) -> This command is used to delete the user but still home directoy is present,
                           if you want to delete user with home directory use -> userdel -r aws.
6.groupdel devops(groupname) -> This command is used to delete the group..
                           

If you want to add users to the group they are two ways:
                                                         1.usermod -aG devops ansible
                        (here -aG is the supplementory group , devops is groupname and ansible user).

                                                         2.vim editor(just edit the file and add name in the group).


File Permissions:
                 In this first character is file type,next three is for user,next three is for group,last three is
                 for others.

                 -rw-------. 1 root root 2026 Feb 16 07:21 anaconda-ks.cfg
                 lrwxrwxrwx. 1 root root   37 Feb 16 07:35 cmds -> /opt/dev/ops/devops/test/commands.txt
                 -rw-r--r--. 1 root root   94 Feb 18 05:55 example.txt
                 -rw-------. 1 root root 1388 Feb  5 15:50 original-ks.cfg
                 drwxr-xr-x. 2 root root    6 Feb 18 05:53 redir


here d -> file type
rwx -> user                        r -> read, w -> write, x -> execute
r-x -> group
r-x -> other

1.To change the ownership and group of the file use -> chown ansible(username):devops(group) redir(filename)

   if you want to change only ownership of a file -> chown ansible(username) redir(filename)

2.To change the read,write and execute permissions for others use -> chmod o-x redir(filename)
  (here o is others amd redir is filename)

                drw-r--r-x. 2 jenkins devops 25 Feb 18 08:20 redir

3.To change the read,write and execute permissions for group use -> chmod g+r redir(filename)
  (here it will add read permissions)

                                    or


Change Permissions using numeric methods:
                                         4(for read r)
                                         2(for write w)
                                         1(for execute x)


drw-r--r-x. 2 jenkins devops 25 Feb 18 08:20 redir

to change permissions,for user and group full acess and others no acess use -> chmod 770 redir(filename).
(here 1st digit for user, 2nd digit for group and last digit for others).



Services:
         services that are running in our machine and know their status.

         1.systemctl status httpd -> To see the status whether it is active or not.  i.e here httpd is the service
         2.systemctl start httpd -> To start the service
         3.systemctl stop httpd -> To stop the service                                           
         4.systemctl restart httpd ->To restart the service
         5.systemctl reload httpd ->To reload the changes without even restarting it.
         6.systemctl enable httpd -> modifies the service configuration to tell systemd that the service needs to start up automatically on boot.

provising -> provising in vagrant means executing script or commands when vm is up.
provising executes only one time.

Steps to deploy website manually:
                                 1.first install the dependencies.
                                 2.start the server(using systemctl)
                                 3.manage the configurations if needed
                                 4.deploy the website(download the website and unzip it and restart the server)

Note:
     tooplate.com -> ready-made static websites are available.

Steps to deploy website automatically:
                                       1.In vagrantfile add the provising(scripts or commands)then everything get ready.









Networking: 
            OSI(Open Systems Interconnection) Model : 
                       physical layer(0's & 1's) -> data link layer(frames) -> network layer(packets) -> transport layer -> session layer ->

                       presentation layer -> application layer. 


Whenever the Physical layer receives the signal it converts into bits (0's & 1's) and it sends to Data Link layer.
The data link layer make sure that the signal is error free from node to node over Physical layer, in this layer
data is in frames.Then the data will pass to Network layer,the data is in packets and then data will pass to
Transport layer,in this data will check end to end connections.session layer is to establish,manage and terminate
session .presentation layer is to encrypt and decrypt the data.application layer is to allow acess to network resource
 



OSI Model             DOD Model            Protocols             Devices/Apps


5,6,7 layers         Application           dns,https,ssh etc      web server, mail server etc
                     layer                 


layer 4              host to host          tcp/udp                gateway


layer 3              internet              ip                     router,firewall layer 3 switch


layer 2              network acess         MAC Address            bridge layer 2 switch


layer 1              network acess         eternet                hub



Note:

     1.ipv4 range -> 0.0.0.0 to 255.255.255.255
     2.ip is of two types -> public ip is internet
                             private ip is local network

private ip is classfied into 3 classes and their ranges are
                                                            1.class A => 10.0.0.0 to 10.255.255.255
                                                            2.class B => 172.16.0.0 to 172.31.255.255
                                                            3.class C => 192.168.0.0 to 192.168.255.255


Protocols & port numbers:

 
Label on column        Service name                      UDP and TCP port numbers

1.DNS                  Domain name service-UDP           UDP 53

2.DNS-TCP              Domain Name Service-TCP           TCP 53

3.HTTP                 Web                               TCP 80

4.HTTPS                Secure Web(SSL)                   TCP 443

5.SMTP                 Simple Mail Transfer Protocol     TCP 25

6.POP                  Post Office Protocol              TCP 109,110

7.SNMP                 Simple Network Management         TCP 161,162 UDP 161,162

8.TELNET               Telnet Terminal                   TCP 23

9.FTP                  File Transfer Protocol            TCP 20,21

10.SSH                 Secure Shell(terminal)            TCP 22

11.AFP IP              Apple File Protocol/IP            TCP 44,548



Network Commands:
                  1.ifconfig -> it will show all active network interfaces with names
                                    or
                    ip addr show
                
                  2. ping ip(type total ip address) -> to check whether this ip is connected or not
                  
                  3.netstat -antp -> to see the open ports.



Containers:
            1.Containers offer Isolation and not virtualization.
            2.For virtualization we can use Virtual Machine.
            3.For understanding we can think containers as OS virtualization.

Note:
      Virtualization enables you to run multiple operating systems on the hardware of a single physical server,
      while containerization enables you to deploy multiple applications using the same operating system on a single virtual machine or server.

Docker:
       1.It is a container runtime environment for developing,shipping and running the applications.
       2.Docker registry is place where Docker Container Images are stored.
       3.Docker containers are created from Docker Images

Note:
      1.docker run hello-world -> here docker run means create a container.
      2.docker images -> this will show you available images on your computer.
      3.docker ps -> this will show you running containers.
      4.docker ps -a -> this will show you all the containers(both running and stopped containers).
      5.docker run --name web01 -d -p 9080:80 nginx -> here web01 is the container name
                                                            -d means run in background
                                                            -p means port
                                                             9080 means hostport and 80 means container port.
        here instead -p(small p),if you give -P(captial P) then portnumbers automatically created.

      6.docker inspect web01(container name) -> it will show the ip address.
      7.docker stop web01(container name) -> it will stop the container.
      8. docker rm web01(container names) -> this will delete the containers.
      9. docker rmi image-id -> it will remove the docker images.
      10.if you want to run multiple containers together their is a concept called docker-compose.
      11.docker compose up -d -> this will start the docker all containers
      12.docker compose down -> to stop all the containers
      13.docker system prune -a -> to remove all the stopped containers.







Bash Scripting: 
                In bash script file start with -> #!/bin/bash.

variables:
          These are temporary storing of data in the memory.

          1.Skill="Devops":
                        if you want to acess the values use -> $ (echo $Skill it will print Devops).
        
          2.PACKAGE= "wget unzip httpd":
                                         we can use like -> sudo yum install $PACKAGE -y.
                                         it will download the dependencies.

command line arguments:  
                        means passing the data or value at the time of executing the file.

For example,if you have created a file name with scripts.sh and inside it has the following data

                    #!/bin/bash
                    echo "the value of 0 is :"
                    echo $0

                    echo "the value of 1 is :"
                    echo $1

                    echo "the value of 2 is :"
                    echo $2

when you execute this file it will print, value of 0 is scripts.sh(file name)
                                         value of 1 is empty line
                                         value of 2 is empty line.

if you give argument while executing like ./scripts.sh aws jenkins

output is : 
            value of 0 is ./scripts.sh(file name)
            value of 1 is aws
            value of 2 is jenkins.

Note:
     1.$0 is always the name of the bash script
     2.$1 to $9 are the command line arguments
     3.echo $USER -> this will print the username
     4.echo $HOSTNAME -> it will print the hostname
     5.echo $RANDOM -> it will print the random number.
     6.echo $? -> this is the exit status of last command(0 means sucessful and other than 0 means unsucessfull).



command substitution:
                      Command substitution is method of storing OUTPUT of a command into variable.
                       
                       Two ways to do command substitution:
                                                           1.``
                                                           2.$()
                     1. If you want to store any output to a variable use ->` `
                        UPTIME_1=`uptime`
                        echo $UPTIME_1 ->this will gives the uptime status

                    2.CURRENT_USER=$(who)
                      echo $CURRENT_USER ->This will print the username.


Exporting Variables:
                     variables are temporary stored,if you exit from the shell the data will be gone.

If you want to store the variables permanently then,
  
                         Two ways:
                                   1.export variable are available only for that user then edit .bashrc file(this is available in home directory ls -a)
                                   2.export variable available globally(all users) then edit /etc/profile file.


User Input:
           If you want to take input from the user while executing script use -> read name(any variable).
       
    In user-script.sh file,
                          
                            #!/bin/bash
                      
                            read skill
                            echo "your skill is : $skill"

                            read -p user:usr          -> here -p means prompt
                            read -sp password:pwd     -> here -s means suppress(hide)

                            echo "welcome $usr".


Conditional statements:(if and if-else)
                      
                        #!/bin/bash

                       read -p "Enter the number:" NUM
                       echo

                       if [ $NUM -gt 100 ]   => here gt means greater than
                       then
                           echo "you have entered if block"
                           sleep 3
                           echo "number is greater than 100"
                       else
                           echo "number is less than 100"

                       fi  => here fi means end of condition.


                     (if-elseif)

                     #!/bin/bash

                     value=$(ip addr show | grep -v LOOPBACK | grep -ic mtu)

                     if [ $value -eq 1 ]
                     then
                         echo "one active network interface"

                     elif [ $value -gt 1 ]
                     then
                         echo "found more than one network interface"

                     else
                         echo "no active network interface"

                     fi


Looping:
         for loop:
                   #!/bin/bash
                   for VAR1 in java devops react linux   => here VAR1 is variable name
                   do
                     echo "loop starting"
                     sleep1
                     date
                     echo
                   done     => here done means end of for loop.



                  #!/bin/bash
                  MYUSER="alpha beta gamma"
                  for usr in $MYUSER
                  do
                    echo "adding user $usr"
                    adduser $usr
                    id $usr
                  done


While loop:
           It will run the loop as long as condition is true.
        
        
             #!/bin/bash

            counter=0
            while [ $counter -lt 5 ]
            do
              echo "loop starting"
              echo "the value of counter is $counter"
              counter=$(( counter + 1 ))
              sleep 1
           done

          echo "out of loop"



        Infinite loop:
  
                       #!/bin/bash

                       counter=3
                       while true
                       do
                         echo"starting loop"
                         echo "the value of counter is $counter"
                         counter=$(( counter * 3 ))
                         sleep 1
                       done

                       echo "out of loop"



Remote command execution:   This means from one vm we can connect to other vm's.

                          1.first change the hostname of all the vm's(not mandatory,its for easy identification of hostnames)
                          2.In one vm add the ip address and hostname (i.e 192.168.10.13 web01) in /etc/hosts
                          3.check whether the ip is connected or not( ping web01(hostname).
                          4. ssh vagrant@web01 then it will ask for password(password is username)
                          5.add the user using adduser devops and passwd devops
                          6. add permission to user in visudo file.(devops ALL=(ALL)  NOPASSWD: ALL)
                          7.ubuntu by default doesnot allow to login through password.we need login into web03(ubuntu)
                            and change the permissions i.e enable password login (/etc/ssh/sshd_config ) and
                            then restart ssh(systemctl restart ssh).
                          8. again login into one vm , do ssh vagrant@web03 now it will ask for password 
                             login and add user devops,password devops and edit permission in visudo file
                             (export EDITOR=vim and then visudo)
                          9.now try ssh devops@web01 uptime -> it will ask for password and then displays uptime.
                            without even login into web01 vm we are executing the commands.


SSH Key Exchange:
                
                  It is another way to login remotely without password(it is key based login)

                  1.ssh-keygen -> it will generate the key-pair(/root/.ssh/id_rsa it will store here).
                  2.ssh-copy-id devops@web01 -> it will ask for the password and it will copy the lock to this vm.
                  3.ssh devops@web01 uptime -> now it will not ask for password to login, it will directly execute
                                               the command

Note:
 
         for host in `cat remhosts` 
         do
            echo devops@$host
            date 
         done

         here remhosts -> a file which contains hostnames(web01,web02,web03) and it will print the date



         #!/bin/bash

#PACKAGE="wget unzip httpd"
TEMPDIR="/tmp/webfiles"
URL=" https://www.tooplate.com/zip-templates/2108_dashboard.zip"
ART_NAME="2108_dashboard"
#SVC="httpd"

yum --help &> /dev/null

if [ $? -eq 0 ]
then
    PACKAGE="wget unzip httpd"
    SVC="httpd"

    echo "running on centos"


    sudo yum install $PACKAGE -y > /dev/null

    sudo systemctl start $SVC
    sudo systemctl enable $SVC

    mkdir -p $TEMPDIR
    cd $TEMPDIR
    wget $URL > /dev/null
    unzip $ART_NAME.zip
    sudo cp -r $ART_NAME/* /var/www/html/

    systemctl restart $SVC

    rm -rf $TEMPDIR

    systemctl status $SVC

else
    PACKAGE="wget unzip apache2"
    SVC="apache2"

    echo "running on ubuntu"

    apt update
    sudo apt install $PACKAGE -y > /dev/null

    sudo systemctl start $SVC
    sudo systemctl enable $SVC

    mkdir -p $TEMPDIR
    cd $TEMPDIR
    wget $URL > /dev/null
    unzip $ART_NAME.zip
    sudo cp -r $ART_NAME/* /var/www/html/

    systemctl restart $SVC

    rm -rf $TEMPDIR

    systemctl status $SVC

fi 



               #!/bin/bash

               USR="devops"

               for host in `cat remhosts`
               do
                  echo "connecting to $host"
                  echo "pushing the script to $host"
                  scp mulos_setup.sh $USR@$host:/tmp/    => here scp is used to push the file into others vm's.
                  echo "executing script on $host"
                  ssh $USR@$host /tmp/mulos_setup.sh
                  ssh $USR@$host rm /tmp/mulos_setup.sh
                  echo
              done









AWS:

1.EC2:
       Elastic Compute Cloud provides web services api for provisioning,managing and deprovisioning virtual server
       inside amazon clouds.

Ec2 pricing:

             1.On-Demand(pay per hour or even seconds)
             2.Reserved(Reserve capacity for 2-3 years for discounts)
             3.Spot(Bid your price for unused ec2 capacity)
             4.Dedicated Hosts(physical servers dedicated for you)



Components inside ec2 instance:
                               
                                1.AMI(Amazon Machine Image):
                                                            AMI provides the information requried to launch an instance
                                                            which is a virtual server in cloud(this like vagrant box list)

                                2.Instance Type:
                                                when you launch an instance,the instance type that you specify determines
                                                the hardware of the host computer used for your instance.
                                                (how much size and memory required)

                                3.EBS(Amazon Elastic Block Store):
                                                                   Amazon ec2 provides you with flexible,cost effective and
                                                                   easy to use data storage options for your instances.

                                                                   EBS is like virtual hard drive in which you can store 
                                                                   your OS and your data.

                               4.Tags:
                                      Tag is simple label consisting of customer-defined keys and optional values that can
                                      make it easier to manage,search for,and filter resources.

                               5.Security Groups:
                                                  A Security Group acts as a firewall that controlls the traffic for one
                                                  or more instances.

                               6.Amazon EC2 uses the public-pair cryptography to encrypt and decrypt the login information.


Note:
     1.In ec2,whenever the ec2 instance stopped,the public ip will be gone and private ip
       will be there.
     2.when you start the ec2 instance again,the new public ip will generate and private ip
       remain same.

     3.if you want static ip(fixed ip) then you can use elastic ip's.In right menu search for
        elastic ip and select the region and click on create
     4.it will create the elastic ip's and we can associate that ip to our ec2 instance.


AWS CLI:
         if you want to connect aws through command line use follwing steps:
         
         1.install aws cli in your machine.
         2.create user or existing user also fine, in user setting search for acess keys
           and create acess keys 
         3.open gitbash and type aws configure, it will ask for acesss key,secret acess key
           output format is json and region. then configuration is done


  aws sts get-caller-identity -> this command shows the account ID and account number you are
                                 using.



EBS(Amazon Elastic Block Store):
                                 Amazon ec2 provides you with flexible,cost effective and
                                 easy to use data storage options for your instances.

                                 EBS is like virtual hard drive in which you can store 
                                 your OS and your data in the form of volumns.
                                
                                 snapshot is backup of a volumn.


          EBS Types:
                     1.General purpose(SSD-Solid State Drive) - most work loads

                     2.Provisioned IOPS - Large Databases.

                     3.Throughput Optimized HD - big data and data warehouse.

                     4.Cold HDD(Hard Disk Drive) - file servers.

                     5.Magnetic - backups and Archieves.


If you want to store anything separately then follow the below steps:
 
 1.first you need to create a volume(choose the size based on your requirment).
 2.Attach that volume to the instance(just make sure that volume and instance is in same zone).
 
 fdisk -l -> which will show all the disks.

 3.volume we have created for this one,partition is not present we need to create it.

             first step is to create the partitioning.

 4.fdisk /dev/xvdf(hard disk path) -> it will open the hard disk utility.
   click m -> for help
 5. n -> to create the partition.and then click enter upto last step.
 6. p -> to print the disk partition.
 7. w -> to write the partitions(partitions are created).

            next step is to formatting it.

8.mkfs and click the tab button 2 times -> it will show all the available utilities.
9.choose any utility you want and mkfs.ext4 /dev/xvdf1(hard disk path) click enter.
10.the partition is formatted with ext4 format.

           next step is to mount it.

11.create a directory  in temp directory and moves all images file to created directory.
   here mkdir /tmp/img-backups
        mv /var/www/html/images/* /tmp/img-backups/
12.now the images directory is empty.
13.This is a temporary mount -> mount /dev/xvdf1(hard disk path) /var/www/html/images/(where you want to mount i.e path).
14. run df -h command to see the mounted.(how much is used and how much is available)
15.if you want unmount it(delete it) -> umount /var/www/html/images/(path)

now lets see the permanent mount

16.open vim /etc/fstab -> add /dev/xvdf1	/var/www/html/images/	ext4	defaults	0 0
17. mount -a -> this will mount all the entries from fstab file.
18. next is to move the images from temp directory to mounted directory.
    mv /tmp/img-backups/* /var/www/html/images/
19.restart the service -> systemctl restart httpd
20.check the status of the service if mounting is failed then the service will not run -> systemctl status httpd.
21.check in the browser whether it shows images or not.



EBS Snapshots:
               Snapshots are usually to backup's and restores the data.

 
              we will use the previous instance for the backup

      for that first, change the name of the instance and deattach the volume from this instance.
      unmount the previous partition -> umount /var/www/html/images/.
	
 create  a new volume and attach that volume to ec2 instance.


 fdisk -l -> which will show all the disks.

 3.volume we have created for this one partition is not present we need to create it.

             first step is to create the partitioning.

 4.fdisk /dev/xvdf(hard disk path) -> it will open the hard disk utility.
   click m -> for help
 5. n -> to create the partition.and then click enter upto last step.
 6. p -> to print the disk partition.
 7. w -> to write the partitions(partitions are created).

            next step is to formatting it.

8.mkfs and click the tab button 2 times -> it will show all the available utilities.
9.choose any utility you want and mkfs.ext4 /dev/xvdf1(hard disk path) click enter.
10.the partition is formatted with ext4 format.
11.mkdir -p /var/lib/mysql -> this is where mysql database is present.

           now mount it.

16.open vim /etc/fstab -> add /dev/xvdf1	/var/lib/mysql	ext4	defaults	0 0
17. mount -a -> this will mount all the entries from fstab file.

18 now lets install mysql -> yum install mariadb-server -y (since it is a centos).
19.start the service -> systemctl start mariadb
20. ls /var/lib/mysql/ -> here you can see the downloaded files.

Note:
      if you want to restore the existing partitioning then snapshot will not do that instead
      of that snapshot will create a volume for it and store the data.


     Snapshots backups and restores:

     if you loose the data and already taken the snapshot then,

                                     1.unmount partition.
                                     2.deattach volume
                                     3.create a new volume from snapshot
                                     4.attach the created volume from snapshot
                                     5.mount it back.

21.create a snapshot from volume(in volume section).
22.go to /var/lib/mysql directory delete the mysql data from it  -> rm -rf *
23.stop the service -> systemctl stop mariadb.
24.unmount the partition -> unmount /var/lib/mysql/
25.deattch the volume(in volume section).
26.go to snapshot section and select the snapshot and click on create volume.
27.now attach the recovered volume to ec2 instance.
28.mount -a -> this will mount all the entries.
29.now check the data whether it came or not -> ls /var/lib/mysql/.




ELB-Elastic Load Balancer:
                          Elastic load balancing distributes incoming applications or network
                          traffic across multiple targets such as ec2 instances or containers etc.

      Elastic load balancer supports 3 types of load balancers:
                     
                                  1.Application load balancer - which supports only web traffic
                                  2.network load balancer - which is very high performing load balancer and expensive too.
                                  3.classic load balancer - which is simplest one.


     Main flow of elastic load balancer is:
                                            1.create a ec2 instance with website.
                                            2.create AMI for that instance.
                                            3.create launch template and launch it.
                                            4.create target groups.
                                            5.create load balancer.



CloudWatch:
            it monitors the performance of aws environment-standard infrastructure metrics.


            Metrics:
                    Aws cloud watch allows you to record metrics for services such as EBS,EC2
                    Amazon RDS,ColudFront etc.

            Events: 
                  Aws events delivers a near real time stream of systems that describe change
                  in amazon web services resources.

            Logs:
                you can use amazon cloudwatch logs to monitor,store and acess your log files
                from amazon ec2 instance(Elastic Compute Cloud) and other resources.

Note:
     1.alarms monitors cloudwatch metrics for instances.

      SNS(Simple Notification Services) is a web service that co-ordinate and manage the
      sending of messages to subscribing endpoints.

 flow:
       ec2 instance ----> amazon cloudwatch------> Alarms------------>SNS(email notifications).



EFS(Amazon Elastic File System):
                                 It is a shared file system for data storage.

    1.It is similar to EBS(Elastic Block Store) the only difference is, in EBS we can store data
      for single ec2 instance only.
    2.But in EFS(Elastic File System) it shared file system i.e common storage for multiple
      ec2 instances.

      flow:
           1.create the ec2 instance
           2.in EFS,create the file sytem
           3.create the acess points to acess that file system
           4.we need to mount this in /etc/fstab file.



Auto Scaling:
              Auto scaling is a service that automatically monitors and adjust compute
              resource to maintain the performance for applications hosted in the aws.


       flow:
            we need to create the auto scaling group which will provide in launch configuration
            template to launch instances based on the load (cpu utilization) alarm will be triggered
            if it cross the threshold and scaling policy will trigger the launch of new instances
            in the auto scaling group or even reduce the instances based on the scaling policies.



Amazon S3(Simple Storage Service):
                                   Amazon S3 is a storage for the internet.you can use
                                   S3 to store and retrieve any amount of data at any time
                                   from anywhere on the web. i.e it is just like google drive.


         S3 Storage Classes:
                             1.S3 Standard: 
                                            General purpose storage of frequently accessed data.
                                            Fast acess and object replication in multi available zones.

                             2.S3 Infrequent Acess:
                                                    Long lived,but less frequently accessed data.
                                                     Slow access,object replication in multi available zones.

                            3.S3 One Zone-Infrequent Access:
                                                             It is for data that is accessed less frequently,
                                                             but requires rapid access when needed.slow access,
                                                             no object replication.

                           4.S3 Inteligent Tiering:
                                                    Automatically moves data to most cost effective tier.

                           5.S3 Glacier:
                                        Low cost storage for data archiving.

                           6. S3 Glacier Deep Archive:
                                                       Lowest cost storage,retrival time of 12hrs.

Note:
      1.By default everything in S3 Bucket is private.
      2.Bucket versioning: 
                           1.If you disable bucket versioning, if you delete anything in the bucket, you can not retrive it.
                           2.If you enable the bucket versioning,you delete anything in bucket we can recover it but the size 
                             of the bucket will grow(size).if you keep on recovering the objects, the size will increases.

      3.while uploading the objects to S3 bucket,you can choose the storage classes.



If you want to host static website in S3 Bucket:
                                                  1.download the static website from tooplate.com
                                                  2.upload the files to S3 Bucket.
                                                  3.In permission,enable the public acess and make the files as public by selecting the files.
                                                  4.enable the static website hosting in properties option.
                                                  5.use the endpoint to display the content in browser(endpoint present static website hosting in properties).

   
    4.By mistakenly you override the object then follow these steps:
                                                                      
        select the file you want to recover and go to versioning there you can find the previous version of your object 
        and download it and then upload the file.make it public then it will display in browser.This is how you can recover
        the objects in S3 bucket.Just make sure to enable bucket versioning.


    5.If you delete the objects in S3 Bucket you want to recover it follow these steps:

       In the S3 Bucket,you will see the list of objects option,you just enable it.there you can see
       the all versions of yours object.The delete files also shown here.select the delete file and click on the delete option
       then it will come to S3 Bucket.


Flow of project setup Vprofile in aws:

                                       1.Login into aws account.
                                       2.Create key pairs.
                                       3.Create security groups.
                                       4.Launch ec2 instances with user data(bash scripts).
                                       5.Update IP to name mapping in route 53.
                                       6.Bulid application from source code.
                                       7.Upload to s3 bucket(refer point 1 to point 3 in note).
                                       8.Download the artifact to tomcat ec2 instances(refer point 4 onwords for this).
                                       9.Setup Elastic Load Balancer(ELB) with https(certificate from amazon certificate manager).
                                       10.Map Elastic Load Balancer endpoint to website name in Godaddy DNS.
                                       11.Verify the entire setup.
                                       12.Bulid auto scaling group for tomcat instances.(1.AMI
                                                                                         2.launch template 
                                                                                         3.autoscaling group).

Note:
      To upload the artifact into amazon S3 bucket follow these steps:
   
       1.first we need to configure the IAM role in AWS CLI(commmand -> aws configure)
       2.create the s3 bucket using command -> aws s3 mb s3://hkh-code-artis(Bucket name).
       3.we copy the artifact to S3 bucket -> aws s3 cp target/vprofile-v2.war s3://hkh-code-artis/

       4.create the role for the s3 bucket and attach that role to the tomcat instance.
       5. aws s3 ls -> it will show all the s3 buckets.
       6.copy the artifact to temp folder -> aws s3://hkh-code-artis/vprofile-v2.war /tmp/
       7.delete the default tomcat page -> rm -rf /var/lib/tomcat9/webapps/ROOT
       8.copy the artifact from temp folder to tomcat default page -> cp /tmp/vprofile-v2.war /var/lib/tomcat9/webapps/ROOT.war




Refactoring the above project:
                          

                                 Comparision


 1.BeanStalk                                    1.Tomacat EC2 instance on vm
 2.ELB in beanstalk                             2.nginx LB/ELB
 3.autoscaling                                  3.autoscaling
 4.EFS/S3                                       4.EFS/S3
 5.RDS                                          5.mysql on ec2 instances.
 6.Elastic cache                                6.memcache on ec2 instance.
 7.Active MQ                                    7.Rabbit MQ on ec2 instance.
 8.Route 53                                     8.Godaddy,local DNS
 9.Cloud Front                                  9.Multi delivery content across world.



Flow of execution:  

                                                           monitors by amazon cloud watch
                                                                               ^
                                                                               |
user ---> Route 53 ---> cloud front ---> application load balancer ---->  ec2 instances in bean stalk ----> stores artifact in s3 bucket.
                                                                               |
                                                                               |
                                    RDS(mysql)<-----Elastic Cache<----- Active MQ
                                                                         

1.Login into aws account.
2.create the key pair for beanstalk instances login.
3.create the security group for Elastic cache,Active MQ and RDS.
4.create
         RDS,Amazon Elastic Cache,Amazon MQ.

5.create Elastic Beanstalk environment.
6.update security group for backend to allow traffic from beanstalk security group.
7.update security group for backend to allow internal traffic.

8.launch EC2 instances for DB initializing.
9.login into the instance and initialize the RDS DB.
10.change the health check on beanstalk to /login.
11.add 443 https listener to ELB.

12.build artifact from backend information.
13.deploy artifact to beanstalk.
14.create CDN(Content Delivery Network) with ssl certificate.	
15.update entry in godaddy DNS Zones.
16.test the url.


Amazon CloudFront:
                   Amazon CloudFront is a content delivery network operated by Amazon Web Services.
                   The content delivery network was created to provide a globally-distributed network of proxy servers
                   to cache content, such as web videos or other bulky media, more locally to consumers,
                   to improve access speed for downloading the content.

Note:
      1.Generally whenver the user request some thing on the browser it will route to S3 since the data is present in S3.
      2.When you use cloudfront,for the first time whenever the user request for data from s3, the data is cached to 
        nearest edge location.so again whenever you request the same data then the data will come from the edge location 
        instead of S3.So that we can able to acess the data very fast.






AWS-part-2:

         VPC(Virtual Private Cloud):
                                     1.vpc is a logical data center within a region.
                                     2.vpc is an on-demand configurable pool of shared computing resources allocated within a public cloud environment.
                                     3.control over network environment,select ip address range,subnets and configure route tables and gateways.

         Subnet Masks:
                       A subnet mask is used to divide an IP address into two parts.
                       One part identifies the host (computer), 
                       the other part identifies the network to which it belongs.
                       To better understand how IP addresses and subnet masks work, look at an IP address and see how it's organized.

                        ex: our ip is 192.168.0.172
                            and subnet mask is 255.255.255.0
                            here 192.168.0.0 is network ip(class c) and last ip is 192.168.0.255 is for broadcast.

                         based on the subnet masks only we can avaiable ips.
                         like start with 192.168.0.1 to 192.168.0.255.here first three octet are fixed because of subnets(255.255.255.0)

         CIDR(Classless Inter-Domain Routing):
                                              Classless Inter-Domain Routing (CIDR) notation is a way to represent an IP address and its network mask.

                                               ex: 192.168.0.172 is network ip                  
                                                   255.255.255.0 is subnet masks

                                          This can be represent as 192.168.0.0/24 which is 255.255.255.0 as subnet

                                                 255.0.0.0
                                                 /8

             this can be represented as          11111111.00000000.00000000.00000000
                                           
                                                 255.255.0.0
                                                 /16

                                                 11111111.11111111.00000000.00000000

                                                 255.255.255.0
                                                 /24
                         
                                                 11111111.11111111.11111111.00000000


         VPC design and components:
                                     vpc is divide into two parts.one is public subnet and another is private subnet.

                                     public subnet is connected to internet gateway and private subnet is connected to NAT gateway present inside public subnet.

         NAT gateway:
                      Network Address Translation(NAT) gateway to enable instances in a private subnet to connect to the internet or other aws resources. 

         Internet Gateway:
                           An internet gateway is horizontally scaled,redudant and 
                           highly available VPC components that allows communication between instances in your vpc and the internet.

        Bastion Host:
                      Bastion host is the single point entry to access your resources in the private subnet.

        VPC Peering:
                     A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses.
                     Instances in either VPC can communicate with each other as if they are within the same network.

        Note:
              1.we can not create a default VPC in aws.
              2.to copy key from one instance to another instance through terminal
                        
                      -> scp -i Downloads/bostion-key.pem Downloads/web-key.pem ubuntu@52.53.153.72:/home/ubuntu/
                         here web-key.pem file is copied into the bostion host instance in /home/ubuntu/ directory.

      flow for creating VPC:
                              1.create VPC
                              2.create subnets(public subnets & private subnets)
                              3.create internet gateway.
                              4.create NAT gateway and before that create Elastic Ip and attach that to NAT gateway.
                              5.create Route Tables and Add the subnet association and Route(public subnet attach to internet gateway)
                                                                                             & private subnet attach to NAT gateway
                              6.Go to subnets and click on action, click on edit subnet settings and Enable auto-assign public IPv4 address.
                                do this for all public subnets.

                              7.Go to your VPC and click on action,click on edit VPC settings and Enable DNS hostnames. 
                             
                              8.create one ec2 instance(Bostion host) and while creating ec2 instance select the created vpc and choose any public subnet in subnet.

                              9.now we will create one ec2 instance and deploy one website in ec2 instance in private subnet and that can be access through bastion host.

                              10.to copy key from  instance(website instance) to another instance(bastion host) through terminal
                        
                                   -> scp -i Downloads/bostion-key.pem Downloads/web-key.pem ubuntu@52.53.153.72:/home/ubuntu/
                                      here web-key.pem file is copied into the bostion host instance in /home/ubuntu/ directory.

                                        chmod 400 web-key.pem (file permission).

                              11.create the elastic load balancer and before that first create the target groups then you can access the website using load balancer DNS name.


EC2 Logs:
         1. common way is achieve the logs and send it to somewhere out of the system and then clean the logs.

steps:
       1.create ec2 instance and deploy website on it.So that logs will generate.
       2.ssh to instance and go to logs path and tar the logs files.
       3.clear the logs by cat /dev/null > acess_log(log filename)
       4.create the s3 bucket and we can move the tar file into s3 bucket.
       5.for that first, create IAM user and create access key and secret key.
       6.ssh to instance and install awscli
       7.configure aws, provide access key and secret key,region and output format as json->aws configure
       8.copy the tar file into s3 bucket -> aws s3 cp fullpath(including filename) s3://bucketname/

                                or

            aws s3 sync sourcepath s3://bucketname/

        NOTE: instead point 5,6 simply you can use -> 1.create a role and select the policy you want,in our case it is AmazonS3fullAccess
                                                      2.attach this role to ec2 instance.click action -> security -> modify IAM role.

 
       2.To stream logs live:


            steps:
                    1.create ec2 instance and deploy website on it.So that logs will generate.
                    2.create a role and select the policy you want,in our case it is Amazoncloudwatchlogfullaccess
                    3.attach this role to ec2 instance.click action -> security -> modify IAM role.
                    4.ssh to instance and install awscli and aws logs(this cloudwatch agent takes logs from our log file that we specify 
                      and stream it to cloudwatch
                    5.edit the file, at the end add the file our log file path(/var/log/httpd/access_log) -> vim /etc/awslogs/awslogs.conf
                    6.restart the awslogs service -> systemctl restart awslogsd and  systemctl enabled awslogsd
                    7.check in the cloudwatch, logs will generate over here.

                     if you want to more specifically send this log file to cloudwatch then
                           1.copy the log path -> /var/log/httpd/access_log
                           2.edit the file, at the end add the file our log file path(/var/log/httpd/access_log) -> vi /var/awslogs/etc/awslogs.conf

                   8.from cloudwatch, you can export logs to s3 bucket.
                    







GIT:
     Git is distributed version control system.
     
    Two types of version control system
                                         1.central control system:
                                                                   In these,everybody needs to make changes from the centrol repository that means one repository.
                                                                   due to some error occured and the entire data will be lost here.
                                        
                                         2.Distributed control system:
                                                                       here,everyone has the one local copy of the repository.whenever the some issue occured still
                                                                       data will not be lost.

Note:
      1.git log -> This command is used to show the logs(previous executed commites).
      2.git log --oneline -> This will display the one line commit id.
      3.git show commit-id -> It will show the content of that file.
      4.git branch -a -> This will show all the branches in the current repositories.
      5.git branch -c sprint1 -> it will create the branch called sprint1 in the current repositories.
      6.git rm filename -> To remove the file.
      7.git mv existing_filename updated_filename -> to modify the filename.
      8.git checkout branchname -> it will switch from one branch to another branch.
               or
        git switch branch
      9.git clone https_link -> it will clone the repository to our local system.
      10.git checkout filename -> it will rollback the changes how acutually it was, when the file is not staged.
      11.git diff(it will work will when the file is not staged i.e that means not added) -> it will show the acutual file content and what's been modified.
      12.git diff --cached -> it will work only when the file is staged.
      13.git restore --staged filename -> if you want to rollback the changes when the file is staged.
      14.git revert HEAD -> if you want to rollback the changes when the file is commited (for revert, the history will be saved i.e commit message will be shown).
      15.git reset --hard commit_id -> it will same as above.Only difference is it will not store the history.

  
      Pushing the code from local repository to remote repository(github):

                                                                            1.git init
                                                                            2.git add filename(to add single file)
                                                                                 or
                                                                              git add .(to add all the files).
                                                                            3.git commit -m "message for the commit"
                                                                            4.git add origin https_repository url
                                                                            5.git push origin branch name(i.e main or master)







Maven:
      maven is a build tool for java.

Maven life cycle:
              
                  1.validate:
                              validate the project is correct and all necessary information is correct.

                  2.compile: 
                             compile the source code of the project.

                  3.test:
                          test the compiled source code using suitable unit testing framework.
                          These tests should not require the code to be packaged(i.e jar or war).

                  4.package:
                             take the compiled source code and packaged(i.e jar or war).

                  5.verify:
                            run any checks on results of integration tests to ensure quality criteria are met.

                  6.install:
                             install the packages into the local repository,for use as a dependency in other project locally.

                  7.deploy:
                            done in the build environment,copies the final package to the remote repository for sharing with other developers and projects.









Jenkins:
           

flow of continuous integration pipeline:


devloper ---> github----->jenkins(fetch code from github)------->Build(maven)------>unit test(maven)
                                                                                          |
                                                                                          |
                                upload artifact(nexus sonatype)<--------  code analysis(sonar qube)



Steps to setup CI:

                    1.jenkins setup
                    2.nexus setup
                    3.sonarqube setup
                    4.security group
                    5.plugins(nexus artifact uploader,sonarqube scanner,build timestamp(for versioning artifact),pipeline maven integration,pipeline utility step).
                    6.integrate
                                 i)nexus
                                 ii)sonarqube

                    7.write pipeline script(if anything goes wrong it will not bulid it)
                    8.set notifications.



Declarative Pipeline:
               

pipeline {
    agent any
    tools {
        maven "Maven3"
        jdk "OracleJDK11"
    }
    stages {
        stage('Fetch Code') {
            steps {
                git branch: 'main', url: 'https://github.com/devopshydclub/vprofile-repo.git'

            }
        }
        stage('Build') {
            steps {
                sh 'mvn install -DskipTests'
            }
            post {
                success {
                    echo "Now Archiving it"
                    archiveArtifacts artifacts: '**/target/*.war'
                }
            }
        }
        stage('UNIT TEST') {
            steps {
                sh 'mvn test'

            }
        }
    }
}


if you want to send the junit,jacoco,checkstyle reports to sonarqube using sonarscanner,



pipeline {
    agent any
    tools {
        maven 'Maven3'
        jdk 'OracleJDK11'
    }
    stages {
        stage('Fetch Code') {
            steps {
                git branch: 'main', url: 'https://github.com/devopshydclub/vprofile-repo.git'
            }

        }
        stage('Build') {
            steps {
                sh 'mvn install -DskipTests'
                
            }
            post {
                success {
                    echo "Now Archiving it"
                    archiveArtifacts artifacts: '**/targets/*.war'

                }
            }
        }
        stage('UNIT TEST') {
            steps {
                sh 'mvn test'
            }
        }

        stage('Checkstyle Analysis') {
            steps {
                sh 'mvn checkstyle:checkstyle'
            }
        }
        stage('Sonar Analysis') {
            environment {
                scannerHome = tool 'sonar4.7'
            }
            steps {
                withSonarQubeEnv('sonar') {
                    sh '''${scannerHome}/bin/sonar-scanner  -Dsonar.projectKey=vprofile \
                    -Dsonar.projectName=vprofile \
                    -Dsonar.projectVersion=1.0 \
                    -Dsonar.sources=src/ \
                    -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
                    -Dsonar.junit.reportsPath=target/surefire-reports/ \
                    -Dsonar.jacoco.reportsPath=target/jacoco.exe \
                    -Dsonar.java.checkstyle.reportsPath=target/checkstyle-result.xml'''
                }

            }
        }
    }
}



quality gate: based some metrics(bugs) we can bulid or failure the build.

to create this in sonarqube:
                             1.create the quality gate based on the bugs(mention total number of bugs to build failure).
                             2.then go to project and attch the quality gate(project settings) to project.
                             3.create the webhook, in that mention url of jenkins server.

pipeline {
    agent any
    tools {
        maven 'Maven3'
        jdk 'OracleJDK11'
    }
    stages {
        stage('Fetch Code') {
            steps {
                git branch: 'main', url: 'https://github.com/devopshydclub/vprofile-repo.git'
            }

        }
        stage('Build') {
            steps {
                sh 'mvn install -DskipTests'
                
            }
            post {
                success {
                    echo "Now Archiving it"
                    archiveArtifacts artifacts: '**/targets/*.war'

                }
            }
        }
        stage('UNIT TEST') {
            steps {
                sh 'mvn test'
            }
        }

        stage('Checkstyle Analysis') {
            steps {
                sh 'mvn checkstyle:checkstyle'
            }
        }
        stage('Sonar Analysis') {
            environment {
                scannerHome = tool 'sonar4.7'
            }
            steps {
                withSonarQubeEnv('sonar') {
                    sh '''${scannerHome}/bin/sonar-scanner  -Dsonar.projectKey=vprofile \
                    -Dsonar.projectName=vprofile \
                    -Dsonar.projectVersion=1.0 \
                    -Dsonar.sources=src/ \
                    -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
                    -Dsonar.junit.reportsPath=target/surefire-reports/ \
                    -Dsonar.jacoco.reportsPath=target/jacoco.exe \
                    -Dsonar.java.checkstyle.reportsPath=target/checkstyle-result.xml'''
                }

            }
        }
        stage('Quality Gates') {
            steps {
                timeout(time: 1, unit: 'HOURS') {
                    waitForQualityGate abortPipeline: true
                }
            }
        }
    }
}


To store the artifact in nexus repository:
                                           1.create the repository in nexus(choose maven hosted).
                                           2.add the nexus credentials in jenkins.(manage jenkins --> manage credentials --> click on jenkins ---> global credentials --> add credentials)

Note:
      1.To attach the timestamp for the artifact, we need to first enable build timestamp(in that mention pattern)
        ->path is manage jenkins ----> build timestamp.

pipeline {
    agent any
    tools {
        maven 'Maven3'
        jdk 'OracleJDK11'
    }
    stages {
        stage('Fetch Code') {
            steps {
                git branch: 'main', url: 'https://github.com/devopshydclub/vprofile-repo.git'
            }

        }
        stage('Build') {
            steps {
                sh 'mvn install -DskipTests'
                
            }
            post {
                success {
                    echo "Now Archiving it"
                    archiveArtifacts artifacts: '**/targets/*.war'

                }
            }
        }
        stage('UNIT TEST') {
            steps {
                sh 'mvn test'
            }
        }

        stage('Checkstyle Analysis') {
            steps {
                sh 'mvn checkstyle:checkstyle'
            }
        }
        stage('Sonar Analysis') {
            environment {
                scannerHome = tool 'sonar4.7'
            }
            steps {
                withSonarQubeEnv('sonar') {
                    sh '''${scannerHome}/bin/sonar-scanner  -Dsonar.projectKey=vprofile \
                    -Dsonar.projectName=vprofile \
                    -Dsonar.projectVersion=1.0 \
                    -Dsonar.sources=src/ \
                    -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
                    -Dsonar.junit.reportsPath=target/surefire-reports/ \
                    -Dsonar.jacoco.reportsPath=target/jacoco.exe \
                    -Dsonar.java.checkstyle.reportsPath=target/checkstyle-result.xml'''
                }

            }
        }
        stage('Quality Gates') {
            steps {
                timeout(time: 1, unit: 'HOURS') {
                    waitForQualityGate abortPipeline: true
                }
            }
        }

        stage('Upload Artifact') {
            steps {
                nexusArtifactUploader {
                    nexusVersion: 'nexus3',
                    protocol: 'http',
                    nexusUrl: '172.31.18.21:8081',
                    groupId: 'QA',
                    version: "${env.BUILD_ID}-${env.BUILD_TIMESTAMP}",
                    repository: 'vprofile-repo',
                    credentialsId: 'nexuslogin',
                    artifacts: [
                        [artifactId: 'vproapp',
                        classifier: '',
                        file: 'target/vprofile-v2.war',
                        type: 'war']
                    ]

                }
            }
        }
    }
}


last is send notification whether the build is passed or failure:

For notifications, we are using tge slack notification:
                                                         1.create the slack account and then login in.
                                                         2.create the workspace and then create the channel(i.e group in watsup).
                                                         3.integrate our channel with jenkins (for that search for add apps to slack in browser,
                                                                                               search for jenkins and then click on add to slack,
                                                                                               we need to choose the channel and copy the token).
                                                         4.install slack notification in jenkins.
                                                         5.configure the slack.
                                                           (going through manage jenkins --> slack notifications-->secrete text -> paste the token and give id(name) and description)
                                                         


def COLOR_MAP = [
    'SUCCESS': 'good',
    'FAILURE': 'danger',
]
pipeline {
    agent any
    tools {
        maven 'Maven3'
        jdk 'OracleJDK11'
    }
    stages {
        stage('Fetch Code') {
            steps {
                git branch: 'main', url: 'https://github.com/devopshydclub/vprofile-repo.git'
            }

        }
        stage('Build') {
            steps {
                sh 'mvn install -DskipTests'
                
            }
            post {
                success {
                    echo "Now Archiving it"
                    archiveArtifacts artifacts: '**/targets/*.war'

                }
            }
        }
        stage('UNIT TEST') {
            steps {
                sh 'mvn test'
            }
        }

        stage('Checkstyle Analysis') {
            steps {
                sh 'mvn checkstyle:checkstyle'
            }
        }
        stage('Sonar Analysis') {
            environment {
                scannerHome = tool 'sonar4.7'
            }
            steps {
                withSonarQubeEnv('sonar') {
                    sh '''${scannerHome}/bin/sonar-scanner  -Dsonar.projectKey=vprofile \
                    -Dsonar.projectName=vprofile \
                    -Dsonar.projectVersion=1.0 \
                    -Dsonar.sources=src/ \
                    -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
                    -Dsonar.junit.reportsPath=target/surefire-reports/ \
                    -Dsonar.jacoco.reportsPath=target/jacoco.exe \
                    -Dsonar.java.checkstyle.reportsPath=target/checkstyle-result.xml'''
                }

            }
        }
        stage('Quality Gates') {
            steps {
                timeout(time: 1, unit: 'HOURS') {
                    waitForQualityGate abortPipeline: true
                }
            }
        }

        stage('Upload Artifact') {
            steps {
                nexusArtifactUploader {
                    nexusVersion: 'nexus3',
                    protocol: 'http',
                    nexusUrl: '172.31.18.21:8081',
                    groupId: 'QA',
                    version: "${env.BUILD_ID}-${env.BUILD_TIMESTAMP}",
                    repository: 'vprofile-repo',
                    credentialsId: 'nexuslogin',
                    artifacts: [
                        [artifactId: 'vproapp',
                        classifier: '',
                        file: 'target/vprofile-v2.war',
                        type: 'war']
                    ]

                }
            }
        }
    }

    post {
        always {
            echo 'slack notification.'
            slackSend channel: '#jenkinscicd'.
            color: COLOR_MAP[currentBuild.currentResult],
            message: "*${currentBuild.currentResult}:* Job ${env.JOB_NAME} build ${env.BUILD_NUMBER} \n More info at: ${env.BUILD_URL}"

        }
    }
}



Docker CI in jenkins:


flow: 


                                                                   sonarqube(upload results
                                                                                     sonarqube)
                                                                          |
                                                                          |
devloper --> github --> jenkins --->unit test -->checkstyle --> code analysis(sonarqube)-->docker build(artifact)
                                                                                                 |
                                                                                                 |
                                                                                                  Amazon ECR(upload artifact to ECR).

steps:
       1.install docker engine in ec2 jenkins.
          
             *Add jenkins user to docker group & reboot(usermod -aG docker jenkins).

       2.install AWS CLI in jenkins ec2.
       3.create IAM user in AWS (attach these two policies amazonec2containerRegistryfullacess,amazonECSfull acess and generate the secrete keys)
       4.create ECR repo in AWS.
       5.plugins
                 *ECR,docker pipeline, aws sdk for credentials and ,cloudBees docker build and publish.
       6.store aws credentials in jenkins((manage jenkins --> manage credentials --> click on jenkins ---> global credentials --> add credentials).
       7.run the pipeline.


pipeline {
    agent any
    tools {
	    maven "MAVEN3"
	    jdk "OracleJDK8"
	}

    environment {
        registryCredential = 'ecr:us-east-2:awscreds'(here us-eat-2 is region code and awscreds is CredentialIDinJenkins)
        appRegistry = "951401132355.dkr.ecr.us-east-2.amazonaws.com/vprofileappimg"(RegistryUrl/Registryname)
        vprofileRegistry = "https://951401132355.dkr.ecr.us-east-2.amazonaws.com"(RegistryUrl)
    }
  stages {
    stage('Fetch code'){
      steps {
        git branch: 'docker', url: 'https://github.com/devopshydclub/vprofile-project.git'
      }
    }


    stage('Test'){
      steps {
        sh 'mvn test'
      }
    }

    stage ('CODE ANALYSIS WITH CHECKSTYLE'){
            steps {
                sh 'mvn checkstyle:checkstyle'
            }
            post {
                success {
                    echo 'Generated Analysis Result'
                }
            }
        }

        stage('build && SonarQube analysis') {
            environment {
             scannerHome = tool 'sonar4.7'
          }
            steps {
                withSonarQubeEnv('sonar') {
                 sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
                   -Dsonar.projectName=vprofile-repo \
                   -Dsonar.projectVersion=1.0 \
                   -Dsonar.sources=src/ \
                   -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
                   -Dsonar.junit.reportsPath=target/surefire-reports/ \
                   -Dsonar.jacoco.reportsPath=target/jacoco.exec \
                   -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
                }
            }
        }

        stage("Quality Gate") {
            steps {
                timeout(time: 1, unit: 'HOURS') {
                    // Parameter indicates whether to set pipeline to UNSTABLE if Quality Gate fails
                    // true = set pipeline to UNSTABLE, false = don't
                    waitForQualityGate abortPipeline: true
                }
            }
        }

    stage('Build App Image') {
       steps {
       
         script {
                dockerImage = docker.build( appRegistry + ":$BUILD_NUMBER", "./Docker-files/app/multistage/")
             }

     }
    
    }

    stage('Upload App Image') {
          steps{
            script {
              docker.withRegistry( vprofileRegistry, registryCredential ) {
                dockerImage.push("$BUILD_NUMBER")
                dockerImage.push('latest')
              }
            }
          }
     }

  }
}








Docker CI/CD in jenkins:


flow: 


                                                                   sonarqube(upload results
                                                                                     sonarqube)
                                                                          |
                                                                          |
devloper --> github --> jenkins --->unit test -->checkstyle --> code analysis(sonarqube)-->docker build(artifact)
                                                                                                 |
                                                                                                 |
                                                                                                  Amazon ECR(upload artifact to ECR).
                                                                                                         |
                                                                                                         |
                                                                                                        Amazon ECS(image that is created in ECR will be deployed here).


steps:
       1.install docker engine in ec2 jenkins.
          
             *Add jenkins user to docker group & reboot(usermod -aG docker jenkins).

       2.install AWS CLI in jenkins ec2.
       3.create IAM user in AWS (attach these two policies amazonec2containerRegistryfullacess,amazonECSfull acess and generate the secrete keys)
       4.create ECR repo in AWS.
       5.plugins
                 *ECR,docker pipeline, aws sdk for credentials and ,cloudBees docker build and publish.
       6.store aws credentials in jenkins((manage jenkins --> manage credentials --> click on jenkins ---> global credentials --> add credentials).
       7.create the ECS cluster and ECS service.
       8.install plugin pipeline:aws steps.
       7.run the pipeline.



pipeline {
    agent any
    tools {
	    maven "MAVEN3"
	    jdk "OracleJDK8"
	}

    environment {
        registryCredential = 'ecr:us-east-2:awscreds'
        appRegistry = "951401132355.dkr.ecr.us-east-2.amazonaws.com/vprofileappimg"
        vprofileRegistry = "https://951401132355.dkr.ecr.us-east-2.amazonaws.com"
        cluster = "vprofile"(cluster name)
        service = "vprofileappsvc"(service name)
    }
  stages {
    stage('Fetch code'){
      steps {
        git branch: 'docker', url: 'https://github.com/devopshydclub/vprofile-project.git'
      }
    }


    stage('Test'){
      steps {
        sh 'mvn test'
      }
    }

    stage ('CODE ANALYSIS WITH CHECKSTYLE'){
            steps {
                sh 'mvn checkstyle:checkstyle'
            }
            post {
                success {
                    echo 'Generated Analysis Result'
                }
            }
        }

        stage('build && SonarQube analysis') {
            environment {
             scannerHome = tool 'sonar4.7'
          }
            steps {
                withSonarQubeEnv('sonar') {
                 sh '''${scannerHome}/bin/sonar-scanner -Dsonar.projectKey=vprofile \
                   -Dsonar.projectName=vprofile-repo \
                   -Dsonar.projectVersion=1.0 \
                   -Dsonar.sources=src/ \
                   -Dsonar.java.binaries=target/test-classes/com/visualpathit/account/controllerTest/ \
                   -Dsonar.junit.reportsPath=target/surefire-reports/ \
                   -Dsonar.jacoco.reportsPath=target/jacoco.exec \
                   -Dsonar.java.checkstyle.reportPaths=target/checkstyle-result.xml'''
                }
            }
        }

        stage("Quality Gate") {
            steps {
                timeout(time: 1, unit: 'HOURS') {
                    // Parameter indicates whether to set pipeline to UNSTABLE if Quality Gate fails
                    // true = set pipeline to UNSTABLE, false = don't
                    waitForQualityGate abortPipeline: true
                }
            }
        }

    stage('Build App Image') {
       steps {
       
         script {
                dockerImage = docker.build( appRegistry + ":$BUILD_NUMBER", "./Docker-files/app/multistage/")
             }

     }
    
    }

    stage('Upload App Image') {
          steps{
            script {
              docker.withRegistry( vprofileRegistry, registryCredential ) {
                dockerImage.push("$BUILD_NUMBER")
                dockerImage.push('latest')
              }
            }
          }
     }
     
     stage('Deploy to ecs') {
          steps {
        withAWS(credentials: 'awscreds', region: 'us-east-2') {
          sh 'aws ecs update-service --cluster ${cluster} --service ${service} --force-new-deployment'
        }
      }
     }

  }
}


Build Trigger:

              Automatically run the build whenever their is a change or commit change in github code.


Steps:
       1.create git repository on github.
       2.ssh auth.(paste the public key in github from ssh keygen.exe)
       3.create a Jenkinsfile in git repo & commit.
       4.go to manage jenkins --> configure global security --> scroll down and you will see git host key verification configuration(in drop down select accept first connection).
       5.create Jenkins job to access Jenkinsfile from git repo.
       6.test triggers.

1.git webhook:
               go to github repository settings and click on webhook and add webhook.
               url is --> jenkins url/github-webhook/ and select the event when to trigger the build.
               while creating the job, in build trigger select github hook trigger for gitscm polling.

2.poll scm:
            this will trigger based on the time we have set.
            while creating the job,in build trigger select the poll scm and give * * * * *(minute hour day month dayoftheweek(0 to 7) ) ) 
            here * means run every minute hour day month and day of the week.

3.scheduled jobs:
                  whenever you want to run the build at particular time.
                  while creating the job,in build trigger select the build periodically and give 30 20 * * 1-5(minute hour day month dayoftheweek(0 to 7) ) ) 
                  here 30 8 * * 1-5 means run at 8:30 pm every day month and from monday to friday.

4.build trigger remotely:
                          you can trigger the build from anywhere.

Generate JOB URL
1. Job Configure => Build Triggers
2. Check mark on “Trigger builds remotely”
3. Give a token name
4. Generate URL & save in a file

Generate Token for User

1. Click your username drop down button (Top right corner of the page)
2. configure => API Token => Generate
3. Copy token name and save username:tokenname in a file

Generate CRUMB

1. wget command is required for this, so download wget binary for git bash
2. Extract content in c:/program files/Git/mingw64/bin
3. Run below command in Git Bash, (replace username,password,Jenkins URL)
wget -q --auth-no-challenge --user username --password password --output-document -
'http://JENNKINS_IP:8080/crumbIssuer/api/xml?xpath=concat(//crumbRequestField,":",//crumb)'
4. Save the token in a file

Build Job from URL

By now we should have below details
1. JENKINS Job URL with token
E:g http://52.15.216.180:8080/job/vprofile-Code-Analysis/build?token=testtoken

2. API Token
USERNAME:API_TOKEN
E:g admin:116ce8f1ae914b477d0c74a68ffcc9777c

3. Crumb
E:g Jenkins-Crumb:8cb80f4f56d6d35c2121a1cf35b7b501

Fill all the above details in below URL and Execute

curl -I -X POST http://username:APItoken @Jenkins_IP:8080/job/JOB_NAME/build?token=TOKENNAME
-H "Jenkins-Crumb:CRUMB"

e:g curl -I -X POST http://admin:110305ffb46e298491ae082236301bde8e@52.15.216.180:8080/job/
vprofile-Code-Analysis/build?token=testtoken -H "Jenkins-Crumb:8cb80f4f56d6d35c2121a1cf35b7b501"



Jenkins master/slave:
                         

pre requisites for nodes setup:
                                 1.Any OS(ec2)
                                 2.network access from master(check firewall rules)
                                 3.java,jdk,jre(install in ec2)
                                 4.create user
                                 5.directory with user ownership.
                                 6.tools are required by the jenkins job(i.e maven,ant etc).





Python scripting:
                   The main difference between bash scripting and python scripting is, in python scripting indentation(i.e spaces)
                   is very important where as in bash scripting it really doesn't care about indentation.


Quotes And Comments:

                      1.# -> this is a single line comment.
                      2.""" multi line comment in double quote""".
                             or
                        ''' multi line comment in single quote'''.
                      3.print("""this is a paragraph string 
                                 you can use in multiple line using double quotes""").
                                             
                                             or

                      4.print('''this is a paragraph string
                                 you can use in multiple lines using single quotes''').


Note:
      In python, single quotes and double quotes both are same.


Variables:
            variable are like temporary storage of values.

            1.x = "abc" -> single variable assignment.

            2.a = b = c = 65 -> multi variable assignment to single value and values of all variables is same.

            3.a, b, c = "alpha", 3, 3.5 -> multi variable assignment to multiple values and values of all variables follows the order.
          
            4.type -> this method is used to print the type of the variable(i.e print("the type of a is",type(q)) ).

            
Data types:

             1.list:
                      first_list = ["alpha", 123, 3.4].
             
             2.tuple:
                      first_tuple = ("alpha", 123, 3.4).

               * list and tuple both are same by visibility.The difference is list enclosed in sqaure brackets and
               tuple is enclosed in rounded brackets.
         
               * The main difference is list is mutable(i.e we can modify its values) and tuple is immutable(i.e we can not modify its values). 

             3.dictionary:
                           first_dictionary = {"Name:"raju", "Weight":85, "Exercises":["Boxing", "Running", "Pushups"]}

               * In dictionary data is store in key-value pair and enclosed in curly brackets.

             4.boolean:
                        x = True and y = False. 


Print Format:
               name = "raju"
               weight = 85

               1. print("name of the person is",name)
                      
                          or

                  print("name of the person is {}".format(name))
               
                     *here {} will be replaced by value of name.

                     *here you have an option to place the curly brackets({}) anywhere you want.

                  print("{} is name of the person".format(name))


               2.if you want to put multiple values then
                         
                     * print("name of the person is {} and his weight is {}".format(name,weight))

                     *here first curly bracket replaced by name and second curly bracket is replaced by weight.

               3.print(f"name of the person is {name} and his weight is {weight}") --> it will work only on python version above 3.6 

               4.concatenation:
                                print("name of the person is"+" "+name)



Slicing:
         slicing means getting a substring from string.

         galaxy = "we are in milkyway galaxy"
         
         1.print(galaxy[0]) -> it will give 'w' as output.
       
         2.print(galaxy[ : ] -> it will print entire string.this is range(start and end-1)
         
         3.print(galaxy[0:2] -> it will give 'we' as output.
 
         4.print(galaxy[-1] -> it will give 'y' as output and this is called negative indexing(starts with -1,-2,-3 etc).


  slicing in list and tuple:
                              slicing in list and tuple both are same.


                               devops = ["Linux, "Vagrant", "Bash scripting", "Aws", "python scripting", "Ansible"]

                               1.print(devops[0]) -> it will print Linux
 
                               2.print(devops[0:3]) -> it will print ['Linux', 'vagrant', 'Bash scripting']

                               3.print(devops[0:3][2][5:11]) -> it will print script


 slicing in dictionary:
                         Skills = {"DevOps": ("Linux", "Vagrant", "Bash scripting"), "Development": ["Java", "NodeJS", "React"]}

                         1.print(Skills[DevOps]) -> it will print ('Linux', 'Vagrant', 'Bash scripting')

                         2.print(Skills[DevOps][2]) -> it will print Bash scripting

                         3.print(Skills[DevOps][2][5:11] -> it will print script

                         4.print(Skills[DevOps][2][-1] -> it will print g



membership operator:

                      Skills = ("DevOps","Linux",23,45.7)

                       1.ans = "DevOps" in Skills
                         print(ans) -> it will print True (because DevOps present in Skills tuple).

                       2.ans = "Aws" in Skills
                         print(ans) -> it will print False.

                       3.ans = 20 not in Skills
                         print(ans) -> it will print True.


identity operator:
                    Identity operator is same as == operator.
                    a = 10 , b = 15
                  
                    1. ans = a is b
                       print(ans) -> it will print False

                    2.ans =  a is not b
                       print(ans) -> it will print True.


Conditions:

             1. if/else:

                            x = 10

                            if x < 10:
                               print("X is less than 10")
                            else:
                               print("X is equal to 10")

              
             2. if/elif/else:

                              x = 20

                              if x > 20:
                                 print("x is greater than 20")
                              elif x == 20:
                                 print("x is equals to 20")
                              else:
                                 print("x is less than 20")



sample program using above concepts:

                                     DevOps = ["Linux", "Vagrant", "Bash scripting", "Aws", "Python"]
                                     Developement = ("Java", ".net", "React", "NodeJS")
                                     contract_emp1 = {"Name":"Raju", "Skill":"Full stack", "Code":1234}
                                     contract_emp2 = {"Name":"sandy", "Skill":"Jenkins", "Code":5678}

                                     user_skill = input("Enter the skills to be searched")

                                     if user_skill in DevOps:
                                        print(f"we have {user_skill} in devops team.")
                                     elif user_skill in Development:
                                        print(f"we have {user_skill} in developement team.")
                                     elif (user_skill in contract_emp1.values()) or (user_skill in contract_emp2.values()):
                                        print(f"we have contract employee with {user_skill} skills in  team.")
                                      else:
                                        print("No such skills found")


Loops:

        1. For Loop:
                      PLANET = "EARTH"
 
                     for i in PLANET:    // here i is variable name
                         print(i)
                     print("outside of for loop) //it will print E 
                                                                 A
                                                                 R
                                                                 T
                                                                 H  

                   
                     SKILLS = ["Linux", "Aws", "Vagrant"]

                     for i in SKILLS:   // here i is variable name
                         print(i) // it prints every word in new line

             *Nested for loop:
                               
                                SKILLS = ["Linux", "Aws", "Vagrant"]

                                for skill in SKILLS:
                                    print("")
                                    print("the skill is")
                                    for i in skill:
                                        print(i)
                                print("outside of for loop") // output is
                                                                            the skill is
                                                                             L
                                                                             i
                                                                             n
                                                                             u
                                                                             x and follws with others values.

                               


         2. While Loop:

                        x = 1

                        while x <= 10:
                             print("the value of x:",x)
                             x+=1
                        print("outside of while loop") // it will print 1 to 10


              import time

                 x = 2

                 while True:
                     print("the value of x is",x)
                     x*=2
                     time.sleep(1)
                 print("outside of while loop) // it wil print 2, 4, 8, 16, etc while delay of 1 sec for infinite times.


Break & Continue:
                   Break:
                         it will break the loop.

                         for i in "DevOps":
                             print(i)
                             if i == 'O':
                             break
                         print("out of loop")


                     import random

                     VACCINE = ["Covacine", "Modona", "Pfizer", "CoronaVac"]
                     random.shuffle(VACCINE)
                     LUCKY = random.choice(VACCINE)

                     for vac in VACCINE:
                         if vac == LUCKY:
                             print(f"{vac} Test was sucessfull")
                             break
                         print(f"{vac} Test was failed")


                   Continue:
                             it will skip the current iteration.

                             for i in "DevOps":
                                 if i == 'O':
                                    continue
                                    print(i)
                             print("out of loop")


                     import random

                     VACCINE = ["Covacine", "Modona", "Pfizer", "CoronaVac"]
                     random.shuffle(VACCINE)
                     LUCKY = random.choice(VACCINE)

                     for vac in VACCINE:
                         if vac == LUCKY:
                             print(f"{vac} Test was sucessfull")
                             continue
                         print(f"{vac} Test was failed")



Built-in functions:
                   message = "welcome to python scripting"

                    1.dir([]) -> it will return all the built-in methods available for that.
                    2.print(message.find("python") -> it will print 11 (because python starting index number is 11)
                    3.in the above example the search word is not present then it will print -1.
 
                    seq1 = ("192","168","90","40")
                    4.print(".".join(seq1)) // 192.168.90.40
                    5.print("-".join(seq1)) //192-168-90-40

 combining two list:
                     mountain = ["everest", "kilimanjaro", "sahara"]
                     numbers = [1,2,3,4,5]
                     mountain.extend(numbers)
                     print(mountain)


Note:
       1.The main difference between append and insert is, in append the value is added at the last.
         whereas in insert we can add the value at specific position.
       2.pop() -> this will delete the value from the list at the end.
       3.pop(3) -> this will delete the value that is present is 3 index number.
    
      4.print(contract_emp1.keys()) -> in dictionaries it will print will keys.
      5.print(contract_emp1.values()) -> in dictionaries it will print will values.
      6.print(contract_emp1.clear()) -> in dictionaries it will delete all entire dictionary and prints empty dictionary.


Functions:
           A function is block of orgnized, reusable code that is used to perform single,related action.

           def add(arg1,arg2):
                total = arg1 + arg2
               return total
           print(add(10,20)) // 30



           def summ(args):
                 x = 0
                for i in args:
                   x = x + i
                return x
           print(summ([1,2,3]) // 6


default argument:
                    def greeting(MSG="morning"):
                        print(f"good {MSG}")
                        print("welcome to functions")

                    greeting() // good morning
                    greeting("evening") // good evening



 def vac_feedback(vac, effiency):
    print(f"{vac} have the {effiency} % effiency")
    if effiency > 50 and effiency <=75:
        print("It is not effective, Need more trials")
    elif effiency > 75 and effiency < 90:
        print("we can consider it")
    elif effiency >= 90:
        print("Sure, we can take it")
    else:
        print("Need many more trials")
        
vac_feedback("covaxin", 70)

Note:
      in functions order of the argument does not matter,we can give any order just mention the argument name follwed by its value.
  
      i.e vac_feedback(effiency=74, vac="Pfizer")


varibale length argument(non key argument(args)):
                                             def order(min_order,*args):
                                                 print(f"you have ordered: {min_order}")
                                                 for item in args:
                                                     print(f"you have ordered: {item}")
                                                 print("it will be delivered in 30 mins")
                                                 print("enjoy the party")
                                             order("salad", "KFC", "biryani")


variable length argument(key argument(kwargs)):
                                         import random
                                         def time_activity(*args,**kwargs):
                                             min = sum(args) + random.randint(0, 60)
                                             choice = random.choice(list(kwargs.keys()))
                                             print(f"you have {min} minutes for {kwargs[choice]} activity")
    
                                          time_activity(10, 20, 30, sports="Cricket", hobby="Watching Tv", work="DevOps")



sample script:
                #!/usr/bin/python3

                import os
                path = "/tmp/testfile.txt"
           
                if os.path.isdir(path):
                    print("it is a directory")
                elif os.path.isfile(path):
                    print("it is a file")
                else:
                    print("file or directory does not exist")


adding user:
               #!/usr/bin/python3

               import os

               userlist = ["raju", "sandy", "kitty"]
               for user in userlist:
                   exitcode = os.system("id {}".format(user))
                   if exitcode != 0:
                       print("user does not found, we are adding the user")
                       os.system("useradd {}".format(user))
                   else:
                         print("user already exists)
  

adding group:

                 #!/usr/bin/python3

               import os
               exitcode = os.system("grep science /etc/group")
               if exitcode != 0:
                  print("group does not exist, we are adding it")
                  os.system("groupadd science")
               else:
                    print("group already exist")


now we are adding user into the group:
                                          #!/usr/bin/python3

                                           import os

                                           userlist = ["raju", "sandy", "kitty"]
                                           for user in userlist:
                                               exitcode = os.system("id {}".format(user))
                                               if exitcode != 0:
                                                   print("user does not found, we are adding the user")
                                                   os.system("useradd {}".format(user))
                                               else:
                                                  print("user already exists)

                                            exitcode = os.system("grep science /etc/group")
                                            if exitcode != 0:
                                                print("group does not exist, we are adding it")
                                                os.system("groupadd science")
                                            else:
                                                 print("group already exist")

                                            for user in userlist:
                                                 print("adding user {} in the science group".format(user))
                                                 os.system("usermod -G science {}".format(user))

now giving ownership and file permissions:

                                            #!/usr/bin/python3

                                           import os

                                           userlist = ["raju", "sandy", "kitty"]
                                           for user in userlist:
                                               exitcode = os.system("id {}".format(user))
                                               if exitcode != 0:
                                                   print("user does not found, we are adding the user")
                                                   os.system("useradd {}".format(user))
                                               else:
                                                  print("user already exists)

                                            exitcode = os.system("grep science /etc/group")
                                            if exitcode != 0:
                                                print("group does not exist, we are adding it")
                                                os.system("groupadd science")
                                            else:
                                                 print("group already exist")

                                            for user in userlist:
                                                 print("adding user {} in the science group".format(user))
                                                 os.system("usermod -G science {}".format(user))

                                             if os.path.isdir("/opt/science dir"):
                                                 print("directory already exist")
                                             else:
                                                   os.mkdir("/opt/science dir")

                                             print("assigning ownership and permissions to group")
                                             os.system("chown :science "/opt/science dir")
                                             os.system("chmod 770 /opt/science dir")






Ansible:
         Ansible is a automation tool for configuration managament.

system admins or administrators:
                                 the main aim of system admins is to do configuration management.

let's see what they will do :
                              * they should maintain all the virtual machines or on-premises servers are upto date.
 
                              * they have to install the packages and libraries and keep upto date.

                              * for suppose, if your hosting java based web application then you need to install the jdk,webserver and application server
                                and have to keep upto date.

                              * they will maintain the server whether they are running and maintain cpu and memory etc. and all these tasks have to be done 
                                in manual steps and which is like hell lot of work.

if the system admins decide to do automation of tasks using scripts then :

 * consider if the company uses 5 virtual machines and the system system decide to automate the tasks using scripts.

 * let's just say 4 virtual machines are linux based and one is windows based machine.

 * if the system admins wants to download package in all the machines then he will use yum to install the packages then in this case the script will fail
   because the 4 virtual machines are linux it will work and 1 is windows based in this case it will fail.

* consider all the virtual machines are linux then also the script will fail because some virtual machines are centos based machines and some virtual 
  machines ubuntu or debian based machines.To solve this problem here comes the puppet, chef and ansible comes in.

* the main disadvantages of puppet and chef are 
                                                 1.learning curve.we have to undestand ruby and learn it and we have to write the complex puppet and chef 
                                                   style code to automate.which is very difficult.

                                                 2.puppet and chef are agent based. for example if you want to work with puppet or chef, first you need to
                                                   install the agent in the targeted machine i.e you need to login to each machine and install the agent.
                                                   which is bad and these problems will be solved by the ansible.

* the main advantage of using ansible is
                                          1.learning is very easy, you just need to understand the YAML

                                          2.it is a agent less i.e you don't need to install any agent in the targeted machines.

                                          3.ansible uses concept called  control node and manage nodes. control node is where you need to install the 
                                            ansible and manage node is the target machines and by using control node you can connect to target machines
                                            and install anything in it.

                                          4.manage nodes can be of anything it is linux or debian or ubuntu or windows.by using ansible we can do  
                                            automation of tasks irrespect of the type of the machines.

* Ansible is a automation tool that is used for configuration management(to reduce the work of system admins) , it is also used for provisioning i.e
  create the reasource like creating ec2 instance, creating s3 bucket etc.it is also used in deployement in ci/cd i.e to deploy the artifact in target 
  machine and it is also used to do network automation.

* shell script vs python script vs ansible :
                                              for example, if you want to install java in all the virtual machines.

   * using shell script:
                         the shell script will fail if the virtual machines are of different types i.e linux , debain, centos or windows.this is the
                         problem with the shell script.

   * using python script:
                          using python script we can solve the above problem because python is independent it will run on any plateform.

                          * disadvantages are :
                                                 * we must konw the python and how to write the script.
                                                 * and we need to update the python script because new updates will come and old versions will not support.
                                                 * to execute this script we need to login to each and every virtual machine.which is lot of work.

  * ansible:
             using ansible, the above problems will be solved. we don't need to login to each and every machine to do the task.

             * learning ansible is very easy.you just need to understand the YAML.by using the control node we can install anything in target machines.

* ansible uses python, we will the YAML file and while executing this YAML file it will convert into python code and excute the task in target machines.
  the only thing is that in all the target machines python should be installed that's it.

               
  
inventory file -> which contains the information about target machines to login(ssh).
                  i.e ip,username and private keys to login.

ansible web01 -m ping -i inventory -> to connect with the target machines.
                                      here web01 is hostname, -m means module, ping means ssh,
                                      inventory means use this information to connect with target machines.

whenever we try connect ssh to the machine,first time it will ask the fingerprint yes or no.
but we dont want that to be interactive.so need to disable that host_key_checking to false.

1.ansible configuration file available in this path -> cat /etc/ansible/ansible.cfg
2.login with root user and cd to /etc/ansible
3.for safer side we need to backup the ansible configuration file -> mv ansible.cfg ansible.cfg_backup
4.run the command => ansible-config init --disabled -t all > ansible.cfg
5.open the ansible.cfg file -> vim ansible.cfg
6.set host_key_checking=False and remove the semicolon and save the file and exit from root user
7.change the permission of clientkey.pem(private key) file -> chmod 400 clientkey.pem


passwordless authentication:
                             passwordless authentication means whenever one vm tries to connect with another vm it will ask for password and connect
                             without using password is passwordless authentication.

                             * passwordless authentication is of two types 1.using password
                                                                           2.using ssh-keys(pem file)

                             * whenever passwordless authentication is setup at first time it will ask for the password and next time onwords it will not
                               ask for password.

        1.using ssh-keys:
                           ssh-copy-id -f "mention full path of pem file" hostname@public_ip and then test it by ssh hostname@public_ip.

        2.using password:
                          go to location in target machine vim /etc/ssh/sshd_config and edit password authentication to yes and then restart the ssh
                          by using systemctl restart ssh.

                          * create the password for the user -> sudo passwd username
   
                          * ssh-copy-id hostname@public_ip and it will ask for the password.

* inventory is the heart of the ansible where we mention the username and ip address of the target machines.

* we can write the inventory file in two ways 1.inventory.ini file
                                              2.using YAML format.

* while executing the ansible provide the path of the inventory file.

* if you don't want to pass the path of the inventory file then place the inventory file in /etc/ansible/hosts. and better way is write inventory file and 
  pass the path of it.

here the inventory file 

all:
  hosts:
    web01:
      ansible_hostname: 172.31.29.73  //private ip
      ansible_user: ec2-user
      ansible_ssh_private_key_file: clientkey.pem
    web02:
      ansible_host: 172.31.22.218
      ansible_user: ec2-user
      ansible_ssh_private_key_file: clientkey.pem
    db01:
      ansible_host: 172.31.19.81
      ansible_user: ec2-user
      ansible_ssh_private_key_file: clientkey.pem

  children:   //This is grouping
    webservers:
      hosts:
        web01:
        web02:
    dbservers:
      hosts:
        db01:
    dc_oregon:
      children:
        webservers:
        dbservers:

now writing the same inventory file with variables,

all:
  hosts:
    web01:
      ansible_hostname: 172.31.29.73
    web02:
      ansible_host: 172.31.22.218
    db01:
      ansible_host: 172.31.19.81
      
  children:
    webservers:
      hosts:
        web01:
        web02:
    dbservers:
      hosts:
        db01:
    dc_oregon:
      children:
        webservers:
        dbservers:
      vars:
        ansible_user: ec2-user
        ansible_ssh_private_key_file: clientkey.pem

Note:
     1. host level variables has high priority and group level variables has low priority.
     2. whenever the variables are not found in the host level then it will search in group level.

     ** configuration management tools maintains state.it compares the previous state to the present state.
        if anything changes then it will return true orelse it will return false. 


YAML:

{"foods": ["Apple", "Orange", "strawberry", "Mango"], "names": ["raju", "sandy", "sri"], "languages": {"perl":"Elite", "python":"Elite", "pascal":"Lame"} }

YAML SYNTAX:
             foods:
               - Apple
               - Orange
               - Strawberry
               - Mango
             names:
               - raju
               - sandy
               - sri
             languages:
               perl: Elite
               python: Elite
               pascal: Lame



Ad Hoc commands:
                 ad hoc commands are great for tasks you repeat rarely. For example, 
                 if you want to power off all the machines in your lab for Christmas vacation,
                 you could execute a quick one-liner in Ansible without writing a playbook.

                 * they are two ways to give instructions to the ansible 1.playbook(YAML)
                                                                         2.Ad Hoc commands

                 * ad hoc commands are used for simple tasks like installing apache server or creating the files on target machines or restarting servers
                   etc. these are just one step activities.

                 * playbooks are used for complex task like install oracle(let's say it has 20 steps) .


ansible web01 -m ansible.builtin.yum -a "name=httpd state=present" -i inventory --become :

        here target machine is web01 and in that machine download httpd service, and -a means argumements
        --become means it will act as sudo and name is package name that you want to install. -m is module

ansible webservers -m ansible.builtin.yum -a "name=httpd state=present" -i inventory --become :

       here target machine is webservers(web01 & web02) and in that machine download httpd service, -m is module
       -a means argumements and --become means it will act as sudo and name is package name that you want to install.



ansible webservers -m ansible.builtin.service -a "name=httpd state=started enabled=yes" -i inventory --become :
    
       here target machine is webservers(web01 & web02) and in that machine,it will start httpd service and enabled httpd service, -m module
       -a means argumements and --become means it will act as sudo and name is package name that you want to install.

        

ansible webservers -m ansible.builtin.copy -a "src=index.html dest=/var/www/html/index.html" -i inventory --become :

      here target machine is webservers(web01 & web02) and in that machine,it will copy the index.html file to /var/www/html/index.html this location ,
      -a means argumements and --become means it will act as sudo and name is package name that you want to install. -m is module



Playbooks and Modules:
                       A playbook runs in order from top to bottom. Within each play, 
                       tasks also run in order from top to bottom. Playbooks with multiple ‘plays’ can orchestrate multi-machine deployments,
                       running one play on your webservers, then another play on your database servers,
                       then a third play on your network infrastructure, and so on.

                       Playbooks are written in YAML format. and it starts with ---

                       playbook is a combination or collection of plays or tasks.

Playbook syntax:

                   
- name: webservers setup
  hosts: webservers
  become: yes
  tasks: 
    - name: Install httpd
      ansible.builtin.yum:
        name: httpd
        state: present  here present means install and absent means uninstall.

    - name: Start service
      ansible.builtin.service:
        name: httpd
        state: started
        enabled: yes

- name: dbserver setup
  hosts: dbservers
  become: yes
  tasks:
    - name: Install mariadb-server
      ansible.builtin.yum:
        name: mariadb-server
        state: present    

to run playbook -> ansible-playbook -i inventory web-db.yaml   // here web-db.yaml is the playbook.

ansible-playbook -i inventory web-db.yaml --syntax-check -> it will check whether the syntax is correct or not in the playbook.

ansible-playbook -i inventory web-db.yaml -C -> it will execute the dry run 

Executing this command will run the playbook normally, but instead of implementing any modifications,
Ansible will simply provide a report on the changes it would have made.
This report encompasses details such as file modifications, command execution, and module calls.

Check mode offers a safe and practical approach to examine the functionality of your playbooks without risking unintended changes to your systems.
Moreover, it is a valuable tool for troubleshooting playbooks that are not functioning as expected.

ansible-playbook -i inventory web-db.yaml -v ->it will helps to troubleshooting playbooks.
                                               we can pass upto 4v's to troubleshooting playbooks indetail.


modules:

        lets see how we can do copy a file to remote location using playbook modules.

        - name: Webservers setup
          hosts: webservers 
          become: yes
          tasks:
            - name: Install httpd
              ansible.builtin.yum:
                name: httpd
                state: present
      
        - name: Start service
          ansible.builtin.service:
            name: httpd
            state: started
            enabled: yes

       - name: copy a index file
         copy:
           src: files/index.html
           dest: /var/www/html/index.html
           backup: yes

now lets see how we can create a database table with accounts and add users to the table using playbook modules.


steps:
 
    1.need to know the exact package name of pymysql.so login into target machine 
      and check the exact package name of pymysql
    3.you need to download the package ansible-galaxy collection install community.mysql  in host machine.
    2.you need to mention the socket where it is located(path) for interaction.

- name: dbservers setup
  hosts: dbservers
  become: yes
  tasks:
    - name: Install mariadb-server
      ansible.builtin.yum:
        name: mariadb-server
        state: present

    - name: Install pymysql
      ansible.builtin.yum:
        name: python3-PyMySQL
        state: present

    - name: start the service
      ansible.builtin.service:
        name: mariadb
        state: started
        enabled: yes

    - name: Create a new database with name 'accounts'
      community.mysql.mysql_db:
        name: accounts
        state: present
        login_unix_socket: /var/lib/mysql/mysql.sock

    - name: Create database user with name 'vprofile'
      community.mysql.mysql_user:
        name: vprofile
        password: 'admin123'
        priv: '*.*:ALL'
        state: present
        login_unix_socket: /var/lib/mysql/mysql.sock


Ansible ROLES:
                 In playbook YAML file we write everything in the one playbook only i.e vars, tasks, handlers and meta data. if the playbook is huge
                 let's say 20 vars , 70 tasks etc. which is not much readable and the person who open this file lost his interest after seeing this
                 many lines of code and maintainance is also difficult.

                 * to solve this problem, ansible roles come into play where we separate the vars in one file, tasks in one file and handlers in one file
                   and metadata in one file. which is more readable and maintainance is also simple.

                 * the other advantage with the ansible role is we can share roles across organization.

                 * the command to create roles is -> ansible-galaxy role init test   i.e here test means name of the role.it will automatically creates
                   the entire folder structure.

                 * if the tasks are simple then go for playbook and if the tasks are complex then go for ansible roles.

                 * let's see what are all the folder that are present inside the roles -> 1.vars -> to store all the variables

                                                                                          2.tasks -> to store all the tasks

                                                                                          3.meta -> to store the metadata like author or about the tasks etc

                                                                                          4.files -> these are the static files that you can copy to the 
                                                                                                     remote location using the 'copy' module. use files when 
                                                                                                     you need to deploy a static files exactly as it is, 
                                                                                                     without any need for customization. 

                                                                                          5.templates -> these are files that use the jinja2 templating
                                                                                                         language to enable dynamic content.use 'templates' 
                                                                                                         when you need to customize the content of the files   
                                                                                                         based on variables or conditions at runtime.

                                                                                          6.defaults -> these are also variables the only thing is it 
                                                                                                        provides the default values to the variable that are 
                                                                                                        present in vars when you not provide the values to 
                                                                                                        the variables that are present in the vars.

                                                                                          7.handlers -> handlers are ansible tasks which you want to execute
                                                                                                        upon particular action.for example if you copy a 
                                                                                                        file to target machine and you want to restart the 
                                                                                                        service then in handler you place the restart 
                                                                                                        service task and in copy task you mention notify and 
                                                                                                        inside notify mention the name of the handler.so 
                                                                                                        whenever this copy task is executed then it will 
                                                                                                        trigger the restart service task that is present in 
                                                                                                        the handler.

Note:
      Ansible is idempotent in nature. let's say, if you take shell script there you create file and execute the script and again execute the script.
      the script will failed because the file is already present. where as in ansible it will not fail, this is called idempotent.




Ansible Galaxy:
                using ansible galaxy we can create the roles.the main advantages of the roles are modularity, readability and we can share the roles
                across different teams in ansible galaxy.

                * ansible galaxy is just like docker hub . we can share our role in ansible galaxy and anyone can access this roles.

                * ansible galaxy reduces the writing the ansible playbook because in ansible galaxy there are some common roles will present and you can
                  go and search then you can modify according to your requirments.

                * login to ansible galaxy using github account. in roles you can find namespaces there you can find your roles that are published by you.


Let's see how we can push our roles to the ansible galaxy
                                                           1.create a new repository in github.
                                                           2.go inside the role and push these files to the github.
                                                           3.go inside role,run ansible-galaxy import github-username repository-name.
                                                           4.then it will throw the authentication error.
                                                           5.go to ansible galaxy and then go to collection and then click api token and click on load and
                                                             it will display token and copy the token.
                                                           6.run ansible-galaxy import github-username repository-name --token paste-the-token.
                                                           



we can change the default ansible configuration settings:
                                              
                                                     order of ansible configuration file:
                                                                                           1.ANSIBLE_CONFIG(environment variable if set)
2.ansible.cfg(in the current directory)
3.~/.ansible.cfg(in the home directory)
4./etc/ansible/ansible.cfg

now we can write our own ansible configuration settings:
                                                         place this file ansible.cfg in your repository or current working directory.

[defaults]
host_key_checking=False
inventory=./inventory   //if you mention inventory file in the configuration settings, you don't need to -i inventory while executing the file.
forks=5
log_path=/var/log/ansible.log

[privilege_escalation]
become=True
become_method=sudo
become_ask_pass=False

while setting up above configuration,we will get warning about the log_path to solve this,

1.first create this path -> sudo touch /var/log/ansible.log
2.change the ownership of the file -> sudo chown ubuntu.ubuntu /var/log/ansible.log 


Decision making:

here we are doing based on the operating system we are installing the service and starting the service in target machines.


---
- name: provisioning servers
  hosts: all
  become: yes
  tasks: 
    - name: install ntp agent on centos
      yum:
        name: chrony
        state: present
      when ansible_distribution == "CentOS"

    - name: install ntp agent on ubuntu
      apt:
        name: ntp
        state: present
        update_cache: yes
      when ansible_distribution == "Ubuntu"

    - name: start the ntp service on centos
      service:
        name: chronyd
        state: started
        enabled: yes
      when ansible_distribution == "CentOS"

    - name: start the ntp service on ubuntu
      service:
        name: ntp
        state: started
        enabled: yes
      when ansible_distribution == "Ubuntu" 


Loops:

here we are doing base on the operating system we are installing the multiple services without writting same again using loops
and starting the service in target machines.

---
- name: provisioning servers
  hosts: all
  become: yes
  tasks: 
    - name: install ntp agent on centos
      yum:
        name: "{{item}}"
        state: present
      when ansible_distribution == "CentOS"
      loop:                        // here loop return the item thats why we are using that item as variable in the above.
        - chrony
        - wget
        - zip
        - unzip
        - git

    - name: install ntp agent on ubuntu
      apt:
        name: "{{item}}"
        state: present
        update_cache: yes
      when ansible_distribution == "Ubuntu"
      loop:                // here loop return the item thats why we are using that item as variable in the above.
        - ntp
        - wget
        - zip
        - unzip
        - git

    - name: start the ntp service on centos
      service:
        name: chronyd
        state: started
        enabled: yes
      when: ansible_distribution == "CentOS"

    - name: start the ntp service on ubuntu
      service:
        name: ntp
        state: started
        enabled: yes
      when: ansible_distribution == "Ubuntu" 


lets say how we can create the folder in target machines and banner file(when the user login into the target machine it displays the message)


---
- name: provisioning servers
  hosts: all
  become: yes
  tasks: 
    - name: install ntp agent on centos
      yum:
        name: "{{item}}"
        state: present
      when ansible_distribution == "CentOS"
      loop:
        - chrony
        - wget
        - zip
        - unzip
        - git

    - name: install ntp agent on ubuntu
      apt:
        name: "{{item}}"
        state: present
        update_cache: yes
      when ansible_distribution == "Ubuntu"
      loop:
        - ntp
        - wget
        - zip
        - unzip
        - git

    - name: create a folder
      file:
        path: /opt/test
        state: directory

    - name: Banner file
      copy:
        content: '# This server is managed by ansible and no manual changes please'
        dest: /etc/motd  // this the default location of banner file.

    - name: start the ntp service on centos
      service:
        name: chronyd
        state: started
        enabled: yes
      when: ansible_distribution == "CentOS"

    - name: start the ntp service on ubuntu
      service:
        name: ntp
        state: started
        enabled: yes
      when: ansible_distribution == "Ubuntu" 



Ansible-AWS:

Anisble collections are used to connect with the 3rd party api's like aws or azure etc.

lets see how we can configure aws using ansible.

1.we need provide authentication for ansible to connect with aws account.
2.create a IAM user
3.we need to export acess_key_id and secret_acess_key. if you export directly then these authentication values stores temporarly.
4.for permanently stores authentication values use bashrc file -> vim .bashrc
   add export AWS_ACCESS_KEY_ID='AK123'
       export AWS_SECRET_ACCESS_KEY='abc123' replace values with original values.
5.run source .bashrc -> to update the values.

                 or

we can secure the authentication details(like passwords) in ansible vaults.

  1.openssl rand -base64 2048 > vault.pass -> here we are generating the random password and storing in a vault.pass

  2.ansible-vault create group_vars/all/pass.yml --vault-password-file vault.pass and after runnning this command it will ask you to insert passwords.
   then AWS_ACCESS_KEY_ID='AK123' and AWS_SECRET_ACCESS_KEY='abc123'. place these keys here.(you can create IAM user and then you can create these keys).

  3.to access the variables in ansible you don't need anything just use "{{variable-name}}" that's it.

  4.if you secure your passwords in vaults and you try to execute the playbook it will fail because you have created a password for vault with base64.
    ansible try to fetch passwords from vault and the vault has the password so it fails to fetch the passwords from vault.

  5.while executing the playbook pass --vault-password-file vault.pass this at end.


NOTE:
      * ansible built-in modules are used to automate the tasks. these are executed on the manage nodes.

      * ansible collections are used to create the infrastructure such as creating the resources in aws or azure.these are executed on the control nodes.

      * extra vars has more precedence than ansible vars has more precedence than the ansible defaults.

      * extra vars usually pass while executing the playbook and pass -e variable-name=value.


Now lets see how we can create key-pair using ansible-playbook. and below are the pre-requisites for aws.


1.first we need to download this package -> ansible-galaxy collection install amazon.aws
2.we need to download the boto to connect to aws.
3. first install pip and then boto -> sudo apt install python3-pip -y
                                      pip3.10 install boto3
4.you need give the region while creating the tasks for key-pair.


- hosts: localhost
  gather_facts: False
  tasks:
    - name: create key-pair
      amazon.aws.ec2_key:
        name: sample-key
        region: us-east-1
      register: keyout   

    - name: print key
      debug:
        var: keyout
        
    - name: save key
      copy:
        content: "{{keyout.key.private_key}}"
        dest: ./sample.pem
      when: keyout.changed == True


Now lets see how we can launch aws ec2 instance using ansible-playbook. and below are the pre-requisites for aws.

1.first we need to download this package -> ansible-galaxy collection install amazon.aws
2.we need to download the boto to connect to aws.
3. first install pip and then boto -> sudo apt install python3-pip -y
                                      pip3.10 install boto3




- hosts: localhost
  gather_facts: False
  tasks:
    - name: create key-pair
      amazon.aws.ec2_key:
        name: sample-key
        region: us-east-1
      register: keyout   

    - name: print key
      debug:
        var: keyout

    - name: save key
      copy:
        content: "{{keyout.key.private_key}}"
        dest: ./sample.pem
      when: keyout.changed 

    - name: start an instance 
      amazon.aws.ec2_instance:
        name: "public-compute-instance"
        key_name: "sample-key"
       #vpc_subnet_id: subnet-5ca1ab1e
        instance_type: t2.micro
        security_group: default
       #network:
         #assign_public_ip: true
       image_id: ami-123456
       exact_count: 1
       region: us-east-1
       tags:
         Environment: Testing

Note:
       for cloud automation terraform is better when compare to ansible.


Error Handling in ansible playbooks:
                                      first let's how the order of execution of tasks in ansible playbook.

   * Let's say you write playbook with three tasks and the inventory file containes information about 3 manage nodes.you start executing the playbook then
     it will take the task-1 and execute in all the manage nodes and it should be successful(complete and successful) then only it will moves to the next 
     task.now it will take task-2 and execute in all the manage nodes and then it should be successful and so on.

   * while executing the task-1, if the playbook is failed then it will stop there only and it will not execute the rest the tasks. 

   * this is the default behaviour of the ansible while executing the tasks. i.e if the 1st tasks is complete and successful then only it will move to 
     another task.

   * if the one task get failed then remaining task won't execute this is the problem with the default behaviour of ansible.

   * To handle this type of errors, in every task at the last use "ignore_errors: yes" then if that task has some errors then it will not fail the playbook
     instead it will move to the next task.

   * if you want to ignore particular type of error in ansible playbook use "failed_when".

   * to use "failed_when", you have to register the output in a variable and inside failed_when on that output check the error that you want to ignore.

   * the difference between "ignore_errors: yes" and "failed_when" is ignore_errors will ignore all the errors and failed_when ignore particular type of 
     error only.

   example real-time scenario:
                                Let's say you have to install docker in 3 manage node and before installing docker on the manage nodes, you need to check 
                                some requirements,if the requirements matches then only you need to install do docker.

                                * the requirements are 1.openssh and openssl need to install and has the latest version
                                                       2.check whether the docker is installed or not.
                                                       3.if the above requirments matches then you need to install the docker.

                                * in these scenario, if one manage node doesnot have openssh latest version then ansible default behaviour will fail and
                                  it won't execute remaining tasks.

                                * if there are error in the task and you need to proceed further then use "ignore_errors: yes" at every task.if the error
                                  is present then it won't stop the execution of the tasks.

                                * this is how you can handle errors in ansible playbooks.

NOTE:
      * if you want to store a output to a variable  -> register: output(here output is variable-name you can give any name).

      * to print it -> - ansible.builtin.debug:
                           var: output(variable-name)


Securing ansible playbooks using Ansible Vault:
                                                 Ansible Vault is used to secure the sensitive information like passwords,api's and files.

  * Let's say your are creating the EC2 instance using ansible book there you need to mention the ACCESS_KEY and SECRET_KEY of aws iam user.if you execute 
    the playbook it will execute the task but if you want to share these playbook to others in github.your sensitive information is visible to everyone 
    which not secure.

  * using Vault we can solve this problem.

  * let' see how Vault works, take the file that contains sensitive information using ansible-vault we will provide the password for that file and it is 
    encrypted. 
   
  * if you want to change or added the content in that file, first you need to provide password for it and if its successful then you can change the content 
    in the file.

  * command to create Vault or encrypt file -> ansible-vault create group_vars/all/pass.yml --vault-password-file vault.pass

     here group_vars/all/pass.yml is location of the new file where it contains the encrpted data.
          --vault-password-file vault.pass is password for the file.

  * command to decrypt the file -> ansible-vault decrypt group_vars/all/pass.yml --vault-password-file vault.pass 

  * openssl rand -base64 2048 > vault.pass -> this commands is used to generate the password and store it in a vault.pass file.

  * command to edit the content in vault -> ansible-vault edit group_vars/all/pass.yml --vault-password-file vault.pass

  * if you want to encrypt the existing file -> ansible-vault encrypt group_vars/all/pass.yml --vault-password-file vault.pass


 NOTE:
       * the difference between create and encrypt in Vault is create commands create the new encrypted file where as encrypt is used to encrypt the 
         existing file.

       * regarding password what is the best practice to have for ansible playbooks,

               1.let's say you have 10 playbooks.each playbook has separate password which also bad practice because if the playbooks increases then it will
                 become difficult to manage password i.e 1000
                   
               2.if you provide the common password for all the ansible palybook which is also bad practice because if someone knows that passwords he can 
                 able to access all the playbooks.

               3.the best practice is keep one password for one environment i.e for dev one password, for staging one password and for prod another 
                 password.



Ansibel interview Questions:

1.what is configuration management :
                                     configuration management is method or process to managing your software or system hardware.Let's say in your 
                                     organisation you want to install software or packages or you wnat to manage upgradation of couple of packages etc.
                                     if you do this process manually it will take long time through this we can simplify the task and time.

2.do you think ansible is better than other configuration management tool? if yes, why :
                                                                                          Ansible is different from another configuration management tools and it is better in achieving the specific task or usecase.ansible is agentless that means if you want to configure 100 servers, to configure this 100 server you don't need to install any agent in the servers.the other configuration management tools have this agent and you need to install the agent in all the server which is bad like puppet, chef tools.in ansible only prereqisites is that you need to have passwordless authentication to connect with the server
or target machines.if you want to connect with linux it uses ssh and if you want to connect with windows it uses winrm.ansible uses YAML to write the script
which is very easy to read and understand. 


3. can you write an ansible playbook to install httpd services and get it running :

  two tasks you need to write 1.install httpd and 2.start httpd.

4.how ansible helped your organization :
                                          explain this with one example and say using script and using ansible to simplify the process. like install packages or upgrading the patches etc. and say using ansible how much time you saved.

5.what is ansible dynamic inventory :
                                      Let's just say your are using aws ec2 instance like 50 and your are tasked to install any package on that instances
and after later point of time you just wanted add more istances.so you go and search for the instances and add in the inventory which is not good.
so to solve this problem ansible comes with dynamic inventory.it will keeping for the instances if any need instances will create then ansible will auto
configure instances using dynamic inventory.

6.what is ansible galaxy command and why it is used for :
                                                           ansible galaxy command is used to bootstrap whole structure of the ansible playbooks.it will creates the files and folder required for it. instead of creating the whole structure by yourself use ansible galaxy.

7.can you explain me structure of ansible playbook using roles :
                                                                  the folder that comes in ansible playbook using roles are vars, defaults, files, templates, tasks, handlers and meta. ansible galaxy command is used to create this structure. describe the each folder.

8.what are handlers in ansible and why are they used :
                                                       handler are similar to the task and the only difference is it will only run when they notify.
let's say you have install the httpd and you wan to start the service. and in some cases you want to start on conditional basis then use handlers for it.

9.I would like to run a specific set of tasks only on windows vms and not on linux vms is it possible :
                                                                                                        yes it is possible.you can find the environment variable(for this take os) and use conditional basis(use when condition like when os is windows then run this task) for running of tasks in ansible.

10.does ansible support parallel execution of tasks :
                                                       yes, let's say you have 5 tasks and these 5 tasks should be executed in 5 vms then ansible takes
task-1 and parallel execute the task-1 in all the 5 vms if it's completed then it will move to the another tasks.if you want to execute the tasks parallelly then you need to multiple instances of ansible.

11.what is the protocol that ansible use to connect to windows vms :
                                                                     ansible use winrm to connect with the windows vms. and ssh to connect with linux.

12.can you place then in the order of precedence, palybook group_vars, role vars and extra vars :
                                                                                                   
                                                                                                   palybook group_vars, role vars and extra vars.

13.how do you handle secrets in ansible :
                                          you can store the secrets in ansible vaults. 

14.can we use ansible for IAAC(Infrastucture as a code) ? if yes, can you compare it with any other IAAC tools like terraform :
                                                                                                                               yes ansible supports the IAAC. the main use of ansible is configuration management and it also supports IAAC.


15.can you talk about a ansible playbook that you wrote and how it hepled your company :
                                                                                         let's say you want to install oracle databse and it will takes nearly 40-45 minutes and you want to install it on  50 vms and imagine how much time its take to install .using ansible playbok can save a lot of time.
if you are doing it manually there is a chance of errors.

16.what do you think that ansible can improve :
                                                 the window support for ansible should be improve. and ansible does not support such IDE's.
                                                 
 


                                                                              

AWS CI/CD project:


Elastic Beanstalk:
                   an easier way for the developers to quickly deploy and manage applications in the aws cloud.

  AWS CODE COMMIT:
                   it is a version control service hosted by amazon web services that you can use to privately store
                   and manage assets(such as documents,source code and binary files) in the cloud.
 
                    * simply it is like github to store the source code.

        
flow:
       1.create the beanstalk(first create application and then inside create environment).
          create a role for beanstalk with the policy names -> 1.AdministratorAccess-AWSElasticBeanStalk
                                                               2.AWSElasticBeanStalkCustomPlatformforEC2Role
                                                               3.AWSElasticBeanStalkRoleSNS
                                                               4.AWSElasticBeanStalkWebTier
       2.create RDS separately. 
       3.update the security group of rds(copy the security group-id of any instances and in rds security group allow 3306 from instance security group)
                                           so that we can ssh to instance and connect to RDS

       4.ssh to instance and install mysql in it and then connect to RDS
            -> mysql -h RDS-endpoint -u admin(username) -padmin123(password) accounts(databasename)

       5.clone the source code and dump the sql data to accounts.

             mysql -h RDS-endpoint -u admin(username) -padmin123(password) accounts(databasename) < src/main/resources/db_backup.sql 

       6.create the code commit(source) i.e upload the source code to codecommit in source.
             * create a IAM user and create a policy, attach to that user
         
             * we need ssh keys for aws CodeCommit -> open gitbash ssh-keygen.exe
                                                      save the key with different name.
                                                       cd .ssh
                                                       cat pubkey and copy the public key and upload ssh public key.

                                                       in the same path( cd .ssh). create config file.

                                                       Host git-codecommit.*.amazonaws.com
                                                         User atduibfdoiihnhvsrf(ssh key id)
                                                         IdentityFile ~/.ssh/vprofile-codecommit_rsa(file name that saved ssh-keygen.exe)

                                                       change file permission -> chmod 600 config

          * clone repository -> cd /tmp
                                git clone ssh://git-codecommit.us-east-1.amazonaws.com/v1/repos/vprofile-codecommit-repo -> this is from awscodecommit url
                                cd vprofile-codecommit-repo/
                                ssh git-codecommit.us-east-2.amazonaws.com -> to check whether we can connect to aws codecommit.
                                
                                cd ..
                                cd /Downloads
                                cd /project
                                git clone https://github.com/devopshydclub/vprofile-project.git
                                cd vprofile-project/ 

                                git branch -a -> to list all branches and select the branch you want to push.
                                git checkout master -> it will switch to master branch
                                git branch -a | grep -v HEAD | cut -d '/' -f3 | grep -v master -> it will display only branch name.
                                git branch -a | grep -v HEAD | cut -d '/' -f3 | grep -v master > /tmp/branches
                                for i in `cat /tmp/branches`:do git checkout $i;done

                                instead of above steps you can do checkout manually for all the branches.

                                git remote rm origin -> removing the remote repository.

                                git remote add origin ssh://git-codecommit.us-east-1.amazonaws.com/v1/repos/vprofile-codecommit-repo 
                                git push origin --all -> pushing to aws codecommit.

                                
       7.build the artifact and then upload to s3 bucket.
                 
             * create one bucket in s3 and add that bucket to store the artifact.
                 
             * in the build specification add the content:

this is to replace the database credentials

version: 0.2

#env:
  #variables:
     # key: "value"
     # key: "value"
  #parameter-store:
     # key: "value"
     # key: "value"

phases:
  install:
   runtime-versions:
      java: corretto8
  pre_build:
    commands:
      - apt-get update
      - apt-get install -y jq 
      - wget https://downloads.apache.org/maven/maven-3/3.8.8/binaries/apache-maven-3.8.8-bin.tar.gz
      - tar xzf apache-maven-3.8.8-bin.tar.gz
      - ln -s apache-maven-3.8.8 maven
      - sed -i 's/jdbc.password=admin123/jdbc.password=ILw5ZDjavaNHSqfhbzmG/' src/main/resources/application.properties
      - sed -i 's/jdbc.username=admin/jdbc.username=admin/' src/main/resources/application.properties
      - sed -i 's/db01:3306/vprords.corr19umlgnd.us-east-2.rds.amazonaws.com:3306/' src/main/resources/application.properties
  build:
    commands:
      - mvn install
  post_build:
    commands:
       - mvn package
artifacts:
  files:
     - '**/*'
  base-directory: 'target/vprofile-v2'

       8.create pipeline.




DOCKER:

       docker manages containers so it is called container runtime environment.



VM's Vs Container:

1.Containers offer isolation not virtualization.
2.containers are OS virtualization
3.VM's are hardware virtualization
4.VM needs OS
5.containers dont need os
6.containers uses Host OS for compute resources.

Containers:
            1.Containers offer Isolation and not virtualization.
            2.For virtualization we can use Virtual Machine.
            3.For understanding we can think containers as OS virtualization.

Note:
      Virtualization enables you to run multiple operating systems on the hardware of a single physical server,
      while containerization enables you to deploy multiple applications using the same operating system on a single virtual machine or server.

Docker:
       1.It is a container runtime environment for developing,shipping and running the applications.
       2.Docker registry(hub) is place where Docker Container Images are stored.
       3.Docker containers are created from Docker Images.

Note:
      1.docker run hello-world -> here docker run means create a container.
      2.docker images -> this will show you available images on your local computer.
      3.docker ps -> this will show you running containers.
      4.docker ps -a -> this will show you all the containers(both running and stopped containers).
      5.docker run --name web01 -d -p 9080:80 nginx -> here web01 is the container name
                                                            -d means run in background
                                                            -p means port
                                                             9080 means hostport(your system) and 80 means container port.
        here instead -p(small p),if you give -P(captial P) then portnumbers automatically created.

      6.docker inspect web01(container name) -> it will show the ip address.
      7.docker stop web01(container name) -> it will stop the container.
      8. docker rm web01(container names) -> this will delete the containers.
      9. docker rmi image-id -> it will remove the docker images.
      10.if you want to run multiple containers together their is a concept called docker-compose.
      11.docker compose up -d -> this will start the docker all containers
      12.docker compose down -> to stop all the containers
      13.docker system prune -a -> to remove all the stopped containers.
      14.docker pull nginx(image-name) -> it will download the image to local.
      15.docker pull nginx(image-name):mainline-alpine-perl(tags) -> it will download the image to local with these tag.
      16.docker exec myweb(image-name) ls / -> it will execute the command in myweb container and list the item.(just like we ssh to vm and do ls,in containers we can not ssh to the containers)
      17.docker logs nginx(image-name or image-id) -> it will show of the logs(container output) of that container.


* whenever you install docker in os, root user only have privilege to run the docker commands.
* if you execute the commands using normal user then you will get permission denied.
* if you want to execute commands the to normal user, then you need to change the permission to normal user
   
                -> sudo usermod -aG docker ubuntu(user)


problems with the containers are:

1.if you are hosting the nginx application in the container and the information about who logged and who accessed the application those things are stored 
  in a log file.Due to some problem the container goes down then the entire log file will be deleted.

2.if there is a frontend and backend application that shares information, due to some reason the backend goes down then the information that backend have 
  will be deleted.

3.if the container wants to read something from the host machine and it does not know how to do this.


To solve this problem docker come up with these persistent storage called docker volumes and bind mount.


Container Volumes:
                    Docker has two options for containers to store the files in the host machine.
                     
                             1.Volumes:
                                        Managed by Docker(/var/lib/docker/volume/) on linux
                             2.Bind Mounts:
                                        Stored anywhere in the host machine.

Bind Mount:
            bind mount are used to bind the folder or directory in host machine to container or vice versa.

            * if the container goes down then the information is available on the host machine and we can access those information even if the container 
              goes down because we binded to folder from container to host machine.

             1.pull mysql image from the dockerhub -> docker pull mysql:5.7
             2.create the directory in the current -> /home/ubuntu/vprofiledata
             3.bind this(host machine) directory with mysql directory.

                  -> docker run --name vprofiledb -d -e MYSQL_ROOT_PASSWORD=raju12 -p 5000:3306 -v /home/ubuntu/vprofiledata:/var/lib/mysql mysql:5.7 
                    check the mysql directory by running the inspect command(docker inspect imagename) and in volumes the path is present.

             4.then the data from /var/lib/mysql present in the vprofiledata.

   * Bind Mount is mostly used to inject data from host machine to container.
   * To store data better option is docker volumes.

Docker Volumes:
                 1.pull mysql image from the dockerhub -> docker pull mysql:5.7
                 2.create the volume -> docker volume create mydbdata
                 3.bind this(host machine) volume with mysql directory.

                    -> docker run --name vprofiledb -d -e MYSQL_ROOT_PASSWORD=raju12 -p 5000:3306 -v mydbdata:/var/lib/mysql mysql:5.7 
                       check the mysql directory by running the inspect command(docker inspect imagename) and in volumes the path is present.

                 4.then the data from /var/lib/mysql present in the mydbdata(/var/lib/docker/volumes/).

* docker volume ls -> it will show the available volumes.

* docker volume inspect demo(volume-name) -> it will show the all the details of the volume.

* docker volume rm demo(volume-name) -> it will delete the volume.

Note:

     1.Docker volumes may be interacted with using CLIs and APIs. Bind mounts cannot be accessed by CLI commands.

     2.All you need is the volume name to mount it.

     3.When using bind mounts for mounting, a route to the host computer must be supplied.

     4.container are light weight because they dont have the complete OS and they are using the resources from the host OS they are running onS.
 
     5.why we are moving from physical server to virtualization and virtualization to container because they are not using the maximum capacity of the
       resources.

     6. virtualization solves upto some extend from physical servers and containers solves upto some extend from virtualization.

     7.container is like package which consists of application,libraries(dependencies) and system libraries.


LifeCycle of Docker:

                    1.write the Dockerfile and execute it.
                    2.From that it will create the Image.
                    3.execute the image to create the container.

                              build                   run
               Dockerfile------------------>image---------------->container

  * Whenever you want to create the image use build command to create the image then from that image execute the run command to create the container.

Drawbacks of docker:
                     1.docker is fully dependented on DockerEngine.
                     2.whenever the the DockerEngine is down the entire docker containers will stop working.
                     3.buildah will solve the above problem and it also supports the docker images.
                     4.in buildah, you don't need to write the Dockerfile instead we write the shell script(commands) and create the images.

docker github link for notes: https://github.com/iam-veeramalla/Docker-Zero-to-Hero

* Docker daemon is the heart of the docker and if the docker doeman goes down then the docker will goes down.
* whenever you run commmand through CLI in docker, these commands will goes to doemon and it will execute those commands.

what is the problem that docker is solving is
          
* when you want to host application normal way is to setup the ec2 instance and setup the application and so on. they are many steps are involved.

* while using docker these problems will be solved easily, you just build the image then run it and that's it.you can share the image to public repository 
  like docker hub and any one can download and run it.there is no need to setup the entire process for it.just download it and run it that simple it is.

* Docker solves is it reduces the work flow(so many manual steps).

* docker doemon is used to listen to our requests and acts upon that request. and docker doemon run with root user.

* difference between Docker hub ang github is github is version control system is to store the source code and docker hub is also a version control system
  is used to store docker images.

* the difference betweeen CMD and ENTRYPOINT is, both commands are used to execute start command.so whenever someone run the docker run both CMD and 
  ENTRYPOINT serve as start command.

* the only difference is ENTRYPOINT is something that you cannot change or modify and CMD can be modified.

* whenever we have expose the port in the Dockerfile, while running the container we must map that expose port to the host port then only we can access it
  in the host machine.


while containerizing the application follow these steps:
1.select the base image
2.select the working directory, where your application data stores.
3.copy the requirements.txt and source code to the working directory.
4.if you have any start commands then write it in CMD or ENTRYPOINT or use both. 


* Docker images are heavy(in size) because everything we need to place in the same Dockerfile and we have to select the base image like Ubuntu or centos 
  based on the requirment then we need to install the package(dependencies) that the application needs by using run command and finally we write the CMD 
  and Entrypoint.due to these download of dependencies and the base image the size of the image will become big.these problem we can solve using
  multi stage docker builds and distroless images.

multi stage docker builds:
                           in multi stage docker builds everything we will write in one Dockerfile only but in differnt stages.

* there is no limit in stages, we can build the image in any number of stages.

* in first stage, we will select the base image and then install the package or dependencies that the application needs. and then in next stage we will 
  select the runtime environment for the application that is used to run the application i.e jdk or python based on the application and then we will copy 
  the above base image using copy --from build(above base image name) and then we will mention  CMD OR ENTRYPOINT command from the above to here to execute 
  the application.so when the image is created it will remove the base image from the created image and the size of the image is less.


Distroless docker image:
                        distroless images is very minimalistic image that will have only runtime environment and the basic commands like ls will not execute
                        because the main idea is to have only runtime environment and the main purpose of this to execute the application.

                        * distroless images typically contain only essential software required to run an application or service.

                        * the biggest advantage that you can get from the distroless images is security.

for example python distroless image can have only python runtime environment

link for Distroless images : https://github.com/GoogleContainerTools/distroless



Docker Images:
               lets see how we can build images and host application in container

Dockerfile instructions:

1.FROM -> Base Image
2.LABELS -> adds metadata to an image(i.e tags)
3.RUN -> execute commands in a new layer and commit the results.
4.ADD/COPY ->adds files and folder into image

difference between ADD and COPY is COPY just dump the file and ADD is we can provide link and it will download the file and extract it and dump the file.

5.CMD -> runs binaries/commands on docker run
6.ENTRYPOINT -> allow you to configure a container that will run as an executable
7.VOLUME -> create a mount point and marks it as holding externally mounted volumes.
8.EXPOSE -> container listens on the specified network ports at runtime
9.ENV -> sets the environment variables
10.USER -> sets the user name(or UID)
11.WORKDIR -> sets the working directory
12.ARG -> defines a variable that users can pass at build time
13.ONBUILD -> adds to the images a trigger instruction to be executed at a later time.

NOTE:

       1.RUN is used to execute commands during the build process of a Docker image, while CMD is used to specify the default command to run when a Docker 
         container is started from the image

       2.if you pass commands in CMD it will be overridden when you executing the container when you pass commands to it.

       3.ENTRYPOINT commands can not be overridden.

       4.in Dockerfile, if you have two ENTRYPOINTS then while executing the container it will take the latest ENTRYPOINT and the old one will be ignored.

       5.in Dockerfile, if you have two CMD then while executing the container it will take the latest CMD and the old one will be ignored.

       6.if you have CMD and ENTRYPOINT then it will execute the ENTRYPOINT and CMD will be append to the ENTRTPOINT.

now let's create a images and run it.

Dockerfile:


FROM ubuntu:latest
LABEL "Author"="sriraju"
LABEL "Project"="nano"
ENV DEBIAN_FRONTEND=noninteractive
RUN apt update && apt install git apache2 -y
CMD ["/usr/sbin/apache2ctl","-D","FOREGROUND"]
EXPOSE 80
WORKDIR /var/www/html
VOLUME /var/log/apache2
ADD nano.tar.gz /var/www/html

docker build -t sriraju12/nanoimg:v2(imagename:tag) -> command to build image

to push the image into dockerhub:
                                  1.run docker login and give username and password then you will get sucessfull
                                  2.docker push imagename (imagename should be like dockerhub-username/imagename:tag) 


Docker Network:
                networking is used to allow communicate with the containers and host machines.

scenarios:

1.let's see how container communicates with the host machine.

   * whenever the container is created, it automatically creates the virtualEthernet which is dockerzero(0).which is used to connect with host machine.

   * without dockerzero we can not connect with the host and this process is called bridge networking.

   * the default netwroking in docker is bridge networking. 

   * if you delete the dockerzero then the container will never connect or communicate with the host machine and the application inside the container
     will never reach with the internet and the users and their is no point to use the application inside the container. 


The other networking concept is Host Networking,

   * Host networking means the containers directly use the Eth0 of the host machine.

   * in this case whenever the container is created, the docker directly bind the Eth0 of the host machine to the container.so the container and host 
     machine are in the same CIDR range.So when you ping from container to host machine it will connect to it.

   * this is a very problematic approach. primary reason that we will go for container because they are very secure.

   * if the user has the access to the host machine then he can access the container also which is very bad.this is the very insecure way of creating the
     networking. 


* if we have the container that are running in the host machine i.e login container and finance(payment) container.If you use the default networking then  
  login container and the finance container uses the same DockerZero to connect or communicate with container.it is very easy for the hacker to provide 
  a single way to connect with the container which is bad.we need to provide the logical isolation so that they do not use common way(DockerZero) to connect
  to host i.e container should be independent.This is the bridge networking which is also not secure.In order to secure this we can achieve through 
  Bridge networking ifself using custom bridge networking.

* the solution is the login container(not contain any sensitive information) will use the DockerZero to communicate with the host machine and the
  finance container(payment) will use the custom bridge network that we have created so that their is no common way both containers use same DockerZero
  to connect.This is the way to achieve logical isolation to containers using networking. 

* docker network ls -> to see all the networks.

* docker network rm login(network-name) -> to delete the network.

Let's see how we can create the custom bridge network and attach it to the container,

1.create the custom bridge network. it will create the custom bridge network-> docker network create secure-network(name of the network)

2.while executing the container pass --network=secure-network .it will assign this network to the container.you check this by inspecting the container.
  

* if you want to create the host network then while executing the container pass --network=host that's it.

* while executing the container if you don't pass any network arguments then it will take as bridge network.


Docker Compose:

                 Docker Compose allows you to define and manage multi-container applications in a single YAML file.

* The problem with the normal docker container is, if you have multiple containers for example take e-commerce application where it has many services like
  login service, payment service , cart service, menu service, mysql service etc.

* by using docker container we have to write the docker file for each service and then we have to run the containers one after another individual which is 
  like difficult to do.

* instead we will use the docker-compose tool which also developed by docker INC. here we will write docker-compose YAML file and use the single command
  to run the services using docker-compose up

* we can easily up and down the containers using single command instead of running containers one by one.

* the another problem is with normal docker process is like we have to maintain the order while coming up the containers using the parameter "depends_on".
  for example before the payment service the mysql service should come up because if your making a payment you need mysql data without that payment service   
  is of no use.this problem also solve by docker-compose.

* docker-compose is not replacement for docker.

* docker-compose YAML file contains build & run which uses the Dockerfile.

* https://github.com/docker/awesome-compose -> which contains the docker-compose examples.

* the main important steps that you should remember while write the docker-compose is services, inside the services mention the service names and inside 
  the service name mention build(path where our dockerfile located) and mention hostname, ports mappings(internal container port to host port).these are
  all you should remember.

* the usecases of docker-compose is 1.local development - if you write the docker-compose file and share it with the developers for their testing in thier
                                                          local system.it is very simple to use the commands just docker-compose up and down to start and  
                                                          stop the containers.Developers does not need to know the all the docker commands to run the 
                                                          containers.
                                    2.testing - if you want to quickly test the application for the changes.



let's see how we can containerize the application using docker-compose:

   steps:
           1.steps to setup our stack services.
           2.find the right base image from dockerhub.
           3.write Dockerfile to customize the images.
           4.write docker-compose.yaml file to run multiple containers.
           5.test it and host images on dockerhub.


Docker init:
             docker init is a command-line utility that aids in the initialization of Docker resources within a project. It automatically generates 
             Dockerfiles, Compose files, and . dockerignore files based on the nature of the project, significantly reducing the setup time and complexity 
             associated with Docker configurations.


Docker interview Questions:

1.what is docker :
                   docker is a open source containerization plateform.it is used to manage the lifecycle of a containers.i use the docker to write the
                   dockerfile and build the images and then push those images into the docker hub or registry.

2.how containers are different from virtual machines:
                                                      containers are very light weight because they don't have complete operating system.where as virtual
                                                      machines have complete operating system which are heavy in size.

                                                      for example, if iam running a java application, what i need is i need a applications and the runtime
                                                      dependency to run the application and some system libraries that are need to run the application.

3.what is docker lifecycle :
                             here we need to explain from writing the dockerfile using commands like Base image, what dependencies shold be installed for
                             the application to run, copy the source code etc. and then building the image from the dockerfile and then running the 
                             container from the image and then pushing the image to the docker registry or hub.

4.what are the different docker components:
                                            whenever you install the docker in your local or in ec2 .it comes with a docker Cli and the docker daemon
                                            docker daeman is responsible for executing the your tasks.let's say as a user you execute a build command
                                            this request is received by the daemon and it will build the image.docker daemon is the heart of the docker.
                                            if the docker daemon goes down then then docker containers will not run.

5.what is the difference between docker COPY and docker ADD:
                                                             docker ADD can copy the files from the url. wehere as docker COPY which can only copy files
                                                             from the host system into the container.

                                                             docker ADD download the files from the url and then it will extract and dump the filein the
                                                             location. where as docker COPY does not do that just it will dump the file in the location.

6.what is the difference between CMD and ENTRYPOINT:
                                                        CMD parameter can be overridden and ENTRYPOINT parameters cannot be overridden.you can use either of 
                                                        them or combination of both.

7.what are the networking types in docker and what is the default :
                                                                    the default networking in docker is bridge networking. however you can change the 
                                                                    default type and configure one of them 1.custom Bridge
                                                                                                           2.host
                                                                                                           3.overlay
                                                                                                           4.macVlan

8.can you explain how to isolate networking between the containers :
                                                                     bydefault every container use the bridge networking and inside it uses the   
                                                                     virtualEthernet which also dockerzero to communicate with the host machine.

                                                                     if you want to isolate the networking then create the custom bridge networking and 
                                                                     while executing the containers pass argument as --network=secure-network which the
                                                                     custom network that we have created.this way we can achieve isolation between 
                                                                     containers.

9.what is multi stage build in container :
                                           multi stage build allows you to build your docker container in multiple stages allowing you to copy artifacts
                                           from one stage to other.The major advantage of this is to build light weight containers. explain with example. 

10.what are distroless images in docker :
                                          distroless images contain only your application and its runtime dependencies with a very minimum operating system
                                          libraries.They do not contain package managers, shells or any other programs you would expect to find in a 
                                          standard linux distribution.they are very small and light weighted images.

11.Real time challenges with docker :

    * Docker is a single daemon process.which can cause a single point of failure, if the docker daemon goes down for some reason all the applications are 
      down.

    * Docker daemon runs as a root user.which is a security threat.any process running as a root can have adverse effects.when it is comprised for security
      reasons,it can impact other applications or containers on the host. 

    * Resource constraints : if you are runnning too many containers on a single host, you may experience issues with resource constraints.this can result
                             in slow performance or crashes.

                             let's say you are running 10 containers in a single host and one containers is using more memory or leaking the memory the 
                             other containers don't get what they want.so it will effect the other containers as well.

12.what steps would you take to secure containers :

  some steps are

                 1.use distroless images with not too many packages as your final image in multi stage build, so that there is less chance of security               
                   issues.

                 2.ensure that the networking is configured properly.this is one of the most common reason for securtiy issues.if required configure
                   custom bridge networks and assign them to isolate containers.

                 3.use utilities like sync to scan your container images.	
                                          
                            
                                            



Kubernetes:
            Kubernetes automates operational tasks of container management and includes built-in commands for deploying applications,
            rolling out changes to your applications, scaling your applications up and down to fit changing needs, monitoring your applications,
            and more—making it easier to manage applications.

            * While Docker is a container runtime, Kubernetes is a platform for running and managing containers from many container runtimes.


    Master:kube API server:
                             * main hero, handles all the requests and enables communications across stack services.
                             * component in the master that exposes the kubernetes API.
                             * it is the front-end for the kubernetes control plane.
                             * admin connects to it using kubectl CLI
                             * web dashboard can be integrated with this API.


   Master:ETCD server:
                        * stores all the information.
                        * consistent and high available key value store used as kubernetes backing store for all the cluster data
                        * kube API stores retrieves info from it.
                        * should be backup regularly.
                        * stores current state of everything in the cluster.


   Master:kube scheduler:
                           * watches newly created prods that have no node assigned and select a node from them to run on.


  Master : controller manager:
                                * logically, each controller is a separate process.
                                * to reduce complexity,they are all compiled into a single binary and run in a single process.
                                * these controllers include:
                                                              1.Node controller:
                                                                                 responsible for noticing and responding when nodes go down.
                                                              2.Replication controller:
                                                                                       responsible for maintaining the correct number of pods for 
                                                                                       every replication controller object in the system.
                                                              3.Endpoints controller:
                                                                                       populates the endpoints object(i.e joins services and pods)
                                                              4.Service Account and Token controller:
                                                                                                      create default accounts and api access tokens
                                                                                                      for new namespace. 


    worker: kublet:
                    an agent that runs on each node in the cluster.it makes sure that containers are running in a pod.

    worker: kube proxy:
                        * network proxy that runs on each node in your cluster.
                        * network rule:
                                      rules allow network communication to your pods inside or outside of your cluster.

    worker: container runtime: kubernetes supports several container runtime,
                                                                                * Docker
                                                                                * containerd
                                                                                * kubernetes CRI(container runtime interface)

minikube setup:
                1.open powershell as admin
                2.setup chocolaty
                3.install minikube with chocolaty( choco install minikube kubernetes-cli)
                4.open powershell and run(minikube start)
                5.minikube configuration is present in the current working directory(.kube/config)


kops setup:
            1.domain for Kubernetes DNS records(i.e GoDaddy.com)
            2.create a Linux vm and setup(kops,kubectl,ssh keys,awscli)
            3.login to aws account and setup(s3 bucket,IAM user for awscli,route53 hosted zone)
            4.login to domain registry(GoDaddy.com) and create NS records for subdomain pointing to route53 hosted zone NS servers.

note:
      kubectl -> Kubernetes.io/docs/tasks/install-kubectl
      kops -> GitHub.com/Kubernetes/kops/releases


nslookup -type=ns domainname(here kopskubevpro.hkhpro.life) -> it will list all the nameservers.
               


* command for storing the configuration for creating the cluster in s3 bucket:

kops create cluster --name=kopskubevpro.hkhpro.life(domain-name) \        here backward slash means continue command in nextline
--state=s3://kopsbucket1237(s3 bucket-name) --zone=us-east-1a,us-east-1b \
--node-count=2 --node-size=t3.small --master-size=t3.medium --dns-zone=kopskubevpro.hkhpro.life(domain-name) \
--node-volume-size=8 --master-volume-size=8

* if you make any changes to kops configuration then you update the changes by running the command:

     kops update cluster --name kopskubevpro.hkhpro.life(domain-name) --state=s3://kopsbucket1237(s3 bucket-name) --yes --admin

* kops validate cluster --state=s3://kopsbucket1237(s3 bucket-name) -> it will show that your cluster is ready 

* kubectl get nodes -> it will show all the nodes.

* kops delete cluster --name=kopskubevpro.hkhpro.life --state=s3://kopsbucket1237(s3 bucket-name) --yes -> it will delete the cluster



kubernates objects:
                     1.pod -> containers are present inside the pod
                     2.service -> to have an static endpoint to the pod like load balancer.
                     3.replica set -> to create the cluster of pods.
                     4.deployment -> which work similar to replica set and you can deploy new image tag by using deployment(most used)
                     5.config map -> to store our variables and configuration
                     6.secret -> variable and some information to store that are not in clear text.
                     7.volumes -> we can attach different types volumes to pod.



KubeConfig file:
                  use kubeconfig file to organize information about
                                                                     1.clusters
                                                                     2.users
                                                                     3.Namespaces
                                                                     4.Authencation mechanisms 

Namespaces:
            In Kubernetes, namespaces provide a mechanism for isolating groups of resources within a single cluster.
            Names of resources need to be unique within a namespace, but not across namespaces 

            * kubectl get ns -> it will display all the namespaces    
            * kubectl get all -> it will display all the objects that are present in the default namespaces.
            * kubectl get all --all-namespaces -> it will display all the objects of all namespaces. 
            * kubectl create ns kubekart(namespace-name) -> it will create the namespace with the name kubekart.
            * kubectl run ngnix --image=nginx --namespace=kubekart -> it will run the nginx in the namespace.



Pods:
       pod is the basic execution unit of a kubenetes application the smallest and simplest unit in the Kubernetes object model that you 
       create or deploy.A pod represents processes running on your cluster. 

       * Pods that run a single container:
                                           1.The one-container-per-pod model is the most common Kubernetes use case.
                                           2.Pod as a wrapper around a single container.
                                           3.Kubernetes manages the pods rather than containers directly.

      * Multi container pod:
                             1.Tightly coupled and need to share resources.
                             2.One main container and other as sidecar(supporting containers) or init container.
                             3.Each pod is meant to run a single instance of a given application(for example run only tomcat server).
                             4.Should use multiple pods to scale horizontally.

     * kubectl create -f pod-setup.yml -> to create the pod using yaml file
   
                the yaml file looks like this,      apiVersion: v1
                                                    kind: Pod
                                                    metadata:
                                                      name: webapp-pod(pod-name)
                                                      labels:                      //labels are like tags
                                                        app: frontend
                                                        project: vprofile
                                                    spec:
                                                      containers:
                                                        - name: httpd-container(container-name)
                                                          image: httpd
                                                          ports:
                                                            - name: http-port(port-name)
                                                              containerPort: 80

     * kubectl get pod -> it will display the pod details(i.e name of the pod,status,age)

     * kubectl describe pod webapp-pod -> it will display entire details of the pod
  
     * kubectl get pod webapp-pod -o yaml -> it will display the details of pod in yaml format.

     * kubectl edit pod webapp-pod -> to edit the pod but most of the things in pod is non-editable.

     * kubectl apply -f web.yaml -> it will apply the changes to the pod.
  
     * kubectl logs webapp-pod -> it will display logs of that pod.


Service:
         way to expose an application running on a set of pods as a network service(similar to load balancer).

         1.nodeport:
                     exposing(mapping the ports) the pods to outside network(not for production).
        2.clusterIp:
                     you dont want to expose to external world but for the internal communication.
        3.load balancer:
                         expose the pod to external network for the prodution.

        * kubectl create -f service-defs.yaml -> it is used to create the service using yaml file.

        * kubectl get svc -> it will display all the services.

        * kubectl describe svc webapp-service(service-name) -> it will display the details of the service.
      
        * kubectl delete svc webapp-service(service-name) -> it is used to delete the service.

         Note:
                * while creating the service two things are important 1.selector should be match with pod label name
                  and target port should be match with pod ports name or pod ports name.


        let's create a service using nodeport:
                                                1.first create the pod

                                                      apiVersion: v1
                                                      kind: Pod
                                                      metadata:
                                                        name: webapp-pod(pod-name)
                                                        labels:                      //labels are like tags
                                                          app: vproapp
                                                      spec:
                                                        containers:
                                                          - name: vpro-container(container-name)
                                                            image: sriraju12/vprofileapp
                                                            ports:
                                                              - name: vpro-port(port-name)
                                                                containerPort: 8080

    
                                                           
                                                2.create the service
 
                                                     apiVersion: v1
                                                     kind: Service
                                                     metadata:
                                                       name: helloworld-service
                                                     spec: 
                                                       ports:
                                                       - port: 8090 (internal port)
                                                         nodePort: 30001(external port to access and nodeport start from 30000)
                                                         targetPort: vpro-port or 8080 (you can give portnumber or portname of the pod)
                                                         protocol: TCP
                                                       selector:
                                                         app: vproapp
                                                       type: NodePort



      
          let's create a service using loadbalancer:
                                                    1.first create the pod

                                                      apiVersion: v1
                                                      kind: Pod
                                                      metadata:
                                                        name: webapp-pod(pod-name)
                                                        labels:                      //labels are like tags
                                                          app: vproapp
                                                      spec:
                                                        containers:
                                                          - name: vpro-container(container-name)
                                                            image: sriraju12/vprofileapp
                                                            ports:
                                                              - name: vpro-port(port-name)
                                                                containerPort: 8080

    
                                                           
                                                   2.create the service
 
                                                     apiVersion: v1
                                                     kind: Service
                                                     metadata:
                                                       name: helloworld-service
                                                     spec: 
                                                       ports:
                                                       - port: 80 (loadbalancer port to access)
                                                         targetPort: vpro-port or 8080 (you can give portnumber or portname of the pod)
                                                         protocol: TCP
                                                       selector:
                                                         app: vproapp
                                                       type: LoadBalancer



Replica set:
             in simple words replica set means autoscaling.If the pod is goes down then replica will automatically creates new one for us.
            
              
* kubectl get rs -> it will get all the replica sets.

* to scale down two ways -> 1.directly edit the replica yaml file.
                            2.kubectl scale  --replicas=1 rs/frontend(replica-name that is mention in yaml file) => not recommended in production


* to scale up two ways -> 1.directly edit the replica yaml file.
                          2.kubectl edit rs frontend or kubectl edit rs/frontend => not recommended in production.


let's see how we can write replica set yaml file,

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:          // this template information is about pod
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5




Deployment:
             1. A deployment controller provides declarative updates for pods and replicasets.
             2. define desired state in a deployment, and the deployment controller changes
                 the actual state of the desired state at a controlled rate.
             3. deployment creates replicaset to manage number of pods.


* kubectl set image deployment/nginx-deployment(deployment-name mention in yaml file) nginx(image-name)=nginx:1.16.1(mention different version of nginx).
  this is not recommended for production.Best way is directly edit the yaml file and apply the changes.

* kubectl get deploy -> it will display all the deployments.

* whenever you make deployments,then the all the pods will delete one by one and create a new pods one by one by replica set.

* kubectl rollout undo deployment/nginx-deployment(deployment-name mention in yaml file) -> it will rollback to previous version on the image.

* kubectl rollout history deployment/nginx-deployment(deployment-name mention in yaml file) -> it will display the revision numbers of rollout.

* kubectl rollout undo deployment/nginx-deployment(deployment-name mention in yaml file) --to-revision=2 -> it will rollback to revision 2 of the image.

* kubectl scale deployment/nginx-deployment(deployment-name mention in yaml file) --replicas=6 -> it will scale up the pods.

* kubectl delete deploy nginx-deployment(deployment-name mention in yaml file) -> it will delete the deployment.


let's see how we can create the deployment yaml file,

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80




Commands and Arguments:
                        how to pass commands and arguments to pods.


let's see how we create the pod using Commands and Arguments in yaml file

apiVersion: v1
kind: Pod
metadata:
  name: command-demo
  labels:
    purpose: demonstrate-command
spec:
  containers:
  - name: command-demo-container
    image: debian
    command: ["printenv"]
    args: ["HOSTNAME", "KUBERNETES_PORT"]
  restartPolicy: OnFailure



Config Map:
             set and inject variables in the pod. A ConfigMap is an API object used to store non-confidential data in key-value pairs


* kubectl create configmap db-config --from-literal=MYSQL_DATABASE=accounts \
  --from-literal=MYSQL_ROOT_PASSWORD=raju12 (not recommended for production).  -> it will create db-config file of configmap
                                                                                  and inject these variable in the pod
* kubectl get cm -> it will display all the configmaps.

* kubectl get cm db-config(configmap-name mention in yaml file) -o yaml -> it will display the details of the file in yaml format.

* kubectl describe cm db-config(configmap-name mention in yaml file) -> it will show indetails of the file.

let's see how we can create the configmap in yaml,

apiVersion: v1
kind: ConfigMap
metadata:
  name: db-config 
data:
  MYSQL_ROOT_PASSWORD=raju12
  MYSQL_DATABASE=accounts

lets see how we can inject these configmap file to pod,

  * this will inject the entire configmap variables to the pod.


apiVersion: v1
kind: pod
metadata:
  name:db-pod
  labels:
    app: db
    project: vprofile
spec:
  containers:
    - name: mysql-container
      image: mysql:5.7
      envFrom:
        - configMapRef:
            name: db-config
      ports:
        - name: db-port(port-name)
          containerPort: 3306


  * if you want to inject specific variables from configmap file to the pod.


apiVersion: v1
kind: pod
metadata:
  name:db-pod
  labels:
    app: db
    project: vprofile
spec:
  containers:
    - name: mysql-container
      image: mysql:5.7
      env:
        - name: MYSQL_ROOT_PASSWORD(variable name this will store in the container.it will take value from the key and store in this variable)
          valueFrom:
            configMapKeyRef:
              name: db-config
              key: MYSQL_ROOT_PASSWORD
      ports:
        - name: db-port(port-name)
          containerPort: 3306

this will inject only MYSQL_ROOT_PASSWORD variable to the pod.


  * if you want to inject specific variables from configmap file through volumes to the pod.

apiVersion: v1
kind: pod
metadata:
  name:db-pod
  labels:
    app: db
    project: vprofile
spec:
  containers:
    - name: mysql-container
      image: mysql:5.7
      env:
        - name: MYSQL_ROOT_PASSWORD(variable name this will store in the container.it will take value from the key and store in this variable)
          valueFrom:
            configMapKeyRef:
              name: db-config
              key: MYSQL_ROOT_PASSWORD
      volumeMounts:
      - name: config(volume name)
        mountPath: "/config" (below mention path file will be stored in the location)
        readonly: true
  volumes:
  - name: config(name of the volume)
    configMap:
      name: db-config(name of the configmap)
      items:
      - key: "MySQL_ROOT_PASSWORD"
        path: "MySQL_ROOT_PASSWORD" (the value of the key will be stored in this file)
 
      ports:
        - name: db-port(port-name)
          containerPort: 3306

NOTE:****

* kubectl exec --stdin --tty db-pod(pod-name) --bin/sh -> to login into the pod


Secrets:
         store and manage sensitive information such as password.


* kubectl create secret generic db-secret(secret-name) --from-literal=MYSQL_ROOT_PASSWORD=raju12 -> it will create the secret in imperative which bad way
  and password will be stored as encrypted password.

* let's see how we can create secret using declarative way in yaml file

 first you need to encode the password for that use -> echo -n "raju12" | base64  => it will encode the password


apiVersion: v1
kind: Secret
metadata:
  name: dbpass_secret
data:
  username: eghegaehipofugbjda24(by using this echo -n "raju12" | base64 => you will get encoded password)
  password: uwfgoisehoifvbseise24
type: Opaque

kubectl create -f yamlfilename -> it will create the secret.
  

lets create the pod for the secret.
      

apiVersion: v1
kind: Pod
metadata:
  name: secret-pod
spec:
  containers:
  - name: secret-container
    image: sriraju12/vprofiledb
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: dbpass_secret(secret-name)
            key: username
            optional: false

      - name: SECRET_PASSWORD
        valueFrom:
          secretKeyRef:
            name: dbpass_secret
            key: password
            optional: false

  restartPolicy: Never



Ingress:
          An API object that manages external access to the services in a cluster, typically HTTP.

          Ingress may provide load balancing, SSL termination and name-based virtual hosting.

          Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster.
          Traffic routing is controlled by rules defined on the Ingress resource





TerraForm:
            Terraform is an automatication tool for cloud.


* terraform file should be saved with .tf format.

* terraform init -> it will check for the provider and download the necessary plugins for it.

* terraform validate -> to validate the terraform file by checking syntax and colons.

* terraform fmt -> to format the terraform file i.e it will make the file in more readable way.

* terraform plan -> it will show, what is the execution the file.

* terraform apply -> it will start executing the script.

* terraform maintains the state i.e it stores the current information in the state, if you make any changes and apply the changes 
  then it will compare the current state with remote state and then apply the changes.

* terraform destroy -> it will delete the resources.


Let's see how we can launch ec2 instance from terraform script,


steps:
       1.create the IAM user with AdministratorAccess
       2.configure aws by providing the access_key and secret_key of the IAM user.
       3.write the terraform
       4.run the terraform commands to execute the script.


provider "aws" {
   region = "us-east-1"
  #access_key = ""
  #secret_key = ""  // bad pratice to mention crendentials here.Better way is configure through awscli(aws configure command to configure aws).
}

resource "aws_instance(resource type to create ec2 instance)" "intro(name of resource)" {

             ami = "ami-6489284094dsg1344"
             instance_type = "t2.micro"
             availability_zone = "us-east-1a"
             key_name = "terra-key" (create the key in aws)
             vpc_security_group_ids = ["sg-54263t728t348yg3"]
             tags = {
                Name = "terraform-demo"
             }
}


variables:
           variables help to move sensitive or critical information out of the script or moved to another file,
           so that we can easily reuse or modify them. EX : Ami,tags and key-pair etc.

  Let's see how we can write the terraform script for ec2 instance to launch using variables,

steps:
       1.write providers.tf file.
       2.write vars.tf file.
       3.write instance.tf file.
       4.run the terraform commands to execute the script.


provisers.tf file,

                    provider "aws" {
                     
                          region = var.REGION
                    }

vars.tf file,

             variable REGION {
      
               default = "us-east-1"
           }

            variable ZONE1 {
              default = "us-east-1a"
            }

             variable INSTANCETYPE {
               default = "t2.micro"
              }

              variable AMIS {
        
                 type = map
                 default = {
                    us-east-1 = "ami-5t2783490094bgc34"
                    us-west-2 = "ami-234567iugufc45678"

                  }
            } 


instance.tf file,

                    resource "aws_instance" "ec2-launch" {

                             ami = var.AMIS[var.REGION]
                             instance_type = var.INSTANCETYPE
                             availability_zone = var.ZONE1
                             key_name = "terra-key" (create the key in aws)
                             vpc_security_group_ids = ["sg-54263t728t348yg3"]
                             tags = {
                                Name = "terraform-demo"
                            }
                       }


Provisioners:
              provisioning means when the vm is up or when the instance is going to launch at the you want to perform
              some tasks or operation in the vm.

let's see how we can connect to Linux machine through terraform,

SSH

provisioner "file" {

   source = "files/test.conf"
   destination = "/etc/test.conf"

   connection {

     type = "ssh"
     user = "root"
     password = var.PASSWORD
}
}



let's see how we can connect to windows machine through terraform,

WinRM

provisioner "file" {

   source = "conf/myapp.conf"
   destination = "c:/App/myapp.conf"

   connection {

     type = "winrm"
     user = "administrator"
     password = var.ADMIN_PASSWORD
}
}

More Provisioners:
                     1. file -> the file provisioner is used to copy files or directori es
                     2. remote-exec -> invokes a command/script on remote resource.(it will execute in remote machine)
                     3. local-exec -> provisioners invokes a local executable after a resource is created.(it will execute in local machine)



Now let's see how we can create the ec2 instance and how to create a key-pair,how to push file/script from local to remote
and how to connect with ec2 and execute some script on the instance.


first let's see the script we want to execute on remote instance,

#!/bin/bash
yum install wget unzip httpd -y
systemctl start httpd
systemctl enabled httpd
wget https://www.tooplate.com/zip-templates/2096_individual.zip
unzip 2096_individual.zip
cp -r 2096_individual/* /var/www/html/
systemctl restart httpd


let's see vars.tf file,

variable USER {

   default = "ec2-user"
}

variable REGION {

 default = "us-east-1"
}

variable ZONE1 {

default = "us-east-1a"

}

variable AMIS {

type = map

default = {

us-east-1 = ami-4567890gjhe599
us-west-2 = ami-436587vjkhiu47
}
}

now let's see providers.tf file

provider "aws" {

   region = var.REGION

}

now let's see instance.tf file,

resource "aws_key_pair" "terra-key" {

  key_name = "terraform-key"   // this is key-pair that created in aws.
  public_key = file("terra-key.pub")  // this terra-key will generated using ssh-keygen and save this with name terra-key.
                                         it will create the public-key and private-key. file is used to read the content.
}

resource "aws-instance" "terra-instance" {

             ami = "ami-6489284094dsg1344"
             instance_type = "t2.micro"
             availability_zone = var.ZONE1
             key_name = aws_key_pair.terra-key.key_name(resourceType.resourceName.attributeName)
             vpc_security_group_ids = ["sg-54263t728t348yg3"]
             tags = {
                Name = "terraform-demo"
             }

   provisioner "file" {

    source = "web.sh"
    destination = "/tmp/web.sh

   }

provisioner "remote_exec" {

  inline = [
  
       "sudo chmod u+x /tmp/web.sh",
       "sudo /tmp/web.sh"
]
}

 connection {

   user = var.USER
   private_key = file("terra-key")
   host = self.public_ip

  }


}


Output:
        * Terraform stores a retuned values of all resources created.
        * use output block to print the attributes.
        * use local-exec to save information into the file.

ex:
     output "instance_ip_addr"(output-name) {

        value = aws_instance.server.public_ip  // resourceType.resourceName.attributeName
}   // it will print the public ip address.

let's see how we can print the public-ip anf private-ip of above terra-instance,

above code is same but add this in instance.tf file at last,

output "PUBLICIP" {

   value = aws_instance.terra-instance.public_ip   // it will print public-key
}

output "PRIVATEIP {

  value = aws_instance.terra-instance.private_ip  // it will print private-key
}




Backend:
         Terraform maintains the state and if you want to maintain common state or sync the state with all the teams then
         store in s3 bucket.

let's take above code and we will create the backend file called backend.tf 

terraform {

  backend "s3" {
    bucket = "terra-mybucket123" //bucket-name
    key = "terraform/backend   // terraform is the folder-name inside the bucket and backend in this file state should be mentioned.
    region = "us-east-1"
}
}


Now let's see how we can use multiple resources together.

first we will create an ec2 instance, create own key-pair and security-group,create volume and attach to the instance
create vpc and 6 subnets in it( 3 public subnets & 3 private subnets), create internet-gateway and create route tables
and route-table associations. and provisioning one website in the instance and backend state(for maintaining common state).


first let's create the providers file using provisioners.tf file,

provider "aws" {

   region = var.REGION
}

let's create the vpc using vpc.tf file,

resource "aws_vpc" "terra-vpc" {

   cidr_block = "10.0.0.0/16"
   instance_tenancy = "default"
   enable_dns_support = "true"
   enable_dns_hostnames = "true"
 
    tags = {
      Name = "vprofile"
 }


resource "aws_subnet" "terra-pub-1" {

   vpc_id = aws_vpc.terra-vpc.id
   cidr_block = 10.0.1.0/16
   map_public_ip_on_launch = "true"
   availability_zone = var.ZONE1
   tags = {
     Name = "terra-pub-1"
}
}

resource "aws_subnet" "terra-pub-2" {

   vpc_id = aws_vpc.terra-vpc.id
   cidr_block = 10.0.2.0/24
   map_public_ip_on_launch = "true"
   availability_zone = var.ZONE2
   tags = {
     Name = "terra-pub-2"
}
}


resource "aws_subnet" "terra-pub-3" {

   vpc_id = aws_vpc.terra-vpc.id
   cidr_block = 10.0.3.0/24
   map_public_ip_on_launch = "true"
   availability_zone = var.ZONE3
   tags = {
     Name = "terra-pub-3"
}
}

resource "aws_subnet" "terra-priv-1" {

   vpc_id = aws_vpc.terra-vpc.id
   cidr_block = 10.0.4.0/24
   map_public_ip_on_launch = "true"
   availability_zone = var.ZONE1
   tags = {
     Name = "terra-priv-1"
}
}

resource "aws_subnet" "terra-priv-2" {

   vpc_id = aws_vpc.terra-vpc.id
   cidr_block = 10.0.5.0/24
   map_public_ip_on_launch = "true"
   availability_zone = var.ZONE2
   tags = {
     Name = "terra-priv-2"
}
}

resource "aws_subnet" "terra-priv-3" {

   vpc_id = aws_vpc.terra-vpc.id
   cidr_block = 10.0.4.0/24
   map_public_ip_on_launch = "true"
   availability_zone = var.ZONE3
   tags = {
     Name = "terra-priv-3"
}
}

resource "aws_internet_gateway" "terra-IGW" {

  vpc_id = aws_vpc.terra-vpc.id
  tags = {
    Name = "terra-IGW"
 }

}

resource "aws_route_table" "terra-pub-RT" {

    vpc_id = aws_vpc.terra-vpc.id

   route {
       cidr_block = "0.0.0.0/0"
       gateway_id = aws_internet_gateway.terra-IGW.id
   }
     tags = {
       Name = "terra-pub-RT"
}

resource "aws_route_table_association" "terra-pub-1-a" {
    
    subnet_id = aws_subnet.terra-pub-1.id
    route_table_id = aws_route_table.terra-pub-RT.id
}

resource "aws_route_table_association" "terra-pub-2-a" {
    
    subnet_id = aws_subnet.terra-pub-2.id
    route_table_id = aws_route_table.terra-pub-RT.id
}

resource "aws_route_table_association" "terra-pub-3-a" {
    
    subnet_id = aws_subnet.terra-pub-3.id
    route_table_id = aws_route_table.terra-pub-RT.id
}


now let's create thge security_group for VPC using security-group.tf,

  resource "aws_security_group" "terra-sg" {
  
    vpc_id = aws_vpc.terra-vpc.id
    name = "terra-sg"
    description = "security group for vpc and ec2 instance"

    egress {  // outbound-rule

      from_port = 0
      to_port = 0
      protocol = "-1"
      cidr_block = ["0.0.0.0/0"]
}

    ingress {

     from_port = 22
     to_port = 22
     protocol = "tcp"
     cidr_block = [var.MYIP]
    }

  tags = {
    Name = allow-ssh"
}


now let's write the script for provisioning website using web.sh file,

#!/bin/bash
yum install wget unzip httpd -y
systemctl start httpd
systemctl enabled httpd
wget https://www.tooplate.com/zip-templates/2096_individual.zip
unzip 2096_individual.zip
cp -r 2096_individual/* /var/www/html/
systemctl restart httpd

let's write the backend file for storing the state using backend.tf file,

terraform {

  backend "s3" {
    bucket = "terra-mybucket123" //bucket-name
    key = "terraform/backend   // terraform is the folder-name inside the bucket and backend in this file state should be stored.
    region = "us-east-1"
}
}

let's create variables using vars.tf file,

variable USER {

   default = "ec2-user"
}

variable REGION {

 default = "us-east-1"
}

variable ZONE1 {

default = "us-east-1a"

}

variable ZONE2 {

default = "us-east-1b"

}

variable ZONE3 {

default = "us-east-1c"

}

varible MYIP {

  default = "12.24.134.12/24"
}


variable AMIS {

type = map

default = {

us-east-1 = ami-4567890gjhe599
us-west-2 = ami-436587vjkhiu47
}
}

variable PUBLICKEY {

  default = "terrakey.pub"
}

variable PRIVATEKEY {

  default = "terrakey"
}

now let's create the instance.tf file,


resource "aws_key_pair" "terra-key" {

     key_name = "terrakey"
     public_key = file(var.PUBLICKEY)
}


resource "aws-instance" "terra-instance" {

             ami = var.AMIS[var.REGION]
             instance_type = "t2.micro"
             subnet_id = aws_subnet.terra-pub-1.id
             availability_zone = var.ZONE1
             key_name = aws_key_pair.terra-key.key_name(resourceType.resourceName.attributeName)
             vpc_security_group_ids = [aws_security_group.terra-sg.id]
             tags = {
                Name = "terraform-demo"
             }

      provisioner "file" {

          source = "web.sh"
          destination = "/tmp/web.sh

        }

provisioner "remote_exec" {

  inline = [
  
       "sudo chmod u+x /tmp/web.sh",
       "sudo /tmp/web.sh"
]
}

 connection {

   user = var.USER
   private_key = file(var.PRIVATEKEY)
   host = self.public_ip

  }

   }

  resource "aws_ebs_volume" "terra-volume" {

      availability_zone = var.ZONE1
      size = 3
      tags = {
        Name= "terra-volume"
}
}

resource "aws_volume_attachment" "terra-vol-attach" {

   device_name = "/dev/xvdh"
   volume_id = aws_ebs_volume.terra-volume.id
   instance_id = aws_instance.terra-instance.id
}

output "PUBLICIP" {

    value = "aws_instance.terra-instance.public_ip"
}




CloudFormation:
                  CloudFormation is a cloud automation tool just like Terraform.Only difference is t

                 coludformation is specific to aws and where as Terraform supports various types.


   


























   
     


      


   
                                         





               
                        
  







  


 

 
            
                 	 
                           
                        







    

    

        








        

        





                       






                                          
  
    

             

 








                   
  
                   
          

























  
















      


   







         


                                                        




   




	



                                










                    





 
 
          
      
        

 



 



