TerraForm:
            Terraform is an automatication tool for cloud.


* terraform file should be saved with .tf format.

* terraform init -> it will check for the provider and download the necessary plugins for it.

* terraform validate -> to validate the terraform file by checking syntax and colons.

* terraform fmt -> to format the terraform file i.e it will make the file in more readable way.

* terraform plan -> it will show, what is the execution the file.

* terraform apply -> it will start executing the script.

* terraform maintains the state i.e it stores the current information in the state, if you make any changes and apply the changes 
  then it will compare the current state with remote state and then apply the changes.

* terraform destroy -> it will delete the resources.


Let's see how we can launch ec2 instance from terraform script,


steps:
       1.create the IAM user with AdministratorAccess
       2.configure aws by providing the access_key and secret_key of the IAM user.
       3.write the terraform
       4.run the terraform commands to execute the script.


provider "aws" {
   region = "us-east-1"
  #access_key = ""
  #secret_key = ""  // bad pratice to mention crendentials here.Better way is configure through awscli(aws configure command to configure aws).
}

resource "aws_instance(resource type to create ec2 instance)" "intro(name of resource)" {

             ami = "ami-6489284094dsg1344"
             instance_type = "t2.micro"
             availability_zone = "us-east-1a"
             key_name = "terra-key" (create the key in aws)
             vpc_security_group_ids = ["sg-54263t728t348yg3"]
             tags = {
                Name = "terraform-demo"
             }
}


variables:
           variables help to move sensitive or critical information out of the script or moved to another file,
           so that we can easily reuse or modify them. EX : Ami,tags and key-pair etc.

  Let's see how we can write the terraform script for ec2 instance to launch using variables,

steps:
       1.write providers.tf file.
       2.write vars.tf file.
       3.write instance.tf file.
       4.run the terraform commands to execute the script.


provisers.tf file,

                    provider "aws" {
                     
                          region = var.REGION
                    }

vars.tf file,

             variable REGION {
      
               default = "us-east-1"
           }

            variable ZONE1 {
              default = "us-east-1a"
            }

             variable INSTANCETYPE {
               default = "t2.micro"
              }

              variable AMIS {
        
                 type = map
                 default = {
                    us-east-1 = "ami-5t2783490094bgc34"
                    us-west-2 = "ami-234567iugufc45678"

                  }
            } 


instance.tf file,

                    resource "aws_instance" "ec2-launch" {

                             ami = var.AMIS[var.REGION]
                             instance_type = var.INSTANCETYPE
                             availability_zone = var.ZONE1
                             key_name = "terra-key" (create the key in aws)
                             vpc_security_group_ids = ["sg-54263t728t348yg3"]
                             tags = {
                                Name = "terraform-demo"
                            }
                       }


Provisioners:
              provisioning means when the vm is up or when the instance is going to launch at the you want to perform
              some tasks or operation in the vm.

let's see how we can connect to Linux machine through terraform,

SSH

provisioner "file" {

   source = "files/test.conf"
   destination = "/etc/test.conf"

   connection {

     type = "ssh"
     user = "root"
     password = var.PASSWORD
}
}



let's see how we can connect to windows machine through terraform,

WinRM

provisioner "file" {

   source = "conf/myapp.conf"
   destination = "c:/App/myapp.conf"

   connection {

     type = "winrm"
     user = "administrator"
     password = var.ADMIN_PASSWORD
}
}

More Provisioners:
                     1. file -> the file provisioner is used to copy files or directori es
                     2. remote-exec -> invokes a command/script on remote resource.(it will execute in remote machine)
                     3. local-exec -> provisioners invokes a local executable after a resource is created.(it will execute in local machine)



Now let's see how we can create the ec2 instance and how to create a key-pair,how to push file/script from local to remote
and how to connect with ec2 and execute some script on the instance.


first let's see the script we want to execute on remote instance,

#!/bin/bash
yum install wget unzip httpd -y
systemctl start httpd
systemctl enabled httpd
wget https://www.tooplate.com/zip-templates/2096_individual.zip
unzip 2096_individual.zip
cp -r 2096_individual/* /var/www/html/
systemctl restart httpd


let's see vars.tf file,

variable USER {

   default = "ec2-user"
}

variable REGION {

 default = "us-east-1"
}

variable ZONE1 {

default = "us-east-1a"

}

variable AMIS {

type = map

default = {

us-east-1 = ami-4567890gjhe599
us-west-2 = ami-436587vjkhiu47
}
}

now let's see providers.tf file

provider "aws" {

   region = var.REGION

}

now let's see instance.tf file,

resource "aws_key_pair" "terra-key" {

  key_name = "terraform-key"   // this is key-pair that created in aws.
  public_key = file("terra-key.pub")  // this terra-key will generated using ssh-keygen and save this with name terra-key.
                                         it will create the public-key and private-key. file is used to read the content.
}

resource "aws-instance" "terra-instance" {

             ami = "ami-6489284094dsg1344"
             instance_type = "t2.micro"
             availability_zone = var.ZONE1
             key_name = aws_key_pair.terra-key.key_name(resourceType.resourceName.attributeName)
             vpc_security_group_ids = ["sg-54263t728t348yg3"]
             tags = {
                Name = "terraform-demo"
             }

   provisioner "file" {

    source = "web.sh"
    destination = "/tmp/web.sh

   }

provisioner "remote_exec" {

  inline = [
  
       "sudo chmod u+x /tmp/web.sh",
       "sudo /tmp/web.sh"
]
}

 connection {

   user = var.USER
   private_key = file("terra-key")
   host = self.public_ip

  }


}


Output:
        * Terraform stores a retuned values of all resources created.
        * use output block to print the attributes.
        * use local-exec to save information into the file.

ex:
     output "instance_ip_addr"(output-name) {

        value = aws_instance.server.public_ip  // resourceType.resourceName.attributeName
}   // it will print the public ip address.

let's see how we can print the public-ip anf private-ip of above terra-instance,

above code is same but add this in instance.tf file at last,

output "PUBLICIP" {

   value = aws_instance.terra-instance.public_ip   // it will print public-key
}

output "PRIVATEIP {

  value = aws_instance.terra-instance.private_ip  // it will print private-key
}




Backend:
         Terraform maintains the state and if you want to maintain common state or sync the state with all the teams then
         store in s3 bucket.

let's take above code and we will create the backend file called backend.tf 

terraform {

  backend "s3" {
    bucket = "terra-mybucket123" //bucket-name
    key = "terraform/backend   // terraform is the folder-name inside the bucket and backend in this file state should be mentioned.
    region = "us-east-1"
}
}


Now let's see how we can use multiple resources together.

first we will create an ec2 instance, create own key-pair and security-group,create volume and attach to the instance
create vpc and 6 subnets in it( 3 public subnets & 3 private subnets), create internet-gateway and create route tables
and route-table associations. and provisioning one website in the instance and backend state(for maintaining common state).


first let's create the providers file using provisioners.tf file,

provider "aws" {

   region = var.REGION
}

let's create the vpc using vpc.tf file,

resource "aws_vpc" "terra-vpc" {

   cidr_block = "10.0.0.0/16"
   instance_tenancy = "default"
   enable_dns_support = "true"
   enable_dns_hostnames = "true"
 
    tags = {
      Name = "vprofile"
 }


resource "aws_subnet" "terra-pub-1" {

   vpc_id = aws_vpc.terra-vpc.id
   cidr_block = 10.0.1.0/16
   map_public_ip_on_launch = "true"
   availability_zone = var.ZONE1
   tags = {
     Name = "terra-pub-1"
}
}

resource "aws_subnet" "terra-pub-2" {

   vpc_id = aws_vpc.terra-vpc.id
   cidr_block = 10.0.2.0/24
   map_public_ip_on_launch = "true"
   availability_zone = var.ZONE2
   tags = {
     Name = "terra-pub-2"
}
}


resource "aws_subnet" "terra-pub-3" {

   vpc_id = aws_vpc.terra-vpc.id
   cidr_block = 10.0.3.0/24
   map_public_ip_on_launch = "true"
   availability_zone = var.ZONE3
   tags = {
     Name = "terra-pub-3"
}
}

resource "aws_subnet" "terra-priv-1" {

   vpc_id = aws_vpc.terra-vpc.id
   cidr_block = 10.0.4.0/24
   map_public_ip_on_launch = "true"
   availability_zone = var.ZONE1
   tags = {
     Name = "terra-priv-1"
}
}

resource "aws_subnet" "terra-priv-2" {

   vpc_id = aws_vpc.terra-vpc.id
   cidr_block = 10.0.5.0/24
   map_public_ip_on_launch = "true"
   availability_zone = var.ZONE2
   tags = {
     Name = "terra-priv-2"
}
}

resource "aws_subnet" "terra-priv-3" {

   vpc_id = aws_vpc.terra-vpc.id
   cidr_block = 10.0.4.0/24
   map_public_ip_on_launch = "true"
   availability_zone = var.ZONE3
   tags = {
     Name = "terra-priv-3"
}
}

resource "aws_internet_gateway" "terra-IGW" {

  vpc_id = aws_vpc.terra-vpc.id
  tags = {
    Name = "terra-IGW"
 }

}

resource "aws_route_table" "terra-pub-RT" {

    vpc_id = aws_vpc.terra-vpc.id

   route {
       cidr_block = "0.0.0.0/0"
       gateway_id = aws_internet_gateway.terra-IGW.id
   }
     tags = {
       Name = "terra-pub-RT"
}

resource "aws_route_table_association" "terra-pub-1-a" {
    
    subnet_id = aws_subnet.terra-pub-1.id
    route_table_id = aws_route_table.terra-pub-RT.id
}

resource "aws_route_table_association" "terra-pub-2-a" {
    
    subnet_id = aws_subnet.terra-pub-2.id
    route_table_id = aws_route_table.terra-pub-RT.id
}

resource "aws_route_table_association" "terra-pub-3-a" {
    
    subnet_id = aws_subnet.terra-pub-3.id
    route_table_id = aws_route_table.terra-pub-RT.id
}


now let's create thge security_group for VPC using security-group.tf,

  resource "aws_security_group" "terra-sg" {
  
    vpc_id = aws_vpc.terra-vpc.id
    name = "terra-sg"
    description = "security group for vpc and ec2 instance"

    egress {  // outbound-rule

      from_port = 0
      to_port = 0
      protocol = "-1"
      cidr_block = ["0.0.0.0/0"]
}

    ingress {

     from_port = 22
     to_port = 22
     protocol = "tcp"
     cidr_block = [var.MYIP]
    }

  tags = {
    Name = allow-ssh"
}


now let's write the script for provisioning website using web.sh file,

#!/bin/bash
yum install wget unzip httpd -y
systemctl start httpd
systemctl enabled httpd
wget https://www.tooplate.com/zip-templates/2096_individual.zip
unzip 2096_individual.zip
cp -r 2096_individual/* /var/www/html/
systemctl restart httpd

let's write the backend file for storing the state using backend.tf file,

terraform {

  backend "s3" {
    bucket = "terra-mybucket123" //bucket-name
    key = "terraform/backend   // terraform is the folder-name inside the bucket and backend in this file state should be stored.
    region = "us-east-1"
}
}

let's create variables using vars.tf file,

variable USER {

   default = "ec2-user"
}

variable REGION {

 default = "us-east-1"
}

variable ZONE1 {

default = "us-east-1a"

}

variable ZONE2 {

default = "us-east-1b"

}

variable ZONE3 {

default = "us-east-1c"

}

varible MYIP {

  default = "12.24.134.12/24"
}


variable AMIS {

type = map

default = {

us-east-1 = ami-4567890gjhe599
us-west-2 = ami-436587vjkhiu47
}
}

variable PUBLICKEY {

  default = "terrakey.pub"
}

variable PRIVATEKEY {

  default = "terrakey"
}

now let's create the instance.tf file,


resource "aws_key_pair" "terra-key" {

     key_name = "terrakey"
     public_key = file(var.PUBLICKEY)
}


resource "aws-instance" "terra-instance" {

             ami = var.AMIS[var.REGION]
             instance_type = "t2.micro"
             subnet_id = aws_subnet.terra-pub-1.id
             availability_zone = var.ZONE1
             key_name = aws_key_pair.terra-key.key_name(resourceType.resourceName.attributeName)
             vpc_security_group_ids = [aws_security_group.terra-sg.id]
             tags = {
                Name = "terraform-demo"
             }

      provisioner "file" {

          source = "web.sh"
          destination = "/tmp/web.sh

        }

provisioner "remote_exec" {

  inline = [
  
       "sudo chmod u+x /tmp/web.sh",
       "sudo /tmp/web.sh"
]
}

 connection {

   user = var.USER
   private_key = file(var.PRIVATEKEY)
   host = self.public_ip

  }

   }

  resource "aws_ebs_volume" "terra-volume" {

      availability_zone = var.ZONE1
      size = 3
      tags = {
        Name= "terra-volume"
}
}

resource "aws_volume_attachment" "terra-vol-attach" {

   device_name = "/dev/xvdh"
   volume_id = aws_ebs_volume.terra-volume.id
   instance_id = aws_instance.terra-instance.id
}

output "PUBLICIP" {

    value = "aws_instance.terra-instance.public_ip"
}




CloudFormation:
                  CloudFormation is a cloud automation tool just like Terraform.Only difference is t

                 coludformation is specific to aws and where as Terraform supports various types.


===============================================================================================================================================

Terraform :

Why we need to use Terraform:

Let’s say team 1 requested you to create s3 bucket in AWS. So you login into the AWS and then search for S3 bucket and fill the required and then it will create the s3. So to create s3 let’s say it will take 3 mins. And again 100 teams requested you to create the s3 bucket and time taken to complete this is 3* 100. Instead of wasting the time, if you use tool like terraform we can save that time. We just write the terraform script and then we will run it. It will create the s3 automatically. We can save a lot of time.

They are many tools available in the market, cloud formation is also IAC but it is specific to AWS only and azure resources manager is also a IAC but it is also specific to Azure. As a DevOps engineer we cannot learn all this tools. To overcome this problem, Terraform comes into play. We can target any cloud platform like AWS, azure or GCP etc . It will very easy to create the resources in cloud. It uses the syntax called HashiCorp Configuration Language(HCL).

Required things to work with terraform:
1. Install AWS CLI
2. Terraform 
3. For writing terraform file Visual Studio Code.

How to configure AWS:
1. create the IAM user 
2. create the access keys for that IAM user
3. Run the command aws configure
4. It will ask for access key and secret key 
5. Then the configuration is done and to verify run aws s3 ls. This will show all the s3 list.

How to run terraform:
1. terraform init this will check the provider like AWS and download the necessary plugins and make a connection to it.
2. terraform validate -> this will validate the terraform files by checking syntax and colons.
3. terraform fmt -> it will format the terraform files like it will make the files in more readable way.
4. terraform plan -> it will run as dry run i.e it will show the execution of the file. But not create a resources.
5. terraform apply -> it will start executing the script.
6. terraform destroy -> it will delete the resources.

Providers:
Providers are the cloud platform where you want to create the resources. Like AWS, Azure etc

Let’s see how we can write the providers:

provider “aws” {
    region = “us-east-1”
}

This is very easy because we are creating the resources in one region only,
What if we want to create the resources on multiple regions and also multi cloud because you are using hybrid cloud. How can we create this is very important in terraform.

Let’s see how we can create the multiple regions:

We will create multiple providers and  for every provide we have to mention the region.we will use that providers in the resources that’s it. Here alias means just alternate name for that region

provider “aws” {
    alias = “us-east-1”
    region = “us-east-1”
}

provider “aws” {
    alias = “us-west-1”
    region = “us-west-1”
}

resource “aws_instance” “demo” {
   ami = “ ami-1435738hdbjq”
    instance_type = “t2.micro”
     ..other details 
    provider = “aws.us-east-1”
}

resource “aws_instance” “demo” {
   ami = “ ami-1435738hdbjq”
    instance_type = “t2.micro”
     ..other details 
    provider = “aws.us-west-1”
}

Let’s see how we can write the multiple providers for different clouds:

Here we will create multiple providers with different cloud 

provider “aws” {
    region = “us-east-1”
}

provider “azurerm” {
     provide Details
}

Variables:
Variables in terraform can be of two types:
1. Input variable 
2. Output variable 
3. terraform.tfvars

Input variable are the variable which replaced by the hard coded values in the file. The syntax for it is,

variable “ami” { 
    type = string
    default = “ami-16738jbsqwe”
} 
 This how we can create the input variables and can be access using var.ami. var.name of the variable.

2. Output variables are used to print it after the resources are created.
         Let’s take an example, you want to print of public ip of the ec2 instance after it’s creation then 

output “public_ip” {
 value = resource_type.resource_name.public_ip

  value = aws_instance.ec2_demo.public_ip
}
This is how we can create the output variables.

3. terraform.tfvars:
         For input variable file there we only declare the variable and in terraform.tfvars file we will provide the values for that variables.

For example:

In input variable file,

variable “instance_type” {
   description = “value for the instance type”
}

In terraform.tfvars file,

instance_type = “t2.micro”

Like this we will use them and we can change the name of the terraform.tfvars but while executing the terraform using terraform apply, here you need to pass that file name. If you use terraform.tfvars then you don’t need to pass it because by default it will look for it.
         

Conditional expressions:
Whenever you want to perform actions based on the condition then use this conditional expressions. Syntax for it is,
Condition ? Expression 1 : Expression 2
Here when the condition is true then it will execute Expression 1. If the condition is false then it will execute the Expression 2.

Modules:
In terraform usually we write everything in one file instead of that we will divide into modules. In java terraform file is like monolithic architecture everything present in single project and then later they can convert into micro service based project just like modules in terraform.The main advantage are 
    1. Modularity means breaking down the code     
        into smaller components.
2. Reusability means for example you have write seperate module for ec2 creation then another team member want to create the ec2 then he can reuse this module instead of writing again.
3. Testing  it is very easy to test the individual components rather than entire file.

We need to create separate files for each and everything,
   1. Create variable file and put all variables here
2. Create provider file and put that information here
3. Create output variable and put that information here
4. terraform.tfvar put all the values of variable files here
5. Finally main.tf, here mention all the resources.

Let’s see this approach first and then from this we will create the modules.

In provider.tf file,

provider “aws” {
 region = “us-east-1”
}

In main.tf file,

resource “aws_instance” “demo” {

 ami = var.ami
instance_type = var.instance_type
}

In variables.tf file,

variable “ami” {
   description = “value for the ami”
}

variable “instance_type” {
   description = “value for the instance type”
}

In output.tf file,

output “public-ip-address” {
 value = aws_instance.demo.public_ip
}

In terraform.tfvars file,

ami = “ami-342792ggs18h”
instance_type = “t2.micro”

If you execute this, it will create the ec2 instance in aws and print the public ip of that instance in the terminal.

For the same example, if you want to convert into module then,
We will create the folder and name as ec2_ instance_name (name is anything) and move all the files to this folder.
Finally we will create the main.tf and import this folder into this file that’s it.

Let’s see how we can do that,

In main.tf file,

provider “aws” {
region = “us-east-1”
}

module “ec2_instance” {
source =  “./ec2_instance_module” // mention path where the module is located.
ami = “”
instance_type = “”
}

This is how we can import the modules and whenever another team member wants to create ec2 instance then share the above module and ask them to write the main.tf and import the module inside it and also pass the values for it.

NOTE: 
whenever you want to create the module approach then while consuming the module in the main.tf you have mention the provider, variables and then import that module here that’s it.  And also create the terraform.tfvars file because we need to pass the values to that variables.

terraform show -> it will display the state file 

State file:
State file is used to store the information about the created resources in cloud.

Let’s take an example, you have a created a ec2 instance in AWS and that information is stored in the state file. And sudden you wanted add tags to the ec2 instance. So you have modified the script and run terraform apply. If you don’t have the state file then previous created instance information is not there then terraform will create the new ec2 instance with same details and add tag also. If you have the state file then whenever you modify or delete the ec2 instance it thinks that for this instance i need to update the tag in AWS. State file is the HEART of the terraform.

The disadvantage of not having the state file is when you modify the content in the script then it will create the new resource instead of modification. The same apply also for deletion, when you want to delete the ec2 instance then terraform don’t know what to delete. You have state then that information is stored in it.

The Drawbacks of State file:

The state file can become a single point of failure. If it gets corrupted or lost, it can lead to significant issues in resource management.

The state file can contain sensitive information, such as access keys and passwords, which poses security risks if not properly managed.

The above problems are overcome using remote backend:
Let’s see how we can remove those problems,instead of storing the state file in local or in the GitHub we are storing in external storages like S3 bucket. So whenever we want to update the script, just we clone that repository to local and update the script and verify the changes by running terraform apply then the resources will get updated and also the state file in S3 also updated because when you first run terraform init then terraform will get the state file information from S3 and compare the changes with the script and update the changes. Previous state file is present in local or GitHub then it will compare the local state file and the script then it will get updated accordingly. Then we will again push the script to the GitHub.

Let’s we how we can write the remote backend file,

terraform {
  backend “s3” {
   bucket = “raju-bucket-demo-project-1”
   region = “us-east-1”
   key = “raju/terraform.tfstate”
}
}

Here bucket name should be unique and key is, inside s3 bucket it will create the folder called raju and inside it contains the terraform.tfstate state file.

LOCKING Mechanism in terraform:

Let’s see two DevOps Engineer try to update the s3 bucket like one is updating as public and another is updating as private. So both are try to modify the script and applying the changes by running terraform apply then due to this conflict will occur. So to overcome this problem terraform comes up with locking mechanism, if one person is doing modification and running the commands then it will lock the project, another person cannot  modify and run commands. After first person finishes the job then it will open for the second person to modify and run the commands.

Let’s see how we can implement the locking:
This is implemented using dynamo db

resource “aws_dynamodb_table” “terraform_lock” {
 name = “terraform_lock”
 billing_mode = “PAY_PER_REQUEST”
 hash_key = “LockID”
}

attribute {
name = “LockID”
type = “S”
}
Here LockID is the generated hash key and type is lock type here it is string.
And we need to add this lock to the backend file to implement the lock.

terraform {
  backend “s3” {
   bucket = “raju-bucket-demo-project-1”
   region = “us-east-1”
   key = “raju/terraform.tfstate”
    dynamodb_table = “terraform_lock”
}
}

Provisioners:
Provisioner are the actions , whenever the resources are creating if you want to do some actions like executing the script or install any thing on it or copying files. With the help of provisioner we can do that.

Types of provisioners:
1. remote-exec
2. local-exec
3. file provisioners

Remote-exec: 
while creating the resources if you want to execute the script or install anything on it in the remote server then you have to use remote-exec.

Local-exec:
Local-exec is used to print anything in the terminal or console like these resources are created like that  or you want the output to be stored in the file in locally then local-exec is used.

File Provisioners:
File provisioners are used to copy the file at the time of creating the resources in the remote server.

Most Common Task DevOps Engineer Do:

Let’s say one developer came and ask you to create the infrastructure VPC and it’s entire setup and subnet should be public . And also the ec2. In ec2 the developer wants to deploy the application that he has given and he needs the verify the changes in it.
For this scenario developer wants you to create the terraform project.

Let’s the steps how we can create the VPC and deploy application on it:
1. Create the VPC
2. inside VPC, create the subnet. for this VPC we need to create CIDR block.
3. For that subnet, create the route table and for the route table destination is internet gateway. For this subnet also we need to create the. CIDR block.
4. Note: subnet CIDR range should fall under the VPC range only.
5. we need to associate the route table with subnet. So that the subnet will be public subnet.
6. Create the EC2 and in that deploy the application.
7. For that ec2 configure the security group accordingly. i.e expose the inbound traffic to access application from the browser.
8. by using File Provisioners, we will copy the application into ec2.
9. By using remote-exec, we will install the required things and then run the application.

# Define the AWS provider configuration.
provider "aws" {
  region = "us-east-1"  # Replace with your desired AWS region.
}

variable "cidr" {
  default = "10.0.0.0/16"
}

resource "aws_key_pair" "example" {
  key_name   = "terraform-demo-abhi"  # Replace with your desired key name
  public_key = file("~/.ssh/id_rsa.pub")  # Replace with the path to your public key file
}

resource "aws_vpc" "myvpc" {
  cidr_block = var.cidr
}

resource "aws_subnet" "sub1" {
  vpc_id                  = aws_vpc.myvpc.id
  cidr_block              = "10.0.0.0/24"
  availability_zone       = "us-east-1a"
  map_public_ip_on_launch = true
}

resource "aws_internet_gateway" "igw" {
  vpc_id = aws_vpc.myvpc.id
}

resource "aws_route_table" "RT" {
  vpc_id = aws_vpc.myvpc.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.igw.id
  }
}

resource "aws_route_table_association" "rta1" {
  subnet_id      = aws_subnet.sub1.id
  route_table_id = aws_route_table.RT.id
}

resource "aws_security_group" "webSg" {
  name   = "web"
  vpc_id = aws_vpc.myvpc.id

  ingress {
    description = "HTTP from VPC"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
  ingress {
    description = "SSH"
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name = "Web-sg"
  }
}

resource "aws_instance" "server" {
  ami                    = "ami-0261755bbcb8c4a84"
  instance_type          = "t2.micro"
  key_name      = aws_key_pair.example.key_name
  vpc_security_group_ids = [aws_security_group.webSg.id]
  subnet_id              = aws_subnet.sub1.id

  connection {
    type        = "ssh"
    user        = "ubuntu"  # Replace with the appropriate username for your EC2 instance
    private_key = file("~/.ssh/id_rsa")  # Replace with the path to your private key
    host        = self.public_ip
  }

  # File provisioner to copy a file from local to the remote EC2 instance
  provisioner "file" {
    source      = "app.py"  # Replace with the path to your local file
    destination = "/home/ubuntu/app.py"  # Replace with the path on the remote instance
  }

  provisioner "remote-exec" {
    inline = [
      "echo 'Hello from the remote instance'",
      "sudo apt update -y",  # Update package lists (for ubuntu)
      "sudo apt-get install -y python3-pip",  # Example package installation
      "cd /home/ubuntu",
      "sudo pip3 install flask",
      "sudo python3 app.py ",
    ]
  }
}

Workspaces:

Consider this scenario, one developer came to you and ask you to create EC2 and S3. So we have created those resources and everything is good. Then the next day 10 developer came and ask the same thing then you have converted the script into modular approach. This script will be reused and just the developer need to write the main.tf file where they need to import that file in it and you have tell them or provided the read.md in that file you have mention how to write that file. Here the problem is solved.

Then again the problem is they wants to test it in different environments like dev, staging and prod. For every environment different configuration they want like instance type of ec2 for dev is t2.micro, for staging t2.medium and for prod t2.large. How can you resolve this problem instead writing the terraform project(script) for every environment.so you have decided to write the terraform.tfvars file for every environments because in that file you can modify the configuration accordingly to your needs because instance type is variablized. Still the problem is not solved because when you have run it for dev environment then resources are created and also state file is created, everything is good . So when you have run it for the staging environment then the resources are already present then terraform thinks that i just have to modify or update the configuration because the state file is one who maintains the state between the resources you have created. It will update the ec2 instance. Then this approach will not solve the problem.

So these problem are resolved by using WORKSPACES. just create the workspaces for dev, staging and prod or as many you want. The workspaces creates the separate state file for every environment. So whenever you in dev environment then resources are created accordingly and also for staging and prod as well. Here the state file is not collapsed or mixed because for every environment we are maintaining one state file.

Let’s see how we can create the workspace:

 terraform workspace new dev -> this will create the dev workspace.
terraform workspace select dev -> this will switch to dev workspace.
terraform workspace show -> it will show which workspace you are in.
 This will create the workspaces and then you switch into the workspace then execute the terraform commands. just modify the instance type value in terraform.tfvars file whenever you switch to the different environment then in that environment the new resources will get created and also for every environment, separate state file is present.

Instead of modifying the value every time whenever you switch to different environment. You can create the separate terraform.tfvars file for everything environment and while running the terraform apply pass these terraform.tfvars file to it if the file is different like terraform apply -var-file = stage.tfvars
Or 
You can also follow these approach, remove the instance type from the terraform.tfvars file and go to main.tf file, go to variable for instance type and there you write type as map(string) and in the default mention the values for dev, staging and prod. And then go to instance type in resource there you need to mention three values, first var.instance_type this is the name of the variable and second is terraform.workspace while executing what ever workspace in you are that will come here and then last the default value if the above environment is not present in the variable then this default value will be taken.

provider "aws" {
  region = "us-east-1"
}

variable "ami" {
  description = "value"
}

variable "instance_type" {
  description = "value"
  type = map(string)

  default = {
    "dev" = "t2.micro"
    "stage" = "t2.medium"
    "prod" = "t2.xlarge"
  }
}

module "ec2_instance" {
  source = "./modules/ec2_instance"
  ami = var.ami
  instance_type = lookup(var.instance_type, terraform.workspace, "t2.micro")
}

VAULT:
Vault is used to store the secrets. 
Let’s see how we can install the Vault,

# Vault Integration

Here are the detailed steps for each of these steps:

## Create an AWS EC2 instance with Ubuntu

To create an AWS EC2 instance with Ubuntu, you can use the AWS Management Console or the AWS CLI. Here are the steps involved in creating an EC2 instance using the AWS Management Console:

- Go to the AWS Management Console and navigate to the EC2 service.
- Click on the Launch Instance button.
- Select the Ubuntu Server xx.xx LTS AMI.
- Select the instance type that you want to use.
- Configure the instance settings.
- Click on the Launch button.

## Install Vault on the EC2 instance

To install Vault on the EC2 instance, you can use the following steps:

*Install gpg*


sudo apt update && sudo apt install gpg


*Download the signing key to a new keyring*


wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg


*Verify the key's fingerprint*


gpg --no-default-keyring --keyring /usr/share/keyrings/hashicorp-archive-keyring.gpg --fingerprint


*Add the HashiCorp repo*


echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main" | sudo tee /etc/apt/sources.list.d/hashicorp.list



sudo apt update


*Finally, Install Vault*


sudo apt install vault


## Start Vault.

To start Vault, you can use the following command:


vault server -dev -dev-listen-address="0.0.0.0:8200"


## Configure Terraform to read the secret from Vault.

Detailed steps to enable and configure AppRole authentication in HashiCorp Vault:

1. *Enable AppRole Authentication*:

To enable the AppRole authentication method in Vault, you need to use the Vault CLI or the Vault HTTP API.

*Using Vault CLI*:

Run the following command to enable the AppRole authentication method:

bash
vault auth enable approle


This command tells Vault to enable the AppRole authentication method.

2. *Create an AppRole*:

We need to create policy first,


vault policy write terraform - <<EOF
path "*" {
  capabilities = ["list", "read"]
}

path "secrets/data/*" {
  capabilities = ["create", "read", "update", "delete", "list"]
}

path "kv/data/*" {
  capabilities = ["create", "read", "update", "delete", "list"]
}


path "secret/data/*" {
  capabilities = ["create", "read", "update", "delete", "list"]
}

path "auth/token/create" {
capabilities = ["create", "read", "update", "list"]
}
EOF


Now you'll need to create an AppRole with appropriate policies and configure its authentication settings. Here are the steps to create an AppRole:

*a. Create the AppRole*:

bash
vault write auth/approle/role/terraform \
    secret_id_ttl=10m \
    token_num_uses=10 \
    token_ttl=20m \
    token_max_ttl=30m \
    secret_id_num_uses=40 \
    token_policies=terraform


3. *Generate Role ID and Secret ID*:

After creating the AppRole, you need to generate a Role ID and Secret ID pair. The Role ID is a static identifier, while the Secret ID is a dynamic credential.

*a. Generate Role ID*:

You can retrieve the Role ID using the Vault CLI:

bash
vault read auth/approle/role/my-approle/role-id


Save the Role ID for use in your Terraform configuration.

*b. Generate Secret ID*:

To generate a Secret ID, you can use the following command:

bash
vault write -f auth/approle/role/my-approle/secret-id
   

This command generates a Secret ID and provides it in the response. Save the Secret ID securely, as it will be used for Terraform authentication.

Let’s see how we can integrate with the terraform:

Here we are creating the secret in the vault and using that secret we are integrating in terraform. In below for the ec2 tag name we are using the secret. In terraform if you want to create the resource then you should use resource and if you want to read from the resource then you use data. In vault, we are providing the details like ip address, authentication mechanism here it is app role and role id , secret id. In AWS they are secret key and access key. In tag, we are getting the secret from the vault.

provider "aws" {
  region = "us-east-1"
}

provider "vault" {
  address = "<>:8200"
  skip_child_token = true

  auth_login {
    path = "auth/approle/login"

    parameters = {
      role_id = "<>"
      secret_id = "<>"
    }
  }
}

data "vault_kv_secret_v2" "example" {
  mount = "secret" // change it according to your mount
  name  = "test-secret" // change it according to your secret
}

resource "aws_instance" "my_instance" {
  ami           = "ami-053b0d53c279acc90"
  instance_type = "t2.micro"

  tags = {
    Name = "test"
    Secret = data.vault_kv_secret_v2.example.data["foo"]
  }
}

Scenario Questions:

1. If you want to migrate from existing environment to terraform how can you do it:
Let’s say you want to migrate the ec2 instance from cloud formation template to terraform then follow these steps.
Steps:
1. write the main.tf file, in the first mention the provider
2. Then you to write the import block.
          import {
             id = mention-ec2-instance-id
             to = aws_instance.example
    }
3. Here id is ec2 instance id and to is ec2 instance type and example is name of the resource.
4. Then run terraform init and then run terraform apply -generate-config-out=create_resource.tf
5. Here it will import the existing ec2 instance configuration to the create_resource.tf file.create_resource.tf file name should be anything.
6. Copy those details from the file and delete that file and paste the details in main.tf file because it is the ec2 instance resource configuration details and remove the import block.
7. now if you run the terraform apply command, it will create the new ec2 instance because the state file is not present.we don’t that, what we want is that it will not create the new instance. Just simply say it’s already there no changes are required when you run the terraform apply command.
8. terraform import aws_instance.example mention-ec2-instance-id
9. Here aws_instance.example is the resources name that we have mention in the main.tf file. It will create the state file.
10. Now run terraform apply again, it will not create the ec2 instance, simply it say no changes are required.
11. This is how we can migrate from existing infrastructure to terraform.

Let’s see the steps how we can automate the infrastructure for VPC using terraform:
1. create the VPC
2. create the subnets and attach the subnets to VPC and also enable public ip to true
3. Create the internet gateway and attach to the VPC.
4. Create the route table and inside it mention route as CIDR block as 0.0.0.0/0 means every traffic that comes to this will be redirect to internet gateway. And attach this to internet gateway.
5. Now in order to access the traffic from internet gateway, you need to have destination right that is subnet and we need to do subnet association.
6. Create the subnet association and inside it mention the subnet and the route table.
7. create the security group and allow the inbound traffic accordingly. And also attach this to VPC.
8. Create the ec2 instances and deploy them in the subnets.
9. Now this are the steps, we can access from anything from the subnet. If you want to add the load balancer we can do that for distributing the traffic.
10. Create the load balancer which contains load balancer type, security groups and the subnets.
11. when some request comes to the load balancer then it will forward to the target group and then the target group forward that to the instance.
12. We need to create the target group which contains the port, protocol like http and the VPC and also the health check. In health check we need to give the path where the application start with that path like /login or / etc.
13. Now we have created the target group which empty and we need to add the targets.
14. We need to create the load balancer target group attachment. Which contains target group, instance id and the port.
15. Now we need to attach the load balancer and the target group.
16. We need to create the load balancer listener. Which contains the load balancer, port , the protocol and the default action. In default action we need to mention target group and the type is forward. default action is, when the request comes to load balancer then it forward 16. to target group and then the target group to instances.



























